{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "import razdel\n",
    "from isanlp.annotation import Token\n",
    "\n",
    "\n",
    "address_syntax = (SERVER0, 3134)  # <- Put address of the udpipe parser\n",
    "address_rst = (SERVER2, 3335)  # <- Put address of the rst parser\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\" Tokenize text, but keep paragraph boundaries. \"\"\"\n",
    "    \n",
    "    while '\\n\\n' in text:\n",
    "        text = text.replace('\\n\\n', '\\n')\n",
    "    result = []\n",
    "    for paragraph in text.split('\\n'):\n",
    "        result.append(' '.join([tok.text for tok in razdel.tokenize(paragraph)]))\n",
    "    return '\\n'.join(result).strip()\n",
    "\n",
    "\n",
    "class WhitespaceTokenizer:\n",
    "    \"\"\"Performs dummy tokenization\n",
    "    when you want to process pretokenized text\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, text):\n",
    "        result = []\n",
    "        start = 0\n",
    "        for token in text.strip().split():\n",
    "            result.append(Token(token, start, start+len(token)))\n",
    "            start += len(token) + 1\n",
    "        return {'tokens': result}\n",
    "\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "     {'tokens': 'tokens',\n",
    "      'sentences': 'sentences'}),\n",
    "    (ProcessorRemote(address_syntax[0], address_syntax[1], '0'),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'lemma': 'lemma',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'postag': 'ud_postag'}),\n",
    "    (ProcessorMystem(delay_init=False),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'postag': 'postag'}),\n",
    "    (ConverterMystemToUd(),\n",
    "     ['postag'],\n",
    "     {'morph': 'morph',\n",
    "      'postag': 'postag'}),\n",
    "    (ProcessorRemote(address_rst[0], address_rst[1], 'default'),\n",
    "     ['text', 'tokens', 'sentences', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "     {'rst': 'rst'})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.file_reading import read_edus, read_gold\n",
    "from utils.evaluation import *\n",
    "\n",
    "example = 'data_ru/news2_4'\n",
    "text = open(example + '.txt', 'r').read().strip()\n",
    "gold_edus = read_edus(example)\n",
    "gold_pairs = prepare_gold_pairs(read_gold(example, features=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = ppl(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp.annotation_rst import ForestExporter\n",
    "ex = ForestExporter(encoding='utf8')\n",
    "ex(result['rst'], 'news2_4_pred.rs3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import metrics\n",
    "from utils.discourseunit2str import du_to_docs_structure_charsonly\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pairs = pd.read_feather('data_ru/all_pairs.fth')\n",
    "pairs = prepare_gold_pairs(pairs)\n",
    "pairs.snippet_x = pairs.snippet_x.map(lambda row: charsonly(prepare_string(row).replace(' -', ' - ').replace('- ', ' - ').replace('  ', ' ')))\n",
    "pairs.snippet_y = pairs.snippet_y.map(lambda row: charsonly(prepare_string(row).replace(' -', ' - ').replace('- ', ' - ').replace('  ', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gold_tree_as_string(pairs, filename, text):\n",
    "    if filename:\n",
    "        df = pairs[pairs.filename == filename]\n",
    "    else:\n",
    "        df = pairs[:]\n",
    "    _text = charsonly(prepare_string(text)).strip()\n",
    "    \n",
    "    # In NS/SN nucleus is marked as a span; in NN both nuclei are nuclei\n",
    "    df['loc_x'] = df.apply(lambda row: _text.find(row.snippet_x + row.snippet_y), axis=1)\n",
    "    df['loc_y'] = df.apply(lambda row: row.loc_x + len(row.snippet_x), axis=1)\n",
    "    \n",
    "    result = []\n",
    "    for idx, row in df.iterrows():\n",
    "        left_begin, left_end = row.loc_x, row.loc_x + len(row.snippet_x)\n",
    "        right_begin, right_end = row.loc_y, row.loc_y + len(row.snippet_y)\n",
    "        left_rel = 'span' if row.order == 'NS' else row.category_id\n",
    "        right_rel = 'span' if row.order == 'SN' else row.category_id\n",
    "        left_nuc = 'Satellite' if row.order == 'SN' else 'Nucleus'\n",
    "        right_nuc = 'Satellite' if row.order == 'NS' else 'Nucleus'\n",
    "        result.append(f'({left_begin}:{left_nuc}={left_rel}:{left_end},{right_begin}:{right_nuc}={right_rel}:{right_end})')\n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Document level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PARSING_RES_PATH = 'end2end-rstreebank'\n",
    "if not os.path.isdir(PARSING_RES_PATH):\n",
    "    os.mkdir(PARSING_RES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp.annotation_rst import ForestExporter\n",
    "\n",
    "exp = ForestExporter('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_rstreebank\n",
    "\n",
    "train, dev, test = split_rstreebank('./data_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.autonotebook import tqdm\n",
    "from utils.train_test_split import split_rstreebank, split_essays\n",
    "from utils.file_reading import read_edus\n",
    "from utils.evaluation import *\n",
    "from isanlp.annotation_rst import ForestExporter\n",
    "import pickle\n",
    "\n",
    "train, dev, test = split_rstreebank('./data_ru')\n",
    "\n",
    "# news only\n",
    "#test = [filename for filename in test if 'news' in filename]\n",
    "cache = []\n",
    "thrown_error = []\n",
    "\n",
    "test.sort()\n",
    "ex = ForestExporter()\n",
    "\n",
    "global_metric = metrics.DiscourseMetricDoc()\n",
    "\n",
    "for file in tqdm(test):\n",
    "    file = file.replace('.edus', '')\n",
    "    pure_filename = file.split('/')[-1]\n",
    "    text = open(file + '.txt', 'r').read().strip()\n",
    "\n",
    "    for key in text_html_map.keys():\n",
    "        text = text.replace(key, text_html_map[key])\n",
    "\n",
    "    text = tokenize(text)\n",
    "    result = ppl(text)\n",
    "    # result = pickle.load(open(os.path.join(PARSING_RES_PATH, pure_filename + '.pkl'), 'rb'))\n",
    "\n",
    "    pickle.dump(result, open(f'{PARSING_RES_PATH}/{file.split(\"/\")[-1]}.pkl', 'wb'))\n",
    "    exp(result['rst'], f'{PARSING_RES_PATH}/{file.split(\"/\")[-1]}.rs3')\n",
    "\n",
    "    out_file = file.split('/')[-1]\n",
    "\n",
    "    pred = []\n",
    "    for tree in result['rst']:\n",
    "        dstr = du_to_docs_structure_charsonly(tree, charsonly(text))\n",
    "        if dstr:\n",
    "            pred += dstr\n",
    "    pred = ' '.join(pred)\n",
    "\n",
    "    gold = gold_tree_as_string(pairs, file.split('/')[-1], text)\n",
    "\n",
    "    cur_metric = metrics.DiscourseMetricDoc()\n",
    "    cur_metric([pred], [gold])\n",
    "    print('Current metric:', pure_filename, cur_metric)\n",
    "    global_metric([pred], [gold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Sentence and paragraph level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def charsonly(text):\n",
    "    return ''.join(text.split())\n",
    "\n",
    "# news only\n",
    "#test = [filename for filename in test if 'news' in filename]\n",
    "cache = []\n",
    "thrown_error = []\n",
    "\n",
    "test.sort()\n",
    "ex = ForestExporter()\n",
    "global_metric_s = metrics.DiscourseMetricDoc()\n",
    "global_metric_p = metrics.DiscourseMetricDoc()\n",
    "\n",
    "for file in tqdm(test):\n",
    "    pure_filename = file.replace('.edus', '')\n",
    "    \n",
    "    result = pickle.load(open(os.path.join(PARSING_RES_PATH, pure_filename.split('/')[-1] + '.pkl'), 'rb'))\n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(result['rst']), \n",
    "                                        columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "    parsed_pairs['filename'] = pure_filename.split('/')[-1]\n",
    "    parsed_pairs.snippet_x = parsed_pairs.snippet_x.map(lambda row: charsonly(prepare_string(row).replace(' -', ' - ').replace('- ', ' - ').replace('  ', ' ')))\n",
    "    parsed_pairs.snippet_y = parsed_pairs.snippet_y.map(lambda row: charsonly(prepare_string(row).replace(' -', ' - ').replace('- ', ' - ').replace('  ', ' ')))\n",
    "    parsed_pairs['loc_x'] = parsed_pairs.snippet_x.map(lambda row: charsonly(prepare_string(text)).find(row))\n",
    "    parsed_pairs['loc_y'] = parsed_pairs.snippet_y.map(lambda row: charsonly(prepare_string(text)).find(row)) \n",
    "    \n",
    "    gold_edus = read_edus(pure_filename)\n",
    "    gold_edus = [tokenize(edu.replace(' -', ' - ').replace('  ', ' ')) for edu in gold_edus]\n",
    "\n",
    "    sentences = [tok.text for paragraph in result['text'].split('\\n') for tok in razdel.sentenize(paragraph)]\n",
    "    for idx, sentence in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "        _text = prepare_string(charsonly(sentence))\n",
    "        _gold_edus = [edu for edu in gold_edus if prepare_string(charsonly(edu)) in _text]\n",
    "\n",
    "        _this_sentence = pairs.apply(lambda row: charsonly(row.snippet_x) in _text and charsonly(row.snippet_y) in _text, axis=1)\n",
    "        _gold_pairs = pairs[_this_sentence]\n",
    "\n",
    "        _this_sentence = parsed_pairs.apply(lambda row: charsonly(row.snippet_x) in _text and charsonly(row.snippet_y) in _text, axis=1)\n",
    "        _pred_pairs = parsed_pairs[_this_sentence]\n",
    "\n",
    "        if len(_gold_edus) > 1 and not parsed_pairs.empty and not _gold_pairs.empty:\n",
    "\n",
    "            gold = gold_tree_as_string(_gold_pairs, '', _text)\n",
    "            if _pred_pairs.empty:\n",
    "                pred = ''\n",
    "            else:\n",
    "                pred = gold_tree_as_string(_pred_pairs, '', _text)\n",
    "            \n",
    "            global_metric_s([pred], [gold])\n",
    "\n",
    "    sentences = [paragraph for paragraph in result['text'].split('\\n')]\n",
    "    for idx, sentence in tqdm(enumerate(sentences), total=len(sentences)):\n",
    "        _text = prepare_string(charsonly(sentence))\n",
    "        _gold_edus = [edu for edu in gold_edus if prepare_string(charsonly(edu)) in _text]\n",
    "\n",
    "        _this_sentence = pairs.apply(lambda row: charsonly(row.snippet_x) in _text and charsonly(row.snippet_y) in _text, axis=1)\n",
    "        _gold_pairs = pairs[_this_sentence]\n",
    "\n",
    "        _this_sentence = parsed_pairs.apply(lambda row: charsonly(row.snippet_x) in _text and charsonly(row.snippet_y) in _text, axis=1)\n",
    "        _pred_pairs = parsed_pairs[_this_sentence]\n",
    "\n",
    "        if len(_gold_edus) > 1 and not parsed_pairs.empty and not _gold_pairs.empty:\n",
    "\n",
    "            gold = gold_tree_as_string(_gold_pairs, pure_filename.split('/')[-1], _text)\n",
    "            if _pred_pairs.empty:\n",
    "                pred = ''\n",
    "            else:\n",
    "                pred = gold_tree_as_string(_pred_pairs, pure_filename.split('/')[-1], _text)\n",
    "\n",
    "            global_metric_p([pred], [gold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_metric_p  # paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_metric_s  # sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}