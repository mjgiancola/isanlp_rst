{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 3. BiMPM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Output:\n",
    " - ``models/structure_predictor_bimpm/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from utils.file_reading import read_edus, read_gold, read_negative, read_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/structure_predictor_bimpm’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_bimpm'\n",
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_elmo.json  structure_cf_dev.tsv\t structure_cf_train.tsv\r\n",
      "strprdelmo\t  structure_cf_test.tsv\r\n"
     ]
    }
   ],
   "source": [
    "! ls models/structure_predictor_bimpm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import razdel\n",
    "\n",
    "def tokenize(text):\n",
    "    result = ' '.join([tok.text for tok in razdel.tokenize(text)])\n",
    "    return result\n",
    "    \n",
    "train_samples['snippet_x'] = train_samples.snippet_x.map(tokenize)\n",
    "train_samples['snippet_y'] = train_samples.snippet_y.map(tokenize)\n",
    "\n",
    "dev_samples['snippet_x'] = dev_samples.snippet_x.map(tokenize)\n",
    "dev_samples['snippet_y'] = dev_samples.snippet_y.map(tokenize)\n",
    "\n",
    "test_samples['snippet_x'] = test_samples.snippet_x.map(tokenize)\n",
    "test_samples['snippet_y'] = test_samples.snippet_y.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    33744\n",
       "1    16400\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>category_id</th>\n",
       "      <th>order</th>\n",
       "      <th>filename</th>\n",
       "      <th>is_broken</th>\n",
       "      <th>token_begin_x</th>\n",
       "      <th>token_begin_y</th>\n",
       "      <th>token_end_y</th>\n",
       "      <th>...</th>\n",
       "      <th>eucl_embed_dist</th>\n",
       "      <th>snippet_x_tmp</th>\n",
       "      <th>snippet_y_tmp</th>\n",
       "      <th>postags_x</th>\n",
       "      <th>postags_y</th>\n",
       "      <th>sm_x_positive</th>\n",
       "      <th>sm_x_negative</th>\n",
       "      <th>sm_y_positive</th>\n",
       "      <th>sm_y_negative</th>\n",
       "      <th>relation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>NaN</td>\n",
       "      <td>выло</td>\n",
       "      <td>и яростно трещало</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>./data/blogs_16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.935768</td>\n",
       "      <td>0.937028</td>\n",
       "      <td>0.942065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917148</td>\n",
       "      <td>выть_VERB</td>\n",
       "      <td>и_CONJ яростно_ADV трещать_VERB ,</td>\n",
       "      <td>VERB</td>\n",
       "      <td>CONJ ADV VERB</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.341593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>2405.0</td>\n",
       "      <td>Это</td>\n",
       "      <td>помимо явных перемен в виде тут же появившихся...</td>\n",
       "      <td>elaboration</td>\n",
       "      <td>NS</td>\n",
       "      <td>news1_15</td>\n",
       "      <td>False</td>\n",
       "      <td>0.199614</td>\n",
       "      <td>0.200579</td>\n",
       "      <td>0.216972</td>\n",
       "      <td>...</td>\n",
       "      <td>0.529479</td>\n",
       "      <td>это_PRON</td>\n",
       "      <td>помимо_ADP явный_ADJ перемена_NOUN в_ADP вид_N...</td>\n",
       "      <td>PRON</td>\n",
       "      <td>ADP ADJ NOUN ADP NOUN ADV PART VERB ADJ NOUN P...</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.017452</td>\n",
       "      <td>0.065615</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>1316.0</td>\n",
       "      <td>Боль</td>\n",
       "      <td>от</td>\n",
       "      <td>cause</td>\n",
       "      <td>NS</td>\n",
       "      <td>blogs_65</td>\n",
       "      <td>False</td>\n",
       "      <td>0.128819</td>\n",
       "      <td>0.129328</td>\n",
       "      <td>0.130346</td>\n",
       "      <td>...</td>\n",
       "      <td>1.167933</td>\n",
       "      <td>боль_NOUN</td>\n",
       "      <td>от_ADP потеря_NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>ADP NOUN</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.156115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 2065 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     level_0 snippet_x                                          snippet_y  \\\n",
       "350      NaN      выло                                  и яростно трещало   \n",
       "580   2405.0       Это  помимо явных перемен в виде тут же появившихся...   \n",
       "883   1316.0      Боль                                                 от   \n",
       "\n",
       "     category_id order         filename  is_broken  token_begin_x  \\\n",
       "350          NaN   NaN  ./data/blogs_16      False       0.935768   \n",
       "580  elaboration    NS         news1_15      False       0.199614   \n",
       "883        cause    NS         blogs_65      False       0.128819   \n",
       "\n",
       "     token_begin_y  token_end_y  ...  eucl_embed_dist  snippet_x_tmp  \\\n",
       "350       0.937028     0.942065  ...         0.917148      выть_VERB   \n",
       "580       0.200579     0.216972  ...         0.529479       это_PRON   \n",
       "883       0.129328     0.130346  ...         1.167933      боль_NOUN   \n",
       "\n",
       "                                         snippet_y_tmp  postags_x  \\\n",
       "350                  и_CONJ яростно_ADV трещать_VERB ,       VERB   \n",
       "580  помимо_ADP явный_ADJ перемена_NOUN в_ADP вид_N...       PRON   \n",
       "883                                 от_ADP потеря_NOUN       NOUN   \n",
       "\n",
       "                                             postags_y  sm_x_positive  \\\n",
       "350                                     CONJ ADV VERB         0.00001   \n",
       "580  ADP ADJ NOUN ADP NOUN ADV PART VERB ADJ NOUN P...        0.00001   \n",
       "883                                           ADP NOUN        0.00001   \n",
       "\n",
       "     sm_x_negative  sm_y_positive  sm_y_negative  relation  \n",
       "350       0.000010       0.002126       0.341593         0  \n",
       "580       0.000010       0.017452       0.065615         1  \n",
       "883       0.384922       0.000010       0.156115         1  \n",
       "\n",
       "[3 rows x 2065 columns]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[train_samples.snippet_x.map(len) < 5].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50144.000000\n",
       "mean       104.064734\n",
       "std        100.394467\n",
       "min          3.000000\n",
       "25%         39.000000\n",
       "50%         70.000000\n",
       "75%        130.000000\n",
       "max        726.000000\n",
       "Name: snippet_x, dtype: float64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length = train_samples.snippet_x.map(len)\n",
    "length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes: 2\n",
      "class weights: [0.486012 1.      ]\n"
     ]
    }
   ],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\", np.round(counts.min() / counts, decimals=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50144, 2066)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize BiMPM model with adding inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! rm -r models/bimpm_custom_package\n",
    "! mkdir models/bimpm_custom_package\n",
    "! touch models/bimpm_custom_package/__init__.py\n",
    "! mkdir models/bimpm_custom_package/tokenizers\n",
    "! mkdir models/bimpm_custom_package/dataset_readers\n",
    "! mkdir models/bimpm_custom_package/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/dataset_readers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.dataset_readers.custom_reader import CustomDataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/tokenizers/whitespace_tokenizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/tokenizers/whitespace_tokenizer.py\n",
    "\n",
    "from allennlp.data.tokenizers import Token, Tokenizer\n",
    "from overrides import overrides\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@Tokenizer.register(\"whitespace_tokenizer\")\n",
    "class WhitespaceTokenizer(Tokenizer):\n",
    "    def __init__(self, max_length=None) -> None:\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        if self.max_length:\n",
    "            return [Token(token) for token in text.split()][:self.max_length]\n",
    "        return [Token(token) for token in text.split()]\n",
    "\n",
    "    @overrides\n",
    "    def tokenize(self, text: str) -> List[Token]:\n",
    "        tokens = self._tokenize(text)\n",
    "\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tчто решение визового вопроса займет гораздо более длительный срок .\t\"\"\" Поскольку это касается Шенгенской зоны в целом , изменение визового режима должно быть одобрено всеми странами , входящими в ее состав\"\t0\t0\t0\r\n",
      "1\tначиная жестокими избиениями журналистов\tи заканчивая незаконным лишением свободы для « обеспечения национальной безопасности »\t1\t1\t1\r\n",
      "0\tу кого-то очень плохой обмен веществ , чтобы его восстановить и сделать его быстрее . У кого-то уже патологические нарушения в работе гормонов , которые не позволяют увидеть быстрый результат .\tВаше тело нужно правильно программировать\t0\t0\t2\r\n",
      "0\tПо дороге обратно в отель стояли в огромной пробке минут 40 .\tIMG Конечно , надо было зайти на базар\t0\t0\t3\r\n",
      "0\tТакже задавал этот вопрос в личной беседе опытным JavaScript-разработчикам , выступающим на митапах с докладами , и людям не из мира фронтенда , результат развед-опроса был сильно похож на статистику ответов в twitter .\tЯ знал ответ , это же\t0\t0\t4\r\n",
      "0\tпоскольку большинство российских предприятий сильно недооценены и их акции имеют значительный потенциал роста .\tЧто касается долларовых депозитов\t0\t0\t5\r\n",
      "0\t\"\"\" в 2013 году Россия импортировала из ЕС продуктов питания на сумму в 43 млрд . долларов , при этом на товары , [ позднее ] включенные в эмбарго , приходилось почти 9 млрд . долларов \"\" .\"\tВ августе 2014 года глава представительства Евросоюза в РФ [ Вигаудас Ушацкас ( V у gaudas Usackas ) ] заявил\t0\t0\t6\r\n",
      "1\tВедь для того чтобы покрывать дефицит платежного баланса , США необходимо получать от иностранных инвесторов 1,4 миллиарда долларов ежедневно .\t\"Если же деньги покидают Америку , это означает , что для инвесторов риск инвестиций в экономику США перевешивает ожидания возможной прибыли . И это происходит , несмотря на необычайно слабый рост европейской экономики ( 1-2 процента в зависимости от страны ) . Очевидно , действиями инвесторов руководит логика \"\" пусть поменьше , но зато надежно \"\"\"\t0\t0\t7\r\n",
      "0\tподходит для веганов и маркирована как cruelt у-free . IMG Маска предназначена для очищения и питания кожи , предлагается использовать ее через день . На мой взгляд , это довольно часто для маски , так как эта категория уходовой косметики , по идее , должна давать быстрый результат за одно нанесение . 😄 Попробовав действие маски несколько раз в отдельные дни , я\tдля чистоты\t0\t0\t8\r\n",
      "0\tНо вот интересно раздумывать об этом - как нас может сжирать страх , стоит миру как-то пошатнуться .\tЯ - совсем не спортсменка\t0\t0\t9\r\n"
     ]
    }
   ],
   "source": [
    "! head $TRAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# file_path = TRAIN_FILE_PATH\n",
    "# with open(file_path, \"r\") as data_file:\n",
    "#     tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "#     for row in tsv_in:\n",
    "#         if len(row) == 6:\n",
    "#             print('+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/dataset_readers/custom_reader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/dataset_readers/custom_reader.py\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "from typing import Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.fields import LabelField, TextField, Field, ArrayField\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Tokenizer, SpacyTokenizer, PretrainedTransformerTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@DatasetReader.register(\"custom_pairs_reader\")\n",
    "class CustomDataReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    # Parameters\n",
    "    tokenizer : `Tokenizer`, optional\n",
    "        Tokenizer to use to split the premise and hypothesis into words or other kinds of tokens.\n",
    "        Defaults to `WhitespaceTokenizer`.\n",
    "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
    "        Indexers used to define input token representations. Defaults to `{\"tokens\":\n",
    "        SingleIdTokenIndexer()}`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            tokenizer: Tokenizer = None,\n",
    "            token_indexers: Dict[str, TokenIndexer] = None,\n",
    "            combine_input_fields: Optional[bool] = None,\n",
    "            **kwargs) -> None:\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self._tokenizer = tokenizer or SpacyTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "\n",
    "        if isinstance(self._tokenizer, PretrainedTransformerTokenizer):\n",
    "            assert not self._tokenizer._add_special_tokens\n",
    "\n",
    "        if combine_input_fields is not None:\n",
    "            self._combine_input_fields = combine_input_fields\n",
    "        else:\n",
    "            self._combine_input_fields = isinstance(self._tokenizer, PretrainedTransformerTokenizer)\n",
    "\n",
    "    def _read(self, file_path):\n",
    "        logger.info(\"Reading instances from lines in file at: %s\", file_path)\n",
    "        file_path = cached_path(file_path)\n",
    "        with open(file_path, \"r\") as data_file:\n",
    "            tsv_in = csv.reader(data_file, delimiter=\"\\t\")\n",
    "            for row in tsv_in:\n",
    "                if len(row) == 6:\n",
    "                    yield self.text_to_instance(premise=row[1], hypothesis=row[2], label=row[0],\n",
    "                                                same_sentence=row[3], same_paragraph=row[4])\n",
    "\n",
    "    def text_to_instance(\n",
    "            self,  # type: ignore\n",
    "            premise: str,\n",
    "            hypothesis: str,\n",
    "            label: str,\n",
    "            same_sentence: str,\n",
    "            same_paragraph: str,\n",
    "    ) -> Instance:\n",
    "\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokenized_premise = self._tokenizer.tokenize(premise)\n",
    "        tokenized_hypothesis = self._tokenizer.tokenize(hypothesis)\n",
    "\n",
    "        if self._combine_input_fields:\n",
    "            tokens = self._tokenizer.add_special_tokens(tokenized_premise, tokenized_hypothesis)\n",
    "            fields[\"tokens\"] = TextField(tokens, self._token_indexers)\n",
    "        else:\n",
    "            tokenized_premise = self._tokenizer.add_special_tokens(tokenized_premise)\n",
    "            tokenized_hypothesis = self._tokenizer.add_special_tokens(tokenized_hypothesis)\n",
    "            fields[\"premise\"] = TextField(tokenized_premise, self._token_indexers)\n",
    "            fields[\"hypothesis\"] = TextField(tokenized_hypothesis, self._token_indexers)\n",
    "\n",
    "        _same_sentence = list(map(list, zip(*same_sentence)))\n",
    "        _same_paragraph = list(map(list, zip(*same_paragraph)))\n",
    "        fields[\"same_sentence\"] = ArrayField(np.array(_same_sentence).astype(np.float32))\n",
    "        fields[\"same_paragraph\"] = ArrayField(np.array(_same_paragraph).astype(np.float32))\n",
    "\n",
    "        if label is not None:\n",
    "            fields[\"label\"] = LabelField(label)\n",
    "\n",
    "        return Instance(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/__init__.py\n",
    "\n",
    "try:\n",
    "    from bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor\n",
    "except ModuleNotFoundError:\n",
    "    from models.bimpm_custom_package.tokenizers.whitespace_tokenizer import WhitespaceTokenizer\n",
    "    from models.bimpm_custom_package.model.custom_bimpm import BiMpm as CustomBiMpm\n",
    "    from models.bimpm_custom_package.model.multiclass_bimpm import BiMpm as MulticlassBiMpm\n",
    "    from models.bimpm_custom_package.model.custom_bimpm_predictor import CustomBiMPMPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/custom_bimpm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm.py\n",
    "\"\"\"\n",
    "BiMPM (Bilateral Multi-Perspective Matching) model implementation.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from allennlp.common.checks import check_dimensions_match\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.modules import FeedForward, Seq2SeqEncoder, Seq2VecEncoder, TextFieldEmbedder\n",
    "from allennlp.modules.bimpm_matching import BiMpmMatching\n",
    "from allennlp.nn import InitializerApplicator\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.metrics import CategoricalAccuracy, F1Measure\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "@Model.register(\"custom_bimpm\")\n",
    "class BiMpm(Model):\n",
    "    \"\"\"\n",
    "    This ``Model`` augments with additional features the BiMPM model described in `Bilateral Multi-Perspective \n",
    "    Matching for Natural Language Sentences <https://arxiv.org/abs/1702.03814>`_ by Zhiguo Wang et al., 2017.\n",
    "    implemented in https://github.com/galsang/BIMPM-pytorch>`_.\n",
    "    Additional features are added before the feedforward classifier.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 matcher_word: BiMpmMatching,\n",
    "                 encoder1: Seq2SeqEncoder,\n",
    "                 matcher_forward1: BiMpmMatching,\n",
    "                 matcher_backward1: BiMpmMatching,\n",
    "                 encoder2: Seq2SeqEncoder,\n",
    "                 matcher_forward2: BiMpmMatching,\n",
    "                 matcher_backward2: BiMpmMatching,\n",
    "                 aggregator: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 encode_together: bool = False,\n",
    "                 encode_lstm: bool = True,\n",
    "                 dropout: float = 0.1,\n",
    "                 class_weights: list = [],\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__(vocab, **kwargs)\n",
    "\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "\n",
    "        self.matcher_word = matcher_word\n",
    "\n",
    "        self.encoder1 = encoder1\n",
    "        self.matcher_forward1 = matcher_forward1\n",
    "        self.matcher_backward1 = matcher_backward1\n",
    "\n",
    "        self.encoder2 = encoder2\n",
    "        self.matcher_forward2 = matcher_forward2\n",
    "        self.matcher_backward2 = matcher_backward2\n",
    "\n",
    "        self.aggregator = aggregator\n",
    "\n",
    "        self.encode_together = encode_together\n",
    "        self.encode_lstm = encode_lstm\n",
    "\n",
    "        matching_dim = self.matcher_word.get_output_dim()\n",
    "\n",
    "        if self.encode_lstm:\n",
    "            matching_dim += self.matcher_forward1.get_output_dim(\n",
    "            ) + self.matcher_backward1.get_output_dim(\n",
    "            ) + self.matcher_forward2.get_output_dim(\n",
    "            ) + self.matcher_backward2.get_output_dim(\n",
    "            )\n",
    "\n",
    "        check_dimensions_match(matching_dim, self.aggregator.get_input_dim(),\n",
    "                               \"sum of dim of all matching layers\", \"aggregator input dim\")\n",
    "\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        if class_weights:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = [1.] * self.classifier_feedforward.get_output_dim()\n",
    "\n",
    "        self.metrics = {\"accuracy\": CategoricalAccuracy(),\n",
    "                        \"f1\": F1Measure(1)}\n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(self.class_weights))\n",
    "\n",
    "        initializer(self)\n",
    "\n",
    "    @overrides\n",
    "    def forward(self,\n",
    "                premise: Dict[str, torch.LongTensor],\n",
    "                hypothesis: Dict[str, torch.LongTensor],\n",
    "                same_sentence: List[Dict[str, torch.IntTensor]],\n",
    "                same_paragraph: List[Dict[str, torch.IntTensor]],\n",
    "                label: torch.LongTensor = None,  # pylint:disable=unused-argument\n",
    "                ) -> Dict[str, torch.Tensor]:\n",
    "        # pylint: disable=arguments-differ\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : Dict[str, torch.LongTensor]\n",
    "            The premise from a ``TextField``\n",
    "        hypothesis : Dict[str, torch.LongTensor]\n",
    "            The hypothesis from a ``TextField``\n",
    "        label : torch.LongTensor, optional (default = None)\n",
    "            The label for the pair of the premise and the hypothesis\n",
    "        metadata : ``List[Dict[str, Any]]``, optional, (default = None)\n",
    "            Additional information about the pair\n",
    "        Returns\n",
    "        -------\n",
    "        An output dictionary consisting of:\n",
    "        logits : torch.FloatTensor\n",
    "            A tensor of shape ``(batch_size, num_labels)`` representing unnormalised log\n",
    "            probabilities of the entailment label.\n",
    "        loss : torch.FloatTensor, optional\n",
    "            A scalar loss to be optimised.\n",
    "        \"\"\"\n",
    "\n",
    "        def encode_pair(x1, x2, mask1=None, mask2=None):\n",
    "            _joined_pair: Dict[str, torch.LongTensor] = {}\n",
    "\n",
    "            for key in premise.keys():\n",
    "                bsz = premise[key].size(0)\n",
    "                x1_len, x2_len = premise[key].size(1), hypothesis[key].size(1)\n",
    "                sep = torch.empty([bsz, 1], dtype=torch.long, device=premise[key].device)\n",
    "                sep.data.fill_(0)  # 2 is the id for </s>\n",
    "\n",
    "                x = torch.cat([premise[key], hypothesis[key]], dim=1)\n",
    "                _joined_pair[key] = x\n",
    "\n",
    "            x_output = self.dropout(self.text_field_embedder(_joined_pair))\n",
    "            return x_output[:, :x1_len], x_output[:, -x2_len:], mask1, mask2\n",
    "\n",
    "        mask_premise = util.get_text_field_mask(premise)\n",
    "        mask_hypothesis = util.get_text_field_mask(hypothesis)\n",
    "\n",
    "        if self.encode_together:\n",
    "            embedded_premise, embedded_hypothesis, _, _ = encode_pair(premise, hypothesis)\n",
    "        else:\n",
    "            embedded_premise = self.dropout(self.text_field_embedder(premise))\n",
    "            embedded_hypothesis = self.dropout(self.text_field_embedder(hypothesis))\n",
    "\n",
    "        # embedding and encoding of the premise\n",
    "        encoded_premise1 = self.dropout(self.encoder1(embedded_premise, mask_premise))\n",
    "        encoded_premise2 = self.dropout(self.encoder2(encoded_premise1, mask_premise))\n",
    "\n",
    "        # embedding and encoding of the hypothesis\n",
    "        encoded_hypothesis1 = self.dropout(self.encoder1(embedded_hypothesis, mask_hypothesis))\n",
    "        encoded_hypothesis2 = self.dropout(self.encoder2(encoded_hypothesis1, mask_hypothesis))\n",
    "\n",
    "        matching_vector_premise: List[torch.Tensor] = []\n",
    "        matching_vector_hypothesis: List[torch.Tensor] = []\n",
    "\n",
    "        def add_matching_result(matcher, encoded_premise, encoded_hypothesis):\n",
    "            # utility function to get matching result and add to the result list\n",
    "            matching_result = matcher(encoded_premise, mask_premise, encoded_hypothesis, mask_hypothesis)\n",
    "            matching_vector_premise.extend(matching_result[0])\n",
    "            matching_vector_hypothesis.extend(matching_result[1])\n",
    "\n",
    "        # calculate matching vectors from word embedding, first layer encoding, and second layer encoding\n",
    "        add_matching_result(self.matcher_word, embedded_premise, embedded_hypothesis)\n",
    "        half_hidden_size_1 = self.encoder1.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward1,\n",
    "                            encoded_premise1[:, :, :half_hidden_size_1],\n",
    "                            encoded_hypothesis1[:, :, :half_hidden_size_1])\n",
    "        add_matching_result(self.matcher_backward1,\n",
    "                            encoded_premise1[:, :, half_hidden_size_1:],\n",
    "                            encoded_hypothesis1[:, :, half_hidden_size_1:])\n",
    "\n",
    "        half_hidden_size_2 = self.encoder2.get_output_dim() // 2\n",
    "        add_matching_result(self.matcher_forward2,\n",
    "                            encoded_premise2[:, :, :half_hidden_size_2],\n",
    "                            encoded_hypothesis2[:, :, :half_hidden_size_2])\n",
    "        add_matching_result(self.matcher_backward2,\n",
    "                            encoded_premise2[:, :, half_hidden_size_2:],\n",
    "                            encoded_hypothesis2[:, :, half_hidden_size_2:])\n",
    "\n",
    "        # concat the matching vectors\n",
    "        matching_vector_cat_premise = self.dropout(torch.cat(matching_vector_premise, dim=2))\n",
    "        matching_vector_cat_hypothesis = self.dropout(torch.cat(matching_vector_hypothesis, dim=2))\n",
    "\n",
    "        # aggregate the matching vectors\n",
    "        aggregated_premise = self.dropout(self.aggregator(matching_vector_cat_premise, mask_premise))\n",
    "        aggregated_hypothesis = self.dropout(self.aggregator(matching_vector_cat_hypothesis, mask_hypothesis))\n",
    "\n",
    "        # encode additional information\n",
    "        batch_size, _ = aggregated_premise.size()\n",
    "        encoded_same_sentence = same_sentence.float().view(batch_size, -1)\n",
    "        encoded_same_paragraph = same_paragraph.float().view(batch_size, -1)\n",
    "\n",
    "        # the final forward layer\n",
    "        logits = self.classifier_feedforward(\n",
    "            torch.cat([aggregated_premise,\n",
    "                       aggregated_hypothesis,\n",
    "                       encoded_same_sentence,\n",
    "                       encoded_same_paragraph], dim=-1))\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        output_dict = {'logits': logits, \"probs\": probs}\n",
    "\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "            for metric in self.metrics.values():\n",
    "                metric(logits, label)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "    def make_output_human_readable(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Does a simple argmax over the probabilities, converts index to string label, and\n",
    "        add `\"label\"` key to the dictionary with the result.\n",
    "        \"\"\"\n",
    "        predictions = output_dict[\"label_probs\"]\n",
    "        if predictions.dim() == 2:\n",
    "            predictions_list = [predictions[i] for i in range(predictions.shape[0])]\n",
    "        else:\n",
    "            predictions_list = [predictions]\n",
    "        classes = []\n",
    "        for prediction in predictions_list:\n",
    "            label_idx = prediction.argmax(dim=-1).item()\n",
    "            label_str = self.vocab.get_index_to_token_vocabulary(\"labels\").get(\n",
    "                label_idx, str(label_idx)\n",
    "            )\n",
    "            classes.append(label_str)\n",
    "        output_dict[\"label\"] = classes\n",
    "        return output_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\n",
    "            \"f1\": self.metrics[\"f1\"].get_metric(reset)['f1'],\n",
    "            \"accuracy\": self.metrics[\"accuracy\"].get_metric(reset)\n",
    "        }\n",
    "\n",
    "    default_predictor = 'custom_bimpm_predictor'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/bimpm_custom_package/model/custom_bimpm_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/bimpm_custom_package/model/custom_bimpm_predictor.py\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields.label_field import LabelField\n",
    "from allennlp.data.tokenizers.spacy_tokenizer import SpacyTokenizer\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "\n",
    "\n",
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"custom_bimpm_predictor\")\n",
    "class CustomBiMPMPredictor(Predictor):\n",
    "\n",
    "    def predict(self, premise: str, hypothesis: str, same_sentence: str, same_paragraph: str) -> JsonDict:\n",
    "        return self.predict_json({\"premise\": premise, \"hypothesis\": hypothesis,\n",
    "                                  \"same_sentence\": same_sentence, \"same_paragraph\": same_paragraph})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        \"\"\"\n",
    "        Expects JSON that looks like `{\"premise\": \"...\", \"hypothesis\": \"...\", \"metadata\": \"...\"}`.\n",
    "        \"\"\"\n",
    "        premise_text = json_dict[\"premise\"]\n",
    "        hypothesis_text = json_dict[\"hypothesis\"]\n",
    "        same_sentence = json_dict[\"same_sentence\"]\n",
    "        same_paragraph = json_dict[\"same_paragraph\"]\n",
    "        reader_has_tokenizer = (\n",
    "                getattr(self._dataset_reader, \"tokenizer\", None) is not None\n",
    "                or getattr(self._dataset_reader, \"_tokenizer\", None) is not None\n",
    "        )\n",
    "        if not reader_has_tokenizer:\n",
    "            tokenizer = SpacyTokenizer()\n",
    "            premise_text = tokenizer.tokenize(premise_text)\n",
    "            hypothesis_text = tokenizer.tokenize(hypothesis_text)\n",
    "\n",
    "        return self._dataset_reader.text_to_instance(premise_text,\n",
    "                                                     hypothesis_text,\n",
    "                                                     label=None,\n",
    "                                                     same_sentence=same_sentence,\n",
    "                                                     same_paragraph=same_paragraph)\n",
    "\n",
    "    def predictions_to_labeled_instances(\n",
    "            self, instance: Instance, outputs: Dict[str, numpy.ndarray]\n",
    "    ) -> List[Instance]:\n",
    "        new_instance = instance.duplicate()\n",
    "        label = numpy.argmax(outputs[\"label_logits\"])\n",
    "        # Skip indexing, we have integer representations of the strings \"entailment\", etc.\n",
    "        new_instance.add_field(\"label\", LabelField(int(label), skip_indexing=True))\n",
    "        return [new_instance]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/structure_predictor_bimpm/structure_cf_train.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_dev.tsv\n",
      "models/structure_predictor_bimpm/structure_cf_test.tsv\n"
     ]
    }
   ],
   "source": [
    "print(TRAIN_FILE_PATH)\n",
    "print(DEV_FILE_PATH)\n",
    "print(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tчто решение визового вопроса займет гораздо более длительный срок .\t\"\"\" Поскольку это касается Шенгенской зоны в целом , изменение визового режима должно быть одобрено всеми странами , входящими в ее состав\"\t0\t0\t0\r\n",
      "1\tначиная жестокими избиениями журналистов\tи заканчивая незаконным лишением свободы для « обеспечения национальной безопасности »\t1\t1\t1\r\n",
      "0\tу кого-то очень плохой обмен веществ , чтобы его восстановить и сделать его быстрее . У кого-то уже патологические нарушения в работе гормонов , которые не позволяют увидеть быстрый результат .\tВаше тело нужно правильно программировать\t0\t0\t2\r\n",
      "0\tПо дороге обратно в отель стояли в огромной пробке минут 40 .\tIMG Конечно , надо было зайти на базар\t0\t0\t3\r\n",
      "0\tТакже задавал этот вопрос в личной беседе опытным JavaScript-разработчикам , выступающим на митапах с докладами , и людям не из мира фронтенда , результат развед-опроса был сильно похож на статистику ответов в twitter .\tЯ знал ответ , это же\t0\t0\t4\r\n",
      "0\tпоскольку большинство российских предприятий сильно недооценены и их акции имеют значительный потенциал роста .\tЧто касается долларовых депозитов\t0\t0\t5\r\n",
      "0\t\"\"\" в 2013 году Россия импортировала из ЕС продуктов питания на сумму в 43 млрд . долларов , при этом на товары , [ позднее ] включенные в эмбарго , приходилось почти 9 млрд . долларов \"\" .\"\tВ августе 2014 года глава представительства Евросоюза в РФ [ Вигаудас Ушацкас ( V у gaudas Usackas ) ] заявил\t0\t0\t6\r\n",
      "1\tВедь для того чтобы покрывать дефицит платежного баланса , США необходимо получать от иностранных инвесторов 1,4 миллиарда долларов ежедневно .\t\"Если же деньги покидают Америку , это означает , что для инвесторов риск инвестиций в экономику США перевешивает ожидания возможной прибыли . И это происходит , несмотря на необычайно слабый рост европейской экономики ( 1-2 процента в зависимости от страны ) . Очевидно , действиями инвесторов руководит логика \"\" пусть поменьше , но зато надежно \"\"\"\t0\t0\t7\r\n",
      "0\tподходит для веганов и маркирована как cruelt у-free . IMG Маска предназначена для очищения и питания кожи , предлагается использовать ее через день . На мой взгляд , это довольно часто для маски , так как эта категория уходовой косметики , по идее , должна давать быстрый результат за одно нанесение . 😄 Попробовав действие маски несколько раз в отдельные дни , я\tдля чистоты\t0\t0\t8\r\n",
      "0\tНо вот интересно раздумывать об этом - как нас может сжирать страх , стоит миру как-то пошатнуться .\tЯ - совсем не спортсменка\t0\t0\t9\r\n"
     ]
    }
   ],
   "source": [
    "! head $TRAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models/structure_predictor_bimpm’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir models/structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1074-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1124-1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/structure_predictor_bimpm'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/structure_predictor_bimpm/config_elmo.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo.json\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "// (Augmented with additional granularity related features)\n",
    "\n",
    "local NUM_EPOCHS = 50;\n",
    "local LR = 1e-5;\n",
    "local MAX_LEN = 1000;\n",
    "local LSTM_ENCODER_HIDDEN = 25;\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"custom_pairs_reader\",\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"just_spaces\"\n",
    "    },\n",
    "#     \"token_indexers\": {\n",
    "#       \"tokens\": {\n",
    "#         \"type\": \"single_id\",\n",
    "#         \"lowercase_tokens\": true\n",
    "#       },\n",
    "#       \"elmo\": {\n",
    "#         \"type\": \"elmo_characters\"\n",
    "#       }\n",
    "#     }\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\",\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_bimpm/structure_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_bimpm/structure_cf_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": \"custom_bimpm\",\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [0.5, 1.0],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "        \"token_embedders\": {\n",
    "            \"elmo\": {\n",
    "                    \"type\": \"elmo_token_embedder\",\n",
    "                    \"options_file\": \"rsv_elmo/options.json\",\n",
    "                    \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "                    \"do_layer_norm\": true,\n",
    "                    \"dropout\": 0.2\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"dropout\": 0.2,\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 20,\n",
    "                    \"sparse\": false,\n",
    "                    \"vocab_namespace\": \"token_characters\"\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"lstm\",\n",
    "                    \"input_size\": $.model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim,\n",
    "                    \"hidden_size\": LSTM_ENCODER_HIDDEN,\n",
    "                    \"num_layers\": 1,\n",
    "                    \"bidirectional\": true,\n",
    "                    \"dropout\": 0.4\n",
    "              },\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 1024+LSTM_ENCODER_HIDDEN+LSTM_ENCODER_HIDDEN,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 1024+LSTM_ENCODER_HIDDEN+LSTM_ENCODER_HIDDEN,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": $.model.matcher_forward1.hidden_dim+$.model.matcher_backward1.hidden_dim,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 200,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": $.model.matcher_forward2.hidden_dim+$.model.matcher_backward2.hidden_dim+1+1,\n",
    "      \"num_layers\": 2,\n",
    "      \"hidden_dims\": [200, 2],\n",
    "      \"activations\": [\"mish\", \"mish\"],\n",
    "      \"dropout\": [0.5, 0.0]\n",
    "    },\n",
    "    \"initializer\": {\n",
    "      \"regexes\": [\n",
    "        [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "        [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"data_loader\": {\n",
    "    \"batch_sampler\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 20,\n",
    "        \"padding_noise\": 0.0,\n",
    "        \"sorting_keys\": [\"premise\"],\n",
    "    },\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 5,\n",
    "    \"grad_clipping\": 5.0,\n",
    "    \"validation_metric\": \"+f1\",\n",
    "    \"cuda_device\": 0,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adam\",\n",
    "      \"lr\": LR\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mv ../../../maintenance_rst/models/structure_predictor_bimpm ../../../maintenance_rst/models/structure_predictor_bimpm_OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/structure_predictor_bimpm ../../../maintenance_rst/models/structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/train_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/train_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_bimpm/${RESULT_DIR}/ structure_predictor_bimpm/config_${METHOD}.json \\\n",
    "   --include-package bimpm_custom_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models/eval_structure_predictor.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile models/eval_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_dev.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${DEV_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor\n",
    "\n",
    "allennlp predict --use-dataset-reader --silent \\\n",
    "    --output-file structure_predictor_bimpm/${RESULT_DIR}/predictions_test.json \\\n",
    "    structure_predictor_bimpm/${RESULT_DIR}/model.tar.gz structure_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "    --include-package bimpm_custom_package \\\n",
    "    --predictor custom_bimpm_predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2. Using wandb for parameters adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models/wandb_structure_predictor.yaml\n",
    "# usage:\n",
    "# $ cd models\n",
    "# wandb sweep wandb_structure_predictor.yaml\n",
    "\n",
    "name: structure_predictor_stacked\n",
    "program: wandb_allennlp # this is a wrapper console script around allennlp commands. It is part of wandb-allennlp\n",
    "method: bayes\n",
    "## Do not for get to use the command keyword to specify the following command structure\n",
    "command:\n",
    "  - ${program} #omit the interpreter as we use allennlp train command directly\n",
    "  - \"--subcommand=train\"\n",
    "  - \"--include-package=bimpm_custom_package\" # add all packages containing your registered classes here\n",
    "  - \"--config_file=structure_predictor_bimpm/config_elmo.json\"\n",
    "  - ${args}\n",
    "metric:\n",
    "    name: best_f1\n",
    "    goal: maximize\n",
    "parameters:\n",
    "    model.type:\n",
    "        values: [\"custom_bimpm\",]\n",
    "    iterator.batch_size:\n",
    "        values: [20,]\n",
    "    model.encode_together:\n",
    "        values: [\"false\",]\n",
    "    trainer.optimizer.lr:\n",
    "        values: [0.001,]\n",
    "    model.dropout:\n",
    "        values: [0.5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``wandb sweep wandb_structure_predictor.yaml``\n",
    "\n",
    "(returns %sweepname)\n",
    "\n",
    "``wandb agent --count 1 %sweepname``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the best model in structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -r models/wandb/run-20200720_203050-84hl3zwy/training_dumps models/structure_predictor_bimpm/snowy-sweep-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mv models/wandb/run-20200929_034343-5tmisocu models/structure_predictor_bimpm/colorful-sweep-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            result.append(json.loads(line)[\"label\"])\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    print('length of result:', len(result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../../../maintenance_rst/models/structure_predictor_bimpm/colorful-sweep-1-dumps/*.json models/structure_predictor_bimpm/colorful-sweep-1-dumps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = 'colorful-sweep-1-dumps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(DEV_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(TEST_FILE_PATH, sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vocab = [0, 1]\n",
    "catboost_vocab = [0, 1]\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            probs = json.loads(line)['probs']\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict_proba(X)\n",
    "    except AttributeError:\n",
    "        predictions = model._predict_proba_lr(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(pred1, pred2, soft=True):\n",
    "    assert len(pred1) == len(pred2)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pred1)):\n",
    "        sample_result = {}\n",
    "        for key in pred1[i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = pred1[i][key] + pred2[i][key]\n",
    "            else:\n",
    "                sample_result[key] = max(pred1[i][key], pred2[i][key])\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('models/structure_predictor_baseline/model.pkl', 'rb'))\n",
    "scaler = pickle.load(open('models/structure_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('models/structure_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))\n",
    "\n",
    "y_train, X_train = train_samples['relation'].to_frame(), train_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples['relation'].to_frame(), test_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "svm_predictions = load_scikit_predictions(model, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_dev, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_dev, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_dev, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_predictions = load_scikit_predictions(model, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, soft=True)\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_test, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_test, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_test, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
