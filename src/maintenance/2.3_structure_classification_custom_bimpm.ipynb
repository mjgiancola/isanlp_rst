{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Binary structure classification used in tree building: Step 3. BiMPM\n",
    "\n",
    "Prepare data and model-related scripts.\n",
    "\n",
    "Evaluate models.\n",
    "\n",
    "Output:\n",
    " - ``models/structure_predictor_bimpm/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Make a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = 'models/structure_predictor_bimpm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir $MODEL_PATH\n",
    "\n",
    "TRAIN_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_train.tsv')\n",
    "DEV_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_dev.tsv')\n",
    "TEST_FILE_PATH = os.path.join(MODEL_PATH, 'structure_cf_test.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(train_samples.shape)\n",
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples[train_samples.snippet_x.map(len) < 5].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "length = train_samples.snippet_x.map(lambda row: len(row.split()))\n",
    "length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples.snippet_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples.same_paragraph.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Undersample negative examples \n",
    "... or there will not be enough memory in the world to train\n",
    "\n",
    "here 75259 train samples turn into 41356 samples, x.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# We won't discern any sample pairs that are longer than 100 tokens by default,\n",
    "# and they are bad for the classifier. Filter such pairs:\n",
    "max_len = 100\n",
    "train_samples['cutted_snippet_x'] = train_samples.snippet_x.map(lambda row: ' '.join(row.split()[:max_len]))\n",
    "train_samples['cutted_snippet_y'] = train_samples.snippet_y.map(lambda row: ' '.join(row.split()[:max_len]))\n",
    "train_samples = train_samples.drop_duplicates(['cutted_snippet_x', 'cutted_snippet_y', 'relation'])\n",
    "train_samples = train_samples.drop_duplicates(['cutted_snippet_x', 'cutted_snippet_y'], keep=False)\n",
    "# (75205, 2066)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\" Precompute similarities between vectors of negative examples for undersampling.\n",
    "#     Uncomment and run one time to obtain ``neg_examples_svd.npy`` and/or ``neg_examples_sim.npy``  \"\"\"\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Select negative examples\n",
    "train_samples_neg = train_samples[train_samples.relation == 0]\n",
    "\n",
    "# Convert filenames (train set) to numbers, so we won't lose entire documents in undersampling\n",
    "filenames = pd.get_dummies(train_samples_neg['filename'])\n",
    "train_samples_neg = pd.concat([train_samples_neg, filenames], axis=1)\n",
    "\n",
    "# Select digital columns\n",
    "digital_columns = [column for column in train_samples_neg.columns \n",
    "                   if train_samples_neg[column].dtype.name in ['float64', 'int64']]\n",
    "X_train = train_samples_neg[digital_columns]\n",
    "\n",
    "# Reduce dimentions\n",
    "svd = TruncatedSVD(n_components=50)\n",
    "X_train_reduced = svd.fit_transform(X_train) \n",
    "np.save('neg_examples_svd.npy', X_train_reduced.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "km = MiniBatchKMeans(n_clusters=train_samples[train_samples.relation == 1].shape[0], \n",
    "                     verbose=1, batch_size=8192, max_no_improvement=100)\n",
    "prediction = km.fit_predict(X_train_reduced.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples_neg['km_cluster'] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "nearest_centroid_dist = []\n",
    "for idx, km_cluster in tqdm(enumerate(km.labels_), total=km.labels_.shape[0]):\n",
    "    dist = 1. - cosine_similarity([X_train_reduced[idx]], [km.cluster_centers_[km_cluster]])\n",
    "    nearest_centroid_dist.append(dist[0][0])\n",
    "    \n",
    "train_samples_neg['dist2centroid'] = nearest_centroid_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples_neg.dist2centroid.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def select_optimal(contents, threshold=None):\n",
    "    contents = contents.sort_values(by='dist2centroid', ascending=True)\n",
    "    contents.iloc[-1,-1] = True\n",
    "    return contents\n",
    "\n",
    "if 'keep' in train_samples_neg.columns:\n",
    "    train_samples_neg.drop(columns=['keep'])\n",
    "train_samples_neg['keep'] = False\n",
    "train_samples_neg = train_samples_neg.groupby('km_cluster', as_index=False).progress_apply(\n",
    "    lambda row: select_optimal(row, threshold=train_samples_neg.dist2centroid.describe()['75%']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "examples = train_samples_neg[['snippet_x', 'snippet_y', 'km_cluster', 'dist2centroid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "examples[examples.km_cluster == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "keys = train_samples_neg.keys()\n",
    "[key for key in keys if 'parag' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples_neg.same_paragraph.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "undersampled = train_samples_neg[train_samples_neg.keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "undersampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "undersampled = undersampled.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "undersampled.filename.unique().shape[0], train_samples_neg.filename.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del train_samples_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples = pd.concat([train_samples[train_samples.relation == 1], undersampled]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save train, dev and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_samples = train_samples.reset_index()\n",
    "train_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TRAIN_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "dev_samples = dev_samples.reset_index()\n",
    "dev_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    DEV_FILE_PATH, sep='\\t', header=False, index=False)\n",
    "\n",
    "test_samples = test_samples.reset_index()\n",
    "test_samples[['relation', 'snippet_x', 'snippet_y', 'same_sentence', 'same_paragraph', 'index']].to_csv(\n",
    "    TEST_FILE_PATH, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_FILE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! wc -l models/structure_predictor_bimpm/structure_cf_train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "counts = train_samples['relation'].value_counts(normalize=False).values\n",
    "NUMBER_CLASSES = len(counts)\n",
    "print(\"number of classes:\", NUMBER_CLASSES)\n",
    "print(\"class weights:\", np.round(counts.min() / counts, decimals=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Generate config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(TRAIN_FILE_PATH)\n",
    "print(DEV_FILE_PATH)\n",
    "print(TEST_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile $MODEL_PATH/config_elmo_fasttext.jsonnet\n",
    "\n",
    "// Configuration for a sentence matching model based on:\n",
    "//   Wang, Zhiguo, Wael Hamza, and Radu Florian. \n",
    "//   \"Bilateral multi-perspective matching for natural language sentences.\"\n",
    "//   Proceedings of the 26th International Joint Conference on Artificial Intelligence. 2017.\n",
    "// (Augmented with additional granularity related features)\n",
    "\n",
    "\n",
    "local NUM_EPOCHS = 50;\n",
    "local LR = std.parseJson(std.extVar('LR'));\n",
    "local MAX_LEN = 100;\n",
    "local LSTM_ENCODER_HIDDEN = std.parseJson(std.extVar('LSTM_ENCODER_HIDDEN'));\n",
    "local LSTM_AGG_HIDDEN = std.parseJson(std.extVar('LSTM_AGG_HIDDEN'));\n",
    "\n",
    "local dataset_reader_type = \"bimpm_custom_package.dataset_readers.custom_reader.CustomDataReader\";\n",
    "local model_type = \"bimpm_custom_package.model.custom_bimpm.BiMpm\";\n",
    "\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": dataset_reader_type,\n",
    "    \"tokenizer\": {\n",
    "      \"type\": \"just_spaces\"\n",
    "    },\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 30\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\",\n",
    "      },\n",
    "#       \"tokens\": {\n",
    "#         \"type\": \"single_id\",\n",
    "#         \"lowercase_tokens\": true\n",
    "#       },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": \"structure_predictor_bimpm/structure_cf_train.tsv\",\n",
    "  \"validation_data_path\": \"structure_predictor_bimpm/structure_cf_dev.tsv\",\n",
    "  \"model\": {\n",
    "    \"type\": model_type,\n",
    "    \"dropout\": 0.5,\n",
    "    \"class_weights\": [1.0, 1.0],\n",
    "    \"encode_together\": false,\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "#         \"tokens\": {\n",
    "#           \"type\": \"embedding\",\n",
    "#           \"embedding_dim\": 300,\n",
    "#           \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "#           \"trainable\": false\n",
    "#         },\n",
    "        \"elmo\": {\n",
    "          \"type\": \"elmo_token_embedder\",\n",
    "          \"options_file\": \"rsv_elmo/options.json\",\n",
    "          \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "          \"do_layer_norm\": false,\n",
    "          \"projection_dim\": 100,\n",
    "          \"dropout\": 0.0\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "          \"type\": \"character_encoding\",\n",
    "            \"dropout\": 0.2,\n",
    "            \"embedding\": {\n",
    "              \"embedding_dim\": 20,\n",
    "              \"sparse\": false,\n",
    "              \"vocab_namespace\": \"token_characters\"\n",
    "          },\n",
    "          \"encoder\": {\n",
    "            \"type\": \"gru\",\n",
    "            \"input_size\": $.model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim,\n",
    "            \"hidden_size\": LSTM_ENCODER_HIDDEN,\n",
    "            \"num_layers\": 1,\n",
    "            \"bidirectional\": true,\n",
    "          },\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"matcher_word\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": $.model.text_field_embedder.token_embedders.elmo.projection_dim+LSTM_ENCODER_HIDDEN*2,\n",
    "      \"num_perspectives\": 10,\n",
    "      \"with_full_match\": false\n",
    "    },\n",
    "    \"encoder1\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": $.model.text_field_embedder.token_embedders.elmo.projection_dim+LSTM_ENCODER_HIDDEN*2,\n",
    "      \"hidden_size\": 50,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward1\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 50,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward1\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 50,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"encoder2\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": $.model.matcher_forward1.hidden_dim*2,\n",
    "      \"hidden_size\": 50,\n",
    "      \"num_layers\": 1\n",
    "    },\n",
    "    \"matcher_forward2\": {\n",
    "      \"is_forward\": true,\n",
    "      \"hidden_dim\": 50,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"matcher_backward2\": {\n",
    "      \"is_forward\": false,\n",
    "      \"hidden_dim\": 50,\n",
    "      \"num_perspectives\": 10\n",
    "    },\n",
    "    \"aggregator\":{\n",
    "      \"type\": \"lstm\",\n",
    "      \"bidirectional\": true,\n",
    "      \"input_size\": 264,\n",
    "      \"hidden_size\": LSTM_AGG_HIDDEN,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.1,\n",
    "    },\n",
    "    \"classifier_feedforward\": {\n",
    "      \"input_dim\": LSTM_AGG_HIDDEN*4+1+1,\n",
    "      \"num_layers\": 1,\n",
    "      \"hidden_dims\": [2],\n",
    "      \"activations\": [\"mish\"],\n",
    "      \"dropout\": [0.0]\n",
    "    },\n",
    "    \"initializer\": {\n",
    "      \"regexes\": [\n",
    "        [\".*linear_layers.*weight\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*linear_layers.*bias\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*weight_ih.*\", {\"type\": \"xavier_normal\"}],\n",
    "        [\".*weight_hh.*\", {\"type\": \"orthogonal\"}],\n",
    "        [\".*bias.*\", {\"type\": \"constant\", \"val\": 0}],\n",
    "        [\".*matcher.*match_weights.*\", {\"type\": \"kaiming_normal\"}]\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"data_loader\": {\n",
    "    \"type\": 'multiprocess',\n",
    "    \"max_instances_in_memory\": $.data_loader.batch_sampler.batch_size * 10,\n",
    "    \"batch_sampler\": {\n",
    "      \"type\": \"bucket\",\n",
    "      \"batch_size\": 2,\n",
    "      \"padding_noise\": 0.0,\n",
    "      \"sorting_keys\": [\"premise\"],\n",
    "    },\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 2,\n",
    "    \"grad_clipping\": 5.0,\n",
    "    \"validation_metric\": \"+f1\",\n",
    "    \"cuda_device\": 1,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"huggingface_adamw\",\n",
    "      \"lr\": LR\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/structure_predictor_params.json\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"type\": \"int\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"LSTM_ENCODER_HIDDEN\",\n",
    "      \"low\": 10,\n",
    "      \"high\": 20\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"int\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"LSTM_AGG_HIDDEN\",\n",
    "      \"low\": 25,\n",
    "      \"high\": 50\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"float\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"LR\",\n",
    "      \"low\": 2e-4,\n",
    "      \"high\": 2e-2,\n",
    "      \"log\": true\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! echo 'allennlp_optuna' >> models/.allennlp_plugins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/tune_structure_predictor.sh\n",
    "\n",
    "export METHOD=structure_predictor_bimpm\n",
    "export STUDY_NAME=structure_tuning_0\n",
    "mkdir optuna\n",
    "rm -r optuna/$METHOD\n",
    "mkdir optuna/$METHOD\n",
    "\n",
    "allennlp tune ${METHOD}/config_elmo_fasttext.jsonnet structure_predictor_params.json --serialization-dir optuna/$METHOD \\\n",
    "    --study-name $STUDY_NAME \\\n",
    "    --skip-if-exists \\\n",
    "    --metrics best_validation_f1 \\\n",
    "    --direction maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def collect_optuna_results(path):\n",
    "    for trial in glob.glob(os.path.join(path, 'trial_*/')):\n",
    "        try:\n",
    "            metrics = json.load(open(os.path.join(trial, 'metrics.json')))\n",
    "            print(trial, metrics['best_validation_f1'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "collect_optuna_results('models/optuna/structure_predictor_bimpm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! mv models/optuna/structure_predictor_bimpm/trial_15 models/structure_predictor_bimpm/elmo_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r models/optuna/structure_predictor_bimpm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3. Scripts for training/prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Option 1. Directly from the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/train_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh train_structure_predictor.sh {bert|elmo} result_directory\n",
    "\n",
    "export METHOD=${1}\n",
    "export RESULT_DIR=${2}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "rm -r structure_predictor_bimpm/${RESULT_DIR}/\n",
    "allennlp train -s structure_predictor_bimpm/${RESULT_DIR}/ structure_predictor_bimpm/config_${METHOD}.json \\\n",
    "   --include-package bimpm_custom_package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Predict on dev&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/eval_structure_predictor.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_structure_predictor.sh {bert|elmo}\n",
    "\n",
    "export METHOD=${1}\n",
    "export DEV_FILE_PATH=\"structure_cf_dev.tsv\"\n",
    "export TEST_FILE_PATH=\"structure_cf_test.tsv\"\n",
    "\n",
    "\n",
    "allennlp predict --use-dataset-reader --cuda-device 0 --silent \\\n",
    "                 --output-file structure_predictor_bimpm/predictions_dev.json structure_predictor_bimpm/${METHOD}/model.tar.gz structure_predictor_bimpm/${DEV_FILE_PATH} \\\n",
    "                 --include-package bimpm_custom_package \\\n",
    "                 --predictor bimpm_custom_package.model.custom_bimpm_predictor.CustomBiMPMPredictor\n",
    "\n",
    "allennlp predict --use-dataset-reader --cuda-device 0 --silent \\\n",
    "                 --output-file structure_predictor_bimpm/predictions_test.json structure_predictor_bimpm/${METHOD}/model.tar.gz structure_predictor_bimpm/${TEST_FILE_PATH} \\\n",
    "                 --include-package bimpm_custom_package \\\n",
    "                 --predictor bimpm_custom_package.model.custom_bimpm_predictor.CustomBiMPMPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###  Evaluate classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_predictions(path, threshold=0.5):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            if threshold == 0.5:\n",
    "                result.append(json.loads(line)[\"label\"])\n",
    "            else:\n",
    "                result.append(int(json.loads(line)[\"probs\"][1] > threshold))\n",
    "            \n",
    "    result = list(map(int, result))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "RESULT_DIR = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = '../../models/structure_predictor_bimpm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls $MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "true = pd.read_csv(f'{MODEL_PATH}/structure_cf_dev.tsv', sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json', threshold=0.6)\n",
    "print('length of true labels:', len(true))\n",
    "print('length of prediction:', len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "true = pd.read_csv(f'{MODEL_PATH}/structure_cf_test.tsv', sep='\\t', header=None)[0].values.tolist()\n",
    "pred = load_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "print('length of true labels:', len(true))\n",
    "print('length of prediction:', len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('f1: %.2f'%(f1_score(true[:len(pred)], pred)*100))\n",
    "print('pr: %.2f'%(precision_score(true[:len(pred)], pred)*100))\n",
    "print('re: %.2f'%(recall_score(true[:len(pred)], pred)*100))\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %cd ../isanlp_rst\n",
    "# from classifier_wrappers import *\n",
    "# %cd ../maintenance/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_vocab = [0, 1]\n",
    "catboost_vocab = [0, 1]\n",
    "\n",
    "def load_neural_predictions(path):\n",
    "    result = []\n",
    "    \n",
    "    with open(path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            probs = json.loads(line)['probs']\n",
    "            probs = {model_vocab[i]: probs[i] for i in range(len(model_vocab))}\n",
    "            result.append(probs)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def load_scikit_predictions(model, X):\n",
    "    result = []\n",
    "    \n",
    "    try:\n",
    "        predictions = model.predict_proba(X)\n",
    "    except AttributeError:\n",
    "        predictions = model._predict_proba_lr(X)\n",
    "    \n",
    "    for prediction in predictions:\n",
    "        probs = {catboost_vocab[j]: prediction[j] for j in range(len(catboost_vocab))}\n",
    "        result.append(probs)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def vote_predictions(pred1, pred2, soft=True, weights=[1., 1.]):\n",
    "    assert len(pred1) == len(pred2)\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(pred1)):\n",
    "        sample_result = {}\n",
    "        for key in pred1[i].keys():\n",
    "            if soft:\n",
    "                sample_result[key] = (pred1[i][key]*weights[0] + pred2[i][key]*weights[1]) / 2.\n",
    "            else:\n",
    "                sample_result[key] = max(pred1[i][key], pred2[i][key])\n",
    "        \n",
    "        result.append(sample_result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def probs_to_classes(pred):\n",
    "    result = []\n",
    "    \n",
    "    for sample in pred:\n",
    "        best_class = ''\n",
    "        best_prob = 0.\n",
    "        for key in sample.keys():\n",
    "            if sample[key] > best_prob:\n",
    "                best_prob = sample[key]\n",
    "                best_class = key\n",
    "        \n",
    "        result.append(best_class)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model = pickle.load(open('../../models/structure_predictor_baseline/model.pkl', 'rb'))\n",
    "scaler = pickle.load(open('../../models/structure_predictor_baseline/scaler.pkl', 'rb'))\n",
    "drop_columns = pickle.load(open('../../models/structure_predictor_baseline/drop_columns.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IN_PATH = 'data_structure'\n",
    "\n",
    "train_samples = pd.read_pickle(os.path.join(IN_PATH, 'train_samples.pkl'))\n",
    "dev_samples = pd.read_pickle(os.path.join(IN_PATH, 'dev_samples.pkl'))\n",
    "test_samples = pd.read_pickle(os.path.join(IN_PATH, 'test_samples.pkl'))\n",
    "\n",
    "y_train, X_train = train_samples['relation'].to_frame(), train_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_dev, X_dev = dev_samples['relation'].to_frame(), dev_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])\n",
    "y_test, X_test = test_samples['relation'].to_frame(), test_samples.drop('relation', axis=1).drop(\n",
    "    columns=drop_columns + ['category_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_scaled_np = scaler.transform(X_dev)\n",
    "X_dev = pd.DataFrame(X_scaled_np, index=X_dev.index)\n",
    "\n",
    "X_scaled_np = scaler.transform(X_test)\n",
    "X_test = pd.DataFrame(X_scaled_np, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "TARGET = 'relation'\n",
    "svm_predictions = load_scikit_predictions(model, X_dev)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, weights=[0.9, 0.8])\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_dev, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_dev, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_dev, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_dev, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred1 = [_[0] for _ in tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "min(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(neural_predictions) == len(svm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Statistics of greedy span prediction\n",
    "use it for document-level threshold adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp_only_0 = [_ for _ in tmp if _[1] <= _[0]]\n",
    "pred1 = [_[1] for _ in tmp_only_0]\n",
    "print(pd.Series(pred1).describe())\n",
    "pd.Series(pred1).plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp_only_1 = [_ for _ in tmp if _[1] > _[0]]\n",
    "pred1 = [_[1] for _ in tmp_only_1]\n",
    "print(pd.Series(pred1).describe())\n",
    "pd.Series(pred1).plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_samples[(np.array(ensemble_pred) == 1) & dev_samples.category_id.isna()].sample()[['snippet_x', 'snippet_y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top_f1 = .5\n",
    "\n",
    "for w0 in np.arange(0.001, 1., 0.1):\n",
    "    for w1 in np.arange(0.001, 1., 0.1):\n",
    "        tmp = vote_predictions(neural_predictions, svm_predictions, soft=True, weights=[w0, w1])\n",
    "        ensemble_pred = probs_to_classes(tmp)\n",
    "        f1 = metrics.f1_score(y_dev.values, ensemble_pred, average='macro')\n",
    "        if f1 > top_f1:\n",
    "            print(np.round(w0, 2), np.round(w1, 2), '---------------------------------------------------')\n",
    "            print('f1: %.2f'%(metrics.f1_score(y_dev, ensemble_pred)*100.))\n",
    "            print('pr: %.2f'%(metrics.precision_score(y_dev, ensemble_pred)*100.))\n",
    "            print('re: %.2f'%(metrics.recall_score(y_dev, ensemble_pred)*100.))\n",
    "            print()\n",
    "            print(metrics.classification_report(y_dev, ensemble_pred, digits=4))\n",
    "            top_f1 = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_predictions = load_scikit_predictions(model, X_test)\n",
    "neural_predictions = load_neural_predictions(f'{MODEL_PATH}/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "tmp = vote_predictions(neural_predictions, svm_predictions, weights=[0.9, 0.8])\n",
    "ensemble_pred = probs_to_classes(tmp)\n",
    "\n",
    "print('f1: %.2f'%(metrics.f1_score(y_test, ensemble_pred)*100.))\n",
    "print('pr: %.2f'%(metrics.precision_score(y_test, ensemble_pred)*100.))\n",
    "print('re: %.2f'%(metrics.recall_score(y_test, ensemble_pred)*100.))\n",
    "print()\n",
    "print(metrics.classification_report(y_test, ensemble_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp_only_0 = [_ for _ in tmp if _[1] <= _[0]]\n",
    "pred1 = [_[1] for _ in tmp_only_0]\n",
    "print(pd.Series(pred1).describe())\n",
    "pd.Series(pred1).plot(kind='density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tmp_only_1 = [_ for _ in tmp if _[1] > _[0]]\n",
    "pred1 = [_[1] for _ in tmp_only_1]\n",
    "print(pd.Series(pred1).describe())\n",
    "pd.Series(pred1).plot(kind='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Just for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! cp -r ../../../_isanlp_rst/rsv_elmo ../../models/rsv_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ln -s -r ../../models/rsv_elmo rsv_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -laht ../../models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r ../../models/rvs_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "clf = Predictor.from_path('../../models/structure_predictor_bimpm/elmo_ft/model.tar.gz', cuda_device=-1, \n",
    "                          predictor_name='models.bimpm_custom_package.model.custom_bimpm_predictor.CustomBiMPMPredictor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.predict(premise='Сейчас я думаю ,', hypothesis='что это в моих интересах ,', same_sentence='1', same_paragraph='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf.predict(premise='что это в моих интересах ,', hypothesis='потому что ну сколько можно .', same_sentence='1', same_paragraph='1')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}