{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary structure classification used in tree building: Step 1. Negative samples generation\n",
    "\n",
    "Create train and test sets; Save negative samples of file ``filename.rs3`` as `filename.neg`\n",
    "\n",
    "Output:\n",
    " - ``data/*.neg``\n",
    " - ``data_structure/*``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "from _isanlp_rst.src.isanlp_rst.rst_tree_predictor import RSTTreePredictor, GoldTreePredictor\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "from utils.file_reading import *\n",
    "from utils.print_tree import printBTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomNegativeGenerator(object):\n",
    "    def __call__(self, edus, corpus, annot_text):\n",
    "        new_set = self.create_training_set(edus, corpus)\n",
    "        result = []\n",
    "        for item in new_set:\n",
    "            result.append((filename, item[0], item[1], item[2]))\n",
    "\n",
    "        tmp = pd.DataFrame(result, columns=['filename', 'snippet_x', 'snippet_y', 'relation'])\n",
    "\n",
    "        def place_locations(row):\n",
    "            row['loc_x'] = annot_text.find(row.snippet_x)\n",
    "            row['loc_y'] = annot_text.find(row.snippet_y, row['loc_x'] + 1)\n",
    "            return row\n",
    "\n",
    "        return tmp.apply(place_locations, axis=1)\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'RandomNegativeGenerator'\n",
    "    \n",
    "    def create_training_set(self, edus, gold):\n",
    "        training_set = []\n",
    "        \n",
    "        snippet_cache = []\n",
    "        for num, e in enumerate(gold.index):\n",
    "            snippet_x = gold.loc[e, 'snippet_x']\n",
    "            cache_x = self.extract_snippet_ids(snippet_x, edus)\n",
    "\n",
    "            snippet_y = gold.loc[e, 'snippet_y']\n",
    "            cache_y = self.extract_snippet_ids(snippet_y, edus)\n",
    "\n",
    "            if cache_x and cache_y:\n",
    "                snippet_cache.append((cache_x, snippet_x))\n",
    "                snippet_cache.append((cache_y, snippet_y))\n",
    "\n",
    "        for i in range(len(edus) - 1):\n",
    "            if not self.check_snippet_pair_in_dataset(gold, edus[i], edus[i+1]):\n",
    "                training_set.append((edus[i], edus[i+1], False))\n",
    "\n",
    "        for i in gold.index:\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_x'])\n",
    "            training_set += self.extract_negative_samples_for_snippet(gold, edus, gold.loc[i, 'snippet_y'])\n",
    "\n",
    "        for i in range(len(snippet_cache)):\n",
    "            for j in range(i, len(snippet_cache)):\n",
    "                cache_i, snippet_i = snippet_cache[i]\n",
    "                cache_j, snippet_j = snippet_cache[j]\n",
    "\n",
    "                if cache_i[-1] + 1 == cache_j[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_i, snippet_j):\n",
    "                        training_set.append((snippet_i, snippet_j, False))\n",
    "\n",
    "                if cache_j[-1] + 1 == cache_i[0]:\n",
    "                    if not self.check_snippet_pair_in_dataset(gold, snippet_j, snippet_i):\n",
    "                        training_set.append((snippet_j, snippet_i, False))\n",
    "\n",
    "        return list(set(training_set))\n",
    "    \n",
    "    def extract_snippet_ids(self, snippet, edus):\n",
    "        return [edu_nm for edu_nm, edu in enumerate(edus) if (edu in snippet)]\n",
    "    \n",
    "    def check_snippet_pair_in_dataset(self, dataset, snippet_left, snippet_right):\n",
    "        return ((((dataset.snippet_x == snippet_left) & (dataset.snippet_y == snippet_right)).sum(axis=0) != 0) \n",
    "                or ((dataset.snippet_y == snippet_left) & (dataset.snippet_x == snippet_right)).sum(axis=0) != 0)\n",
    "    \n",
    "    def extract_negative_samples_for_snippet(self, gold, edus, snippet):\n",
    "        training_set = []\n",
    "\n",
    "        snippet_ids = self.extract_snippet_ids(snippet, edus)\n",
    "\n",
    "        if not snippet_ids:\n",
    "            return []\n",
    "\n",
    "        if snippet_ids[0] > 0:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[0] - 1]):\n",
    "                training_set.append((edus[snippet_ids[0] - 1], snippet, False))\n",
    "\n",
    "        if snippet_ids[-1] < len(edus) - 1:\n",
    "            if not self.check_snippet_pair_in_dataset(gold, snippet, edus[snippet_ids[-1] + 1]):\n",
    "                training_set.append((snippet, edus[snippet_ids[-1] + 1], False))\n",
    "\n",
    "        return training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GreedyNegativeGenerator:\n",
    "    \"\"\" Inversed greedy parser based on gold tree predictor. \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.forest_threshold = 0.01\n",
    "        self._same_sentence_bonus = 0\n",
    "\n",
    "    def __call__(self, edus, corpus,\n",
    "                 annot_text, annot_tokens,\n",
    "                 annot_sentences,\n",
    "                 annot_lemma, annot_morph, annot_postag,\n",
    "                 annot_syntax_dep_tree):\n",
    "\n",
    "        def to_merge(scores):\n",
    "            return np.argmax(np.array(scores))\n",
    "\n",
    "        negative_nodes = []\n",
    "\n",
    "        self.tree_predictor = GoldTreePredictor(corpus)\n",
    "        nodes = edus\n",
    "        max_id = edus[-1].id\n",
    "\n",
    "        # initialize scores\n",
    "        features = self.tree_predictor.initialize_features(nodes,\n",
    "                                                           annot_text, annot_tokens,\n",
    "                                                           annot_sentences,\n",
    "                                                           annot_lemma, annot_morph, annot_postag,\n",
    "                                                           annot_syntax_dep_tree)\n",
    "\n",
    "        scores = self.tree_predictor.predict_pair_proba(features, _same_sentence_bonus=self._same_sentence_bonus)\n",
    "\n",
    "        for i, score in enumerate(scores):\n",
    "            if score == 0:\n",
    "                negative_nodes.append(\n",
    "                    DiscourseUnit(\n",
    "                        id=None,\n",
    "                        left=nodes[i],\n",
    "                        right=nodes[i + 1],\n",
    "                        relation='no_relation',\n",
    "                        nuclearity='NN',\n",
    "                        proba=score,\n",
    "                        text=annot_text[nodes[i].start:nodes[i + 1].end].strip()\n",
    "                    ))\n",
    "\n",
    "        while len(nodes) > 2 and any([score > self.forest_threshold for score in scores]):\n",
    "            # select two nodes to merge\n",
    "            j = to_merge(scores)  # position of the pair in list\n",
    "\n",
    "            # make the new node by merging node[j] + node[j+1]\n",
    "            relation = self.tree_predictor.predict_label(features.iloc[j])\n",
    "            relation, nuclearity = relation.split('_')\n",
    "\n",
    "            temp = DiscourseUnit(\n",
    "                id=max_id + 1,\n",
    "                left=nodes[j],\n",
    "                right=nodes[j + 1],\n",
    "                relation=relation,\n",
    "                nuclearity=nuclearity,\n",
    "                proba=scores[j],\n",
    "                text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "            )\n",
    "\n",
    "            max_id += 1\n",
    "\n",
    "            # modify the node list\n",
    "            nodes = nodes[:j] + [temp] + nodes[j + 2:]\n",
    "\n",
    "            # modify the scores list\n",
    "            if j == 0:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j], nodes[j + 1],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = _scores + scores[j + 2:]\n",
    "                features = pd.concat([_features, features.iloc[j + 2:]])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            elif j + 1 < len(nodes):\n",
    "                _features = self.tree_predictor.initialize_features([nodes[j - 1], nodes[j], nodes[j + 1]],\n",
    "                                                                    annot_text, annot_tokens,\n",
    "                                                                    annot_sentences,\n",
    "                                                                    annot_lemma, annot_morph, annot_postag,\n",
    "                                                                    annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                features = pd.concat([features.iloc[:j - 1], _features, features.iloc[j + 2:]])\n",
    "                scores = scores[:j - 1] + _scores + scores[j + 2:]\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[0],\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "                if _scores[1] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j],\n",
    "                            right=nodes[j + 1],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores[1],\n",
    "                            text=annot_text[nodes[j].start:nodes[j + 1].end].strip()\n",
    "                        ))\n",
    "\n",
    "            else:\n",
    "                _features = self.tree_predictor.extract_features(nodes[j - 1], nodes[j],\n",
    "                                                                 annot_text, annot_tokens,\n",
    "                                                                 annot_sentences,\n",
    "                                                                 annot_lemma, annot_morph, annot_postag,\n",
    "                                                                 annot_syntax_dep_tree)\n",
    "\n",
    "                _scores = self.tree_predictor.predict_pair_proba(_features,\n",
    "                                                                 _same_sentence_bonus=self._same_sentence_bonus)\n",
    "                scores = scores[:j - 1] + _scores\n",
    "                features = pd.concat([features.iloc[:j - 1], _features])\n",
    "\n",
    "                if _scores[0] == 0:\n",
    "                    negative_nodes.append(\n",
    "                        DiscourseUnit(\n",
    "                            id=None,\n",
    "                            left=nodes[j - 1],\n",
    "                            right=nodes[j],\n",
    "                            relation='no_relation',\n",
    "                            nuclearity='NN',\n",
    "                            proba=_scores,\n",
    "                            text=annot_text[nodes[j - 1].start:nodes[j].end].strip()\n",
    "                        ))\n",
    "\n",
    "        return negative_nodes\n",
    "\n",
    "    def __name__(self):\n",
    "        return 'GreedyNegativeGenerator'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make negative samples, save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7661c39785064b76a72cb5594aeaeec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen1 = RandomNegativeGenerator()\n",
    "# gen2 = GreedyNegativeGenerator()\n",
    "\n",
    "for filename in tqdm(glob.glob('./data/*.gold.pkl')):\n",
    "    filename = filename.replace('.gold.pkl', '')\n",
    "    df = read_gold(filename, features=True)\n",
    "    edus = read_edus(filename)\n",
    "    annot = read_annotation(filename)\n",
    "\n",
    "    tmp = gen1(edus, df, annot['text'])\n",
    "#     tmp1 = gen1(edus, df, annot['text'])\n",
    "    \n",
    "#     _edus = []\n",
    "#     last_end = 0\n",
    "#     last_id = 0\n",
    "#     for max_id in range(len(edus)):\n",
    "#         start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "#         end = start + len(edus[max_id])\n",
    "#         temp = DiscourseUnit(\n",
    "#             id=max_id + last_id,\n",
    "#             left=None,\n",
    "#             right=None,\n",
    "#             relation='edu',\n",
    "#             start=start,\n",
    "#             end=end,\n",
    "#             orig_text=annot['text'],\n",
    "#             proba=1.,\n",
    "#             )\n",
    "#         _edus.append(temp)\n",
    "#         last_end = end\n",
    "#         last_id += 1\n",
    "\n",
    "#     tmp = gen2(_edus, df, annot['text'],\n",
    "#                annot['tokens'], annot['sentences'],\n",
    "#                annot['lemma'], annot['morph'],\n",
    "#                annot['postag'], annot['syntax_dep_tree'])\n",
    "    \n",
    "#     tmp = pd.DataFrame(extr_pairs_forest(tmp, annot['text'], locations=True), \n",
    "#                        columns=['snippet_x', 'snippet_y', \n",
    "#                                 'category_id', 'order', \n",
    "#                                 'loc_x', 'loc_y'])\n",
    "    \n",
    "#     tmp = tmp[tmp.category_id == 'no_relation']\n",
    "#     tmp = tmp.drop(columns=['order', 'category_id'])\n",
    "#     tmp['filename'] = filename\n",
    "#     tmp['relation'] = False\n",
    "    \n",
    "#     tmp = pd.concat([tmp, tmp1])\n",
    "    tmp = tmp[(tmp.loc_x < tmp.loc_y) & (tmp.loc_x > -1)]\n",
    "    \n",
    "    tt = pd.concat([df, tmp])\n",
    "    tt['relation'] = tt.relation.map(lambda row: False if row == False else True)\n",
    "    tt = tt.sort_values('relation', ascending=False).drop_duplicates(\n",
    "        ['filename', 'snippet_x', 'snippet_y'])\n",
    "    tmp = tt[tt.relation == False]\n",
    "    \n",
    "    tmp.drop_duplicates(['snippet_x', 'snippet_y']).reset_index()[\n",
    "        ['filename', 'snippet_x', 'snippet_y', 'relation', 'loc_x', 'loc_y']].to_json(filename + '.json.neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.8 s, sys: 534 ms, total: 37.4 s\n",
      "Wall time: 37.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from _isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8e1efeb9f24ee490a9145bb46c50cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/233 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to locate first snippet >>> [['и запутались,' 'а ведь это далеко не все' 955 955 5443 5443]\n",
      " ['для проверки,'\n",
      "  'что описано в 12.2.6.9 Runtime Semantics: PropertуDefinitionEvaluation.'\n",
      "  913 913 5186 5186]\n",
      " ['AST-ановитесь!'\n",
      "  'На мой субъективный взгляд, по большей мере спецификация это предписание для интерпретатора EcmaScript, держать такой же в собственной голове есть дело тяжелое и неблагодарное.'\n",
      "  1414 1414 7913 7913]\n",
      " ['для ранее созданных тестов,' 'с помощью инструмента ASTEхplorer.' 1483\n",
      "  1483 8408 8409]\n",
      " ['и ставим точку.' 'Весь этот путь был проделан не зря,' 1045 1045 5975\n",
      "  5975]\n",
      " ['с помощью метода bind,'\n",
      "  'что описано в разделе 19.2.3.2 Function.prototуpe.bind,' 1361 1361\n",
      "  7628 7628]\n",
      " ['Вывод' 'Как мы выяснили функция, будучи анонимной, может иметь имя,'\n",
      "  1495 1495 8471 8471]\n",
      " ['Вывод'\n",
      "  'Как мы выяснили функция, будучи анонимной, может иметь имя, поскольку одновременно является также и объектом, что есть следствие мультипарадигмальной природы языка JavaScript.'\n",
      "  1495 1495 8471 8471]\n",
      " ['и запутались,'\n",
      "  'а ведь это далеко не все и я опустил часть перекрестных ссылок между разделами и пунктами спецификации.'\n",
      "  955 955 5443 5443]]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "MAX_LEN = 10000\n",
    "for filename in tqdm(glob.glob(\"data/*.json.neg\")):    \n",
    "    filename = filename.replace('.json.neg', '')\n",
    "    \n",
    "    df = read_negative(filename).drop(columns=['loc_y'])\n",
    "    df = df[df.snippet_x.str.len() > 0]\n",
    "    df = df[df.snippet_y.str.len() > 0]\n",
    "    \n",
    "    annotation = read_annotation(filename)\n",
    "        \n",
    "    result = features_processor(df,\n",
    "                                annotation['text'],\n",
    "                                annotation['tokens'],\n",
    "                                annotation['sentences'],\n",
    "                                annotation['lemma'],\n",
    "                                annotation['morph'],\n",
    "                                annotation['postag'],\n",
    "                                annotation['syntax_dep_tree'])\n",
    "    \n",
    "    result = result[result.is_broken == False]\n",
    "    \n",
    "    result = result[result.tokens_x.map(len) < MAX_LEN]\n",
    "    result = result[result.tokens_y.map(len) < MAX_LEN]\n",
    "    \n",
    "    result.to_pickle(filename + '.neg.features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train/test splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.5344827586206896,\tin dev: 0.6470588235294118,\tin test: 0.6086956521739131\n",
      "ling in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "comp in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "blog in train: 0.43103448275862066,\tin dev: 0.5294117647058824,\tin test: 0.4782608695652174\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709b413153e94762a9f51cd80c1bf006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/168 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa126f271e6a48cc93cd252356496884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260a27f28d84415c8e9b05aee1d0d6cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.file_reading import read_gold\n",
    "\n",
    "\n",
    "random_state = 45\n",
    "\n",
    "train_samples = []\n",
    "test_samples = []\n",
    "dev_samples = []\n",
    "\n",
    "for file in tqdm(train):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    train_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    train_samples.append(negative)\n",
    "\n",
    "for file in tqdm(dev):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    dev_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    dev_samples.append(negative)\n",
    "    \n",
    "for file in tqdm(test):\n",
    "    gold = read_gold(file.replace('.edus', ''), features=True)\n",
    "    gold['relation'] = 1\n",
    "    test_samples.append(gold)\n",
    "    negative = read_negative(file.replace('.edus', ''), features=True)\n",
    "    negative['relation'] = 0\n",
    "    test_samples.append(negative)\n",
    "\n",
    "train_samples = pd.concat(train_samples)\n",
    "dev_samples = pd.concat(dev_samples)\n",
    "test_samples = pd.concat(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.prepare_sequence import _prepare_sequence\n",
    "\n",
    "\n",
    "def correct_samples(row):\n",
    "    if row.snippet_x[0] in (',', '.', '!', '?'):\n",
    "        row.snippet_x = row.snippet_x[1:].strip()\n",
    "    if row.snippet_y[0] in (',', '.'):\n",
    "        row.snippet_x += row.snippet_y[0]\n",
    "        row.snippet_y = row.snippet_y[1:].strip()\n",
    "    return row\n",
    "\n",
    "def prepare_data(data, max_len=100):\n",
    "\n",
    "    data = data[data.tokens_x.map(len) < max_len]\n",
    "    data = data[data.tokens_y.map(len) < max_len]\n",
    "    \n",
    "    data['snippet_x'] = data.tokens_x.map(lambda row: ' '.join(row))\n",
    "    data['snippet_y'] = data.tokens_y.map(lambda row: ' '.join(row))\n",
    "    \n",
    "    data = data.apply(correct_samples, axis=1)\n",
    "    \n",
    "    data = data[data.snippet_x.map(len) > 0]\n",
    "    data = data[data.snippet_y.map(len) > 0]\n",
    "    \n",
    "    data['snippet_x'] = data.snippet_x.map(_prepare_sequence)\n",
    "    data['snippet_y'] = data.snippet_y.map(_prepare_sequence)\n",
    "    \n",
    "    data = data.sort_values(['relation'], ascending=True).drop_duplicates(['snippet_x', 'snippet_y'], keep='last')\n",
    "    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "train_samples = prepare_data(train_samples)\n",
    "dev_samples = prepare_data(dev_samples)\n",
    "test_samples = prepare_data(test_samples)\n",
    "\n",
    "OUT_PATH = 'data_structure'\n",
    "if not os.path.isdir(OUT_PATH):\n",
    "    os.path.mkdir(OUT_PATH)\n",
    "\n",
    "train_samples.to_pickle(os.path.join(OUT_PATH, 'train_samples.pkl'))\n",
    "dev_samples.to_pickle(os.path.join(OUT_PATH, 'dev_samples.pkl'))\n",
    "test_samples.to_pickle(os.path.join(OUT_PATH, 'test_samples.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snippet_x</th>\n",
       "      <th>snippet_y</th>\n",
       "      <th>relation</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6658</th>\n",
       "      <td>﻿Новость : Занятия в Воскресной школе</td>\n",
       "      <td>В каждое воскресение , утром , прихожане Свято...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19489</th>\n",
       "      <td>﻿Новость : Занятия в Воскресной школе</td>\n",
       "      <td>В каждое воскресение , утром , прихожане Свято...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41053</th>\n",
       "      <td>﻿Новость : Занятия в Воскресной школе</td>\n",
       "      <td>В каждое воскресение , утром , прихожане Свято...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/news2_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41443</th>\n",
       "      <td>😄️Игорь Морской😄️🇷🇺 ( @ 812 Hocke у ) April 18...</td>\n",
       "      <td>Что-то не очень-то разобрались</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/blogs_88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16720</th>\n",
       "      <td>😄️Игорь Морской😄️🇷🇺 ( @ 812 Hocke у ) April 18...</td>\n",
       "      <td>Что-то не очень-то разобрались ! Город в полно...</td>\n",
       "      <td>0</td>\n",
       "      <td>./data/blogs_88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               snippet_x  \\\n",
       "6658               ﻿Новость : Занятия в Воскресной школе   \n",
       "19489              ﻿Новость : Занятия в Воскресной школе   \n",
       "41053              ﻿Новость : Занятия в Воскресной школе   \n",
       "41443  😄️Игорь Морской😄️🇷🇺 ( @ 812 Hocke у ) April 18...   \n",
       "16720  😄️Игорь Морской😄️🇷🇺 ( @ 812 Hocke у ) April 18...   \n",
       "\n",
       "                                               snippet_y  relation  \\\n",
       "6658   В каждое воскресение , утром , прихожане Свято...         0   \n",
       "19489  В каждое воскресение , утром , прихожане Свято...         0   \n",
       "41053  В каждое воскресение , утром , прихожане Свято...         0   \n",
       "41443                     Что-то не очень-то разобрались         0   \n",
       "16720  Что-то не очень-то разобрались ! Город в полно...         0   \n",
       "\n",
       "              filename  \n",
       "6658    ./data/news2_2  \n",
       "19489   ./data/news2_2  \n",
       "41053   ./data/news2_2  \n",
       "41443  ./data/blogs_88  \n",
       "16720  ./data/blogs_88  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['😄️Игорь Морской😄️🇷🇺 ( @ 812 Hocke у ) April 18, 2019',\n",
       "        'Что-то не очень-то разобрались ! Город в полном запустении находится . Каналы и реки в парковках , трамваи в пробках стоят , поля стихийных паркингов , пылища и грязища . Это ползучее уничтожение города , люди даже не понимают , что что-то идет не так . Одно дерево спилили , другое кронировали , на третьей площади сделали паркинг , вокруг четвертой забор , пятую реку забросили и вот уже вместо культурной Северной Столицы куда полстраны приезжает на белые ночи , у нас фиг знает что',\n",
       "        0, './data/blogs_88']], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[['snippet_x', 'snippet_y', 'relation', 'filename']].sort_values('snippet_x').tail(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples['len_x'] = train_samples.snippet_x.map(lambda row: len(row.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30972\n",
       "1    16400\n",
       "Name: relation, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples.relation.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
