{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47f7b0f-644c-4c9c-94e6-2a75ff9ce74a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scripts for training and evaluation of the system described in Nguyen et al. on our data.\n",
    "\n",
    "<i>Nguyen, Thanh-Tung, et al. \"RST Parsing from Scratch.\" Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac584b7-9920-45fd-8628-f8fd64e3d720",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! cp ../isanlp_rst/td_rst_parser ."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94292aad-43c7-41a5-9214-288822ec4a2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.discourseunit2str import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3acc99c-2af4-4c5e-9f05-64f285f5de4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_rstreebank\n",
    "\n",
    "print('Loading RSTreebank:')\n",
    "train, dev, test = split_rstreebank('./data_ru')\n",
    "print('Train length:', len(train), 'Dev length:', len(dev), 'Test length:', len(test), '(files)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478cadce-fb1e-401b-8316-6fcce497e41a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "    {'tokens': 'tokens',\n",
    "     'sentences': 'sentences'}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5729878-9bbf-4408-8066-087a388cb2f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "packed_train, packed_dev, packed_test = dict(), dict(), dict()\n",
    "for key in ('InputDocs', 'EduBreak_TokenLevel', 'SentBreak', 'Docs_structure', 'filename'):\n",
    "    packed_train[key], packed_dev[key], packed_test[key] = [], [], []\n",
    "\n",
    "\n",
    "for file in tqdm(train + dev + test):\n",
    "    path_data = 'data_ru' if 'news' in file or 'blog' in file else 'dep_data'\n",
    "    path_du = 'corpus_du' if 'news' in file or 'blog' in file else 'dep_corpus_du'\n",
    "    pure_filename = file.split('/')[-1][:-5]\n",
    "    \n",
    "    for i in range(100):\n",
    "        filename = os.path.join(path_du, f'{pure_filename}_part_{i}.du')\n",
    "        if not os.path.isfile(filename): break\n",
    "        \n",
    "        trees = [pickle.load(open(filename, 'rb'))]\n",
    "        annot = ppl(trees[0].text)\n",
    "        edus = get_edu_breaks(trees, annot)\n",
    "\n",
    "        if len(edus) > 1:\n",
    "            if file in train:\n",
    "                packed_train['InputDocs'].append(get_input_docs(annot))\n",
    "                packed_train['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "                packed_train['SentBreak'].append(get_sentence_breaks(annot))\n",
    "                packed_train['Docs_structure'].append(get_docs_structure(trees))\n",
    "                packed_train['filename'].append(file)\n",
    "\n",
    "            elif file in dev:\n",
    "                packed_dev['InputDocs'].append(get_input_docs(annot))\n",
    "                packed_dev['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "                packed_dev['SentBreak'].append(get_sentence_breaks(annot))\n",
    "                packed_dev['Docs_structure'].append(get_docs_structure(trees))\n",
    "                packed_dev['filename'].append(file)\n",
    "\n",
    "            elif file in test:\n",
    "                packed_test['InputDocs'].append(get_input_docs(annot))\n",
    "                packed_test['EduBreak_TokenLevel'].append(get_edu_breaks(trees, annot))\n",
    "                packed_test['SentBreak'].append(get_sentence_breaks(annot))\n",
    "                packed_test['Docs_structure'].append(get_docs_structure(trees))\n",
    "                packed_test['filename'].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d30d84-a32e-4c5d-952a-1838715f49f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "processed_data_path = 'td_rst_parser/data'\n",
    "if not os.path.isdir(processed_data_path):\n",
    "    os.mkdir(processed_data_path)\n",
    "\n",
    "with open(os.path.join(processed_data_path, 'train_data'), 'wb') as f:\n",
    "\tpickle.dump(packed_train, f)\n",
    "    \n",
    "with open(os.path.join(processed_data_path, 'dev_data'), 'wb') as f:\n",
    "\tpickle.dump(packed_dev, f)\n",
    "\n",
    "with open(os.path.join(processed_data_path, 'test_data'), 'wb') as f:\n",
    "\tpickle.dump(packed_test, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c5e754-2749-43e1-821b-84dcbe419eb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for bugs, output must be empty\n",
    "for i, struct in enumerate(packed_train['Docs_structure']):\n",
    "    assert struct, f\"Check packed_train['EduBreak_TokenLevel'][{i}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95091724-cbc3-4088-af16-ba75b815155b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Word2vec: download, lowercase, remove postags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ad8b7a-cbc0-4152-9563-d9977c8acdac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# cd ./td_rst_parser/src/processed_data/\n",
    "# wget http://vectors.nlpl.eu/repository/20/220.zip\n",
    "# unzip 220.zip model.txt \n",
    "# mv model.txt w2v.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34137859-3ab0-4420-8eb9-b8273341bd61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('./td_rst_parser/data/w2v.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "vocab = []  # We won't use POS tags, so take only the first mention of the form in the w2v (the most frequent one)\n",
    "with open('./td_rst_parser/data/w2v.txt', 'w') as f:\n",
    "    for line in tqdm(lines):\n",
    "        new_line = line.strip()\n",
    "        key = new_line.split(' ')[0]\n",
    "        value = new_line.split(' ')[1:]\n",
    "        \n",
    "        if '_' in key:\n",
    "            key = key.split('_')[0]\n",
    "        \n",
    "        if not key in vocab:\n",
    "            vocab.append(key)\n",
    "            new_line = (key + '\\t' + ' '.join(value)).lower()    \n",
    "            f.write(new_line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224bf2a8-c46d-400e-ba26-228f6165e6c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'unknown' in vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511c4aaa-b62a-43ef-8341-74fb8fd84421",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c8bef7-b913-49a7-b8b3-c9f98e7fbe3b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile td_rst_parser/discourse_config.ini\n",
    "\n",
    "[Network]\n",
    ";batch_size=10000\n",
    ";n_embed=300\n",
    "min_freq=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7921fb-ea7c-4e26-bd0e-f2db4f35f42a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir td_rst_parser/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df14d6f-5d1d-4d03-9f84-e301bba7d689",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile td_rst_parser/run_discourse_doc_goldsegmentation_edu_rep_train.sh\n",
    "\n",
    "export DATA_PATH='./data'\n",
    "export MODE='train'\n",
    "export FEAT='char'\n",
    "export LEARNING_RATE_SCHEDULE='Exponential'\n",
    "export PRETRAINED_EMBEDDING='./data/w2v.txt'\n",
    "export N_EMBED=300\n",
    "\n",
    "export BERT_MODEL=''\n",
    "export BATCH_SIZE=4000  \n",
    "export BEAM_SIZE=20\n",
    "\n",
    "python -m src.cmds.pointing_discourse_gold_segmentation_edu_rep train -b -d 1 -p exp/ptb.pointing.discourse.gold_segmentation_edu_rep.$FEAT \\\n",
    "--data_path $DATA_PATH -f $FEAT --learning_rate_schedule $LEARNING_RATE_SCHEDULE \\\n",
    "--batch-size $BATCH_SIZE --conf 'discourse_config.ini' --n-embed $N_EMBED --unk 'unknown' \\\n",
    "--beam-size $BEAM_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabe086-5349-4d5f-bcb3-cd8e4cc990cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "cd td_rst_parser && sh run_discourse_doc_goldsegmentation_edu_rep_train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f072a8c4-4a9a-443e-b8f8-cbc27e286424",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Sandbox\n",
    " - Evaluate the selected model across documents\n",
    " - Quantize it and measure time and performance of two models; time optimization is crucial for RST parsing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683071ed-f183-43f1-918a-5bdbb1b62d08",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r src\n",
    "! ln -s td_rst_parser/src src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f5eee-4fd1-47e3-9bc6-45fe76a7f15a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "test = pickle.load(open('td_rst_parser/data/test_data', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ff7c7-1f75-46ea-8d9e-6819c670f54d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from td_rst_parser.predict_interactive import TrainedPredictor\n",
    "pr = TrainedPredictor('td_rst_parser/exp/ptb.pointing.discourse.gold_segmentation_edu_rep.char/2022_05_26_08_58_41/model_dev_UF_65.41_NF_43.00_RF_32.09.pt',\n",
    "                      device='cpu')  # quantized models work only on cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d100d3-f6c7-438f-80fd-d90d0c6b867c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7c4e4-a0e4-45b5-a7ce-e6be370d9eac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model_int8 = torch.quantization.quantize_dynamic(pr.parser.model.to('cpu'))\n",
    "pr.parser.model = model_int8\n",
    "pr.parser.save('quantized_model.pt')  # 80M vs 198M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af36995-ec2b-4672-ad9b-022f8ff0ee5f",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = pr.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9e66c-6601-4574-a0dc-769816321737",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def edu2tokens(tree, edu_breaks):\n",
    "    result = []\n",
    "    for node in tree:\n",
    "        left_begin, left_rel, border, right_rel, right_end = node[1:-1].split(':')\n",
    "        left_end, right_begin = border.split(',')\n",
    "        \n",
    "        if left_begin == '1':\n",
    "            left_begin_toks = 0\n",
    "        else:\n",
    "            left_begin_toks = edu_breaks[int(left_end)-2] + 1\n",
    "        \n",
    "        left_end_toks = edu_breaks[int(left_end)-1]\n",
    "        right_begin_toks = left_end_toks + 1\n",
    "        right_end_toks = edu_breaks[int(right_end)-1]\n",
    "        \n",
    "        result.append(f'({left_begin_toks}:{left_rel}:{left_end_toks},{right_begin_toks}:{right_rel}:{right_end_toks})')\n",
    "    return result\n",
    "\n",
    "golds = [edu2tokens(test['Docs_structure'][i], test['EduBreak_TokenLevel'][i]) for i in range(len(test['Docs_structure']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432f6a8-7b8d-4b07-bd24-c4ffbc7a66a0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.metrics import DiscourseMetricDoc\n",
    "\n",
    "metric = DiscourseMetricDoc()\n",
    "for i, tree in enumerate(golds):\n",
    "    pred = predictions['trees'][i].split(' ')\n",
    "    metric(golds=tree, preds=pred)\n",
    "    # print(metric)\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14871bcf-9d48-42b8-9d22-6ccd8f083276",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metric = DiscourseMetricDoc()\n",
    "metric(golds=[' '.join(tree) for tree in golds], preds=predictions['trees'])\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5de3e4-c432-4a83-b4f5-ac36f2f169fa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "FP32 (198MB, 28s):<br>\n",
    "UF: 56.91% NF: 39.26% RF: 30.08% Full RNF: 29.54% <br>\n",
    "UF: 56.04% NF: 38.78% RF: 29.70% Full RNF: 29.16% \n",
    "\n",
    "Int8 (82MB, 22s):<br>\n",
    "UF: 56.98% NF: 39.23% RF: 30.19% Full RNF: 29.65% <br>\n",
    "UF: 56.12% NF: 38.74% RF: 29.80% Full RNF: 29.27% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}