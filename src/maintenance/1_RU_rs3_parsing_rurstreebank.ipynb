{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config IPCompleter.use_jedi=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! /notebook/py39/bin/pip install -U git+https://github.com/tchewik/dis2du.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import xml\n",
    "\n",
    "from dis2du.read_dis import read_dis\n",
    "from dis2du.tree import RSTTree\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read RS3 files into isanlp.DiscourseUnit annotations\n",
    "input:\n",
    " - corpus with .rs3 files\n",
    "output:\n",
    " - ``corpus/file_filename_PART.du``  - pickled isanlp DiscourseUnit with tree number PART from the original *.rs3 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd corpora/\n",
    "rm -r RuRSTreebank_jul22/\n",
    "unzip -q RuRSTreebank_jul22.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Split dataset files into separated trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rs3_forest_splitter import RS3ForestSplitter\n",
    "\n",
    "splitter = RS3ForestSplitter()\n",
    "\n",
    "output_dir = 'data_ru/corpus_rs3'\n",
    "! rm -r data_ru\n",
    "! mkdir data_ru\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    os.mkdir(os.path.join(output_dir, 'train/'))\n",
    "    os.mkdir(os.path.join(output_dir, 'dev/'))\n",
    "    os.mkdir(os.path.join(output_dir, 'test/'))\n",
    "\n",
    "for part in ('train', 'dev', 'test'):\n",
    "    for corpus in ('news1', 'news2', 'blogs'):\n",
    "        for filename in tqdm(glob.glob(os.path.join('corpora', 'RuRSTreebank_jul22', corpus, part, '*.rs3'))):\n",
    "            splitter(filename, os.path.join(output_dir, part))\n",
    "    \n",
    "    for file in glob.glob(os.path.join(output_dir, part, '*.rs3')):\n",
    "        new_filename = part + '.' + os.path.basename(file)\n",
    "        with open(os.path.join(output_dir, new_filename), 'w') as f:\n",
    "            f.write(open(file, 'r').read())\n",
    "        os.remove(file)\n",
    "    \n",
    "for part in ('train', 'dev', 'test'):\n",
    "    os.rmdir(os.path.join(output_dir, part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht data_ru/corpus_rs3/train.* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht data_ru/corpus_rs3/dev.* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht data_ru/corpus_rs3/test.* | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### 2. Convert them all to *.dis files\n",
    "\n",
    "Using https://github.com/rst-workbench/rst-converter-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rst2dis_converter import split_seq, RST2DISConverter\n",
    "\n",
    "BASE_URL = 'localhost:5000'  # <- put rst converter address here\n",
    "THREADS = 10\n",
    "OUTPUT_DIR = 'data_ru/corpus_dis'\n",
    "\n",
    "if os.path.isdir(OUTPUT_DIR):\n",
    "    ! rm -r $OUTPUT_DIR\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# (!) Jupyter kernel does not indicate the connection with the multiprocess IO operations\n",
    "# keep watching on docker if necessary\n",
    "files = glob.glob(f'data_ru/corpus_rs3/*.rs3')\n",
    "for batch in split_seq(files, THREADS):\n",
    "    t = RST2DISConverter(BASE_URL, batch, output_dir=OUTPUT_DIR)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check overall number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -lath data_ru/corpus_dis/*.dis | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Replace ##### with other marker because the dis file reader will somehow ommit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in glob.glob(os.path.join('data_ru', 'corpus_dis', '*.dis')):\n",
    "    with open(file, 'r') as f:\n",
    "        tree_txt = f.read().replace('##### ', '_NEW_LINE_')\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(tree_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile('_!(.*)_!')\n",
    "\n",
    "for file in glob.glob(os.path.join('data_ru', 'corpus_dis', '*.dis')):\n",
    "    with open(file, 'r') as f:\n",
    "        tree_lines = f.readlines()\n",
    "    with open(file, 'w') as f:\n",
    "        for line in tree_lines:\n",
    "            if not 'IMG' in line:\n",
    "                f.write(line)\n",
    "            else:\n",
    "                f.write(pattern.sub('_!IMG_!', line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. Collect DiscourseUnit annotations for isanlp library\n",
    "\n",
    "output:\n",
    " - ``corpus_du/file.du`` - DiscourseUnit tree annotation\n",
    " - ``data/file.txt`` - Original text collected directly from the annotation\n",
    " - ``data/file.edus``  - Text file with edus from .rs3 - each line contains one edu\n",
    " - ``data/all_pairs.fth`` - All the relation pairs from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r data_ru/corpus_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.dataset.dis_file_reading import *\n",
    "\n",
    "input_dir = 'data_ru/corpus_dis'\n",
    "output_dir = 'data_ru/corpus_du'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "failed = []\n",
    "for file in tqdm(glob.glob(os.path.join(input_dir, '*.dis'))):\n",
    "    # try:\n",
    "    tree = read_dis(file, force_brackets=False)\n",
    "    output_file = file.split('/')[-1].replace('.dis', '.du')\n",
    "    with open(os.path.join(output_dir, output_file), 'wb') as f:\n",
    "        pickle.dump(tree, f)\n",
    "    # except:\n",
    "    #     failed.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(failed)  # Bugs in the annotation, number corresponds to the tree number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Collect text files and edus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r data_ru/data\n",
    "! mkdir data_ru/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extr_edus(tree):\n",
    "    if tree.relation == 'elementary':\n",
    "        return [tree.text]\n",
    "    else:\n",
    "        tt = []\n",
    "        tt += extr_edus(tree.left)\n",
    "        tt += extr_edus(tree.right)\n",
    "    return tt\n",
    "\n",
    "all_pairs = []\n",
    "LINES_DELIM = '_NEW_LINE_'\n",
    "for orig_filename in glob.glob(os.path.join('data_ru', 'corpus_du', '*part_0.du')):\n",
    "    text = ''\n",
    "    all_edus = []\n",
    "    for du_filename in sorted(glob.glob(orig_filename.replace('_0.du', '_*.du')),\n",
    "                             key=lambda x: float(re.findall(\"(\\d+)\",x)[-1])):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        edus = extr_edus(tree)\n",
    "        all_edus += edus\n",
    "        text += ' ' + ' '.join(edus)\n",
    "\n",
    "    filename = os.path.basename(orig_filename).replace('_part_0.du', '')\n",
    "\n",
    "    # Write EDUs\n",
    "    with open(os.path.join('data_ru', 'data', filename + '.edus'), 'w') as f:\n",
    "        f.write('\\n'.join([edu.strip().replace(LINES_DELIM, '') for edu in all_edus]))\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Write the text\n",
    "    text = text.replace(LINES_DELIM, '\\n')\n",
    "    with open(os.path.join('data_ru', 'data', filename + '.txt'), 'w') as f:\n",
    "        f.write(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Align trees with the original texts (collect ``start`` and ``end`` for each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def align_du2text(tree, text, start=None, end=None):\n",
    "    tree.text = tree.text.replace(LINES_DELIM, '\\n').strip()\n",
    "    \n",
    "    if start != None:\n",
    "        tree.start = start\n",
    "        tree.end = tree.start + len(tree.text)\n",
    "        \n",
    "    elif end != None:\n",
    "        tree.end = end\n",
    "        tree.start = tree.end - len(tree.text)\n",
    "        \n",
    "    if tree.relation != 'elementary':\n",
    "        tree.left = align_du2text(tree.left, text, start=tree.start)\n",
    "        tree.right = align_du2text(tree.right, text, end=tree.end)\n",
    "        \n",
    "    return tree\n",
    "\n",
    "\n",
    "def extr_pairs(tree, filename):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text,\n",
    "                   tree.left.start, tree.right.start,\n",
    "                   tree.relation, tree.nuclearity, filename])\n",
    "        pp += extr_pairs(tree.left, filename)\n",
    "        pp += extr_pairs(tree.right, filename)\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob(os.path.join('data_ru', 'data', '*.txt'))):\n",
    "    text = open(file, 'r').read()\n",
    "    filename = file.split('/')[-1].replace('.txt', '')\n",
    "\n",
    "    for du_filename in sorted(glob.glob(os.path.join('data_ru', 'corpus_du', filename + '_part_*'))):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        tree_text = tree.text.strip().replace(LINES_DELIM, '\\n').strip()\n",
    "        start = text.find(tree_text)\n",
    "        if start == -1:\n",
    "            print(du_filename)\n",
    "        tree = align_du2text(tree, text, start=start, end=start + len(tree_text))\n",
    "        all_pairs += extr_pairs(tree, filename=filename)\n",
    "        pickle.dump(tree, open(du_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(all_pairs,\n",
    "                     columns=['snippet_x', 'snippet_y', 'loc_x', 'loc_y', 'category_id', 'order', 'filename'])\n",
    "pairs.category_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = pairs.drop_duplicates()\n",
    "pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for the correction. All these operations should return zeros\n",
    "\n",
    "print(pairs[pairs.loc_x == -1].shape, pairs[pairs.loc_y == -1].shape)  # No relations not found in the text sources\n",
    "print(pairs[pairs.loc_x > pairs.loc_y].shape)  # No wrong matching with the text sources\n",
    "print(pairs[pairs.category_id.isna()].shape, pairs[pairs.category_id == 'span'].shape)  # No wrong parsed relation names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = pairs.category_id + '_' + pairs.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rename_relations import rename_relations\n",
    "\n",
    "pairs = rename_relations(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(pairs.relation.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs.reset_index().to_feather(os.path.join('data_ru', 'all_pairs.fth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - ``file.annot.pkl``  - Morphosyntactic annotation in isanlp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "\n",
    "\n",
    "host_spacy = ''  # <- set the hostname\n",
    "port_spacy = '3334'  # <- and the port\n",
    "\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "    {'tokens': 'tokens',\n",
    "     'sentences': 'sentences'}),\n",
    "    (ProcessorRemote(host_spacy, port_spacy, '0'),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'lemma': 'lemma',\n",
    "      'postag': 'postag',\n",
    "      'morph': 'morph',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'entities': 'entities'})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for file in tqdm(glob.glob(os.path.join('data_ru', 'data', '*.txt'))):\n",
    "    text = open(file, 'r').read()\n",
    "    filename = file.replace('.txt', '.annot.pkl')\n",
    "    annot = ppl(text)\n",
    "    pickle.dump(annot, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -laht data_ru/data/*.annot.pkl | wc -l\n",
    "ls -laht data_ru/data/*.edus | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Look at the sentence integrity in the corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_dus(tree):\n",
    "    result = [tree.text]\n",
    "    if tree.left:\n",
    "        result += get_dus(tree.left)\n",
    "        result += get_dus(tree.right)\n",
    "    return result\n",
    "\n",
    "def get_sentences_and_dus(filename):\n",
    "    annot = pickle.load(open(filename, 'rb'))\n",
    "    docname = os.path.basename(filename).replace('.annot.pkl', '')\n",
    "    \n",
    "    # Collect discourse units as texts\n",
    "    dus = []\n",
    "    for i in range(100):\n",
    "        new_filename = os.path.join('data_ru', 'corpus_du', f'{docname}_part_{i}.du')\n",
    "        if not os.path.isfile(new_filename):\n",
    "            # print(new_filename)\n",
    "            continue\n",
    "        tree = pickle.load(open(new_filename, 'rb'))\n",
    "        dus += get_dus(tree)\n",
    "\n",
    "    dus_chr = [''.join(text.split()) for text in dus]\n",
    "    \n",
    "    # Collect sentences as texts\n",
    "    sentences = [''.join([token.text for token in annot['tokens'][sent.begin:sent.end]]) for sent in annot['sentences']]\n",
    "    \n",
    "    return sentences, dus_chr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, dus = [], []\n",
    "for filename in tqdm(glob.glob('data_ru/data/*.annot.pkl')):\n",
    "    snt, chrdus = get_sentences_and_dus(filename)\n",
    "    sentences += snt\n",
    "    dus += chrdus\n",
    "\n",
    "results = sum([sentence in dus for sentence in tqdm(sentences)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results / len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - ``models/tf_idf/pipeline.pkl``  - Is used in default feature extraction\n",
    " - ``data_ru/file.gold.pkl``  - Dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1. Load sentiment models, install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# source /notebook/py39/bin/activate\n",
    "# # python -c \"import nltk; nltk.download('stopwords')\"\n",
    "# # pip install dostoevsky\n",
    "# # dostoevsky download fasttext-social-network-model\n",
    "# pip install textblob tensorflow tensorflow_hub tensorflow_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2. Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../')\n",
    "from features_processors import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(language='ru', verbose=0, use_use=True, use_sentiment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "table = pd.read_feather(os.path.join('data_ru', 'all_pairs.fth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename, df in tqdm(table.groupby('filename')):\n",
    "    annot = pickle.load(open(os.path.join('data_ru', 'data', filename + '.annot.pkl'), 'rb'))\n",
    "    features = features_processor(df,\n",
    "                                  annot['text'], annot['tokens'],\n",
    "                                  annot['sentences'], annot['lemma'],\n",
    "                                  annot['morph'], annot['postag'],\n",
    "                                  annot['syntax_dep_tree'],)\n",
    "    del features['level_0']\n",
    "    features.to_pickle(os.path.join('data_ru', 'data', filename + '.gold.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
