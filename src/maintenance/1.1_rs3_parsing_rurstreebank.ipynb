{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U git+https://github.com/tchewik/dis2du.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import re\n",
    "import pickle\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "import xml\n",
    "\n",
    "from dis2du.read_dis import read_dis\n",
    "from dis2du.tree import RSTTree\n",
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Read RS3 files into isanlp.DiscourseUnit annotations\n",
    "input:\n",
    " - corpus with .rs3 files\n",
    "output:\n",
    " - ``corpus/file_filename_PART.du``  - pickled isanlp DiscourseUnit with tree number PART from the original *.rs3 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# cd corpus/\n",
    "# unzip RuRSTreebank_7.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1. Split dataset files into separated trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rs3_forest_splitter import RS3ForestSplitter\n",
    "\n",
    "splitter = RS3ForestSplitter()\n",
    "\n",
    "output_dir = 'corpus_rs3'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for filename in tqdm(glob.glob('corpus/RuRSTreebank_7/news1_rs3/*.rs3')):\n",
    "    splitter(filename, output_dir)\n",
    "\n",
    "for filename in tqdm(glob.glob('corpus/RuRSTreebank_7/news2_rs3/*.rs3')):\n",
    "    splitter(filename, output_dir)\n",
    "\n",
    "for filename in tqdm(glob.glob('corpus/RuRSTreebank_7/blogs_rs3/*.rs3')):\n",
    "    splitter(filename, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -laht $output_dir/*.rs3 | wc -l  # Overall number of trees in news+blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 2. Convert them all to *.dis files\n",
    "\n",
    "Using https://github.com/rst-workbench/rst-converter-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rst2dis_converter import split_seq, RST2DISConverter\n",
    "\n",
    "BASE_URL = 'localhost:5000'  # <- put rst converter address here\n",
    "THREADS = 10\n",
    "OUTPUT_DIR = 'corpus_dis'\n",
    "\n",
    "if os.path.isdir(OUTPUT_DIR):\n",
    "    ! rm -r $OUTPUT_DIR\n",
    "os.mkdir(OUTPUT_DIR)\n",
    "\n",
    "# (!) Jupyter kernel does not indicate the connection with the multiprocess IO operations\n",
    "# keep watching on docker if necessary\n",
    "files = glob.glob(f'corpus_rs3/*.rs3')\n",
    "for batch in split_seq(files, THREADS):\n",
    "    t = RST2DISConverter(BASE_URL, batch, output_dir=OUTPUT_DIR)\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check overall number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls -lath corpus_dis/*.dis | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Replace ##### with other marker because the dis file reader will somehow ommit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in glob.glob(os.path.join('corpus_dis', '*.dis')):\n",
    "    with open(file, 'r') as f:\n",
    "        tree_txt = f.read().replace('##### ', '_NEW_LINE_')\n",
    "    with open(file, 'w') as f:\n",
    "        f.write(tree_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 3. Collect DiscourseUnit annotations for isanlp library\n",
    "\n",
    "output:\n",
    " - ``corpus_du/file.du`` - DiscourseUnit tree annotation\n",
    " - ``data/file.txt`` - Original text collected directly from the annotation\n",
    " - ``data/file.edus``  - Text file with edus from .rs3 - each line contains one edu\n",
    " - ``data/all_pairs.fth`` - All the relation pairs from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r corpus_du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset.dis_file_reading import *\n",
    "\n",
    "input_dir = 'corpus_dis'\n",
    "output_dir = 'corpus_du'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "failed = []\n",
    "for file in tqdm(glob.glob(os.path.join(input_dir, '*.dis'))):\n",
    "\n",
    "    try:\n",
    "        tree = read_dis(file)\n",
    "        output_file = file.split('/')[-1].replace('.dis', '.du')\n",
    "        with open(os.path.join(output_dir, output_file), 'wb') as f:\n",
    "            pickle.dump(tree, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sorted(failed)  # Bugs in the annotation, number corresponds to the tree number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Collect text files and edus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! rm -r data_ru && mkdir data_ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extr_edus(tree):\n",
    "    if tree.relation == 'elementary':\n",
    "        return [tree.text]\n",
    "    else:\n",
    "        tt = []\n",
    "        tt += extr_edus(tree.left)\n",
    "        tt += extr_edus(tree.right)\n",
    "    return tt\n",
    "\n",
    "all_pairs = []\n",
    "LINES_DELIM = '_NEW_LINE_'\n",
    "for orig_filename in glob.glob(os.path.join('corpus_du/', '*part_0.du')):\n",
    "    text = ''\n",
    "    all_edus = []\n",
    "    for du_filename in sorted(glob.glob(orig_filename.replace('_0.du', '_*.du')),\n",
    "                             key=lambda x: float(re.findall(\"(\\d+)\",x)[-1])):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        edus = extr_edus(tree)\n",
    "        all_edus += edus\n",
    "        text += ' ' + ' '.join(edus)\n",
    "\n",
    "    filename = orig_filename.replace('corpus_du/', '').replace('_part_0.du', '')\n",
    "\n",
    "    # Write EDUs\n",
    "    with open(os.path.join('data_ru/', filename + '.edus'), 'w') as f:\n",
    "        f.write('\\n'.join([edu.strip().replace(LINES_DELIM, '') for edu in all_edus]))\n",
    "        f.write('\\n')\n",
    "\n",
    "    # Write the text\n",
    "    text = text.replace(LINES_DELIM, '\\n')\n",
    "    with open(os.path.join('data_ru/', filename + '.txt'), 'w') as f:\n",
    "        f.write(text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Align trees with the original texts (collect ``start`` and ``end`` for each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extr_pairs(tree, filename):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text,\n",
    "                   tree.left.start, tree.right.start,\n",
    "                   tree.relation, tree.nuclearity, filename])\n",
    "        pp += extr_pairs(tree.left, filename)\n",
    "        pp += extr_pairs(tree.right, filename)\n",
    "    return pp\n",
    "\n",
    "def align_du2text(tree, text, begin=0):\n",
    "    tree.text = tree.text.replace(LINES_DELIM, '\\n').strip()\n",
    "    tree.start = text.find(tree.text, begin)\n",
    "    tree.end = tree.start + len(tree.text)\n",
    "    if tree.relation != 'elementary':\n",
    "        tree.left = align_du2text(tree.left, text)\n",
    "        tree.right = align_du2text(tree.right, text, tree.left.end)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob('data_ru/*.txt')):\n",
    "    text = open(file, 'r').read()\n",
    "    filename = file.split('/')[-1].replace('.txt', '')\n",
    "\n",
    "    for du_filename in sorted(glob.glob(os.path.join('corpus_du/', filename + '_part_*'))):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        tree = align_du2text(tree, text)\n",
    "        all_pairs += extr_pairs(tree, filename=filename)\n",
    "        pickle.dump(tree, open(du_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(all_pairs,\n",
    "                     columns=['snippet_x', 'snippet_y', 'loc_x', 'loc_y', 'category_id', 'order', 'filename'])\n",
    "pairs.category_id.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = pairs.drop_duplicates()\n",
    "pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Check for the correction. All these operations should return zeros\n",
    "\n",
    "print(pairs[pairs.loc_x == -1].shape, pairs[pairs.loc_y == -1].shape)  # No relations not found in the text sources\n",
    "print(pairs[pairs.loc_x > pairs.loc_y].shape)  # No wrong matching with the text sources\n",
    "print(pairs[pairs.category_id.isna()].shape, pairs[pairs.category_id == 'span'].shape)  # No wrong parsed relation names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = pairs.category_id + '_' + pairs.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.dataset.rename_relations import rename_relations\n",
    "\n",
    "pairs = rename_relations(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(pairs.relation.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs.reset_index().to_feather('data_ru/all_pairs.fth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - ``file.annot.pkl``  - Morphosyntactic annotation in isanlp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "\n",
    "\n",
    "host_udpipe = ''  # <- set the hostname\n",
    "port_udpipe = '3134'  # <- and the port\n",
    "\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "    {'tokens': 'tokens',\n",
    "     'sentences': 'sentences'}),\n",
    "    (ProcessorRemote(host_udpipe, port_udpipe, '0'),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'lemma': 'lemma',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'postag': 'ud_postag'}),\n",
    "    (ProcessorMystem(delay_init=False),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'postag': 'postag'}),\n",
    "    (ConverterMystemToUd(),\n",
    "     ['postag'],\n",
    "     {'morph': 'morph',\n",
    "      'postag': 'postag'}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "for file in tqdm(glob.glob(f'data_ru/*.txt')):\n",
    "    text = open(file, 'r').read()\n",
    "    filename = file.replace('.txt', '.annot.pkl')\n",
    "    annot = ppl(text)\n",
    "    pickle.dump(annot, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "ls -laht data_ru/*.pkl | wc -l\n",
    "ls -laht data_ru/*.edus | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - ``models/tf_idf/pipeline.pkl``  - Is used in default feature extraction\n",
    " - ``data_ru/file.gold.pkl``  - Dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "input_dir = 'data_ru/'\n",
    "\n",
    "if not os.path.isdir('../../models'):\n",
    "    os.mkdir('../../models')\n",
    "\n",
    "if not os.path.isdir('../../models/tf_idf'):\n",
    "    os.mkdir('../../models/tf_idf')\n",
    "\n",
    "corpus = []\n",
    "for file in glob.glob(os.path.join(input_dir, f\"*.annot.pkl\")):\n",
    "    tokens = pickle.load(open(file, 'rb'))['tokens']\n",
    "    corpus.append(list(map(lambda token: token.text.lower(), tokens)))\n",
    "\n",
    "from utils.count_vectorizer import MyCountVectorizer\n",
    "\n",
    "count_vect = MyCountVectorizer(ngram_range=(1, 2), tokenizer=MyCountVectorizer.dummy,\n",
    "                               preprocessor=MyCountVectorizer.dummy)\n",
    "\n",
    "svd = TruncatedSVD(n_components=25,\n",
    "                   tol=0.0,\n",
    "                   n_iter=7,\n",
    "                   random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('svd', svd)\n",
    "])\n",
    "\n",
    "pipeline.fit(corpus)\n",
    "pickle.dump(pipeline, open('../../models/tf_idf/pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# python -c \"import nltk; nltk.download('stopwords')\"\n",
    "# pip install dostoevsky\n",
    "# dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='../../models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "table = pd.read_feather('data_ru/all_pairs.fth')\n",
    "table = table[table.snippet_x.map(len) > 0]\n",
    "table = table[table.snippet_y.map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for filename, df in tqdm(table.groupby('filename')):\n",
    "    annot = pickle.load(open(os.path.join('data_ru', filename + '.annot.pkl'), 'rb'))\n",
    "    features = features_processor(df,\n",
    "                                  annot['text'], annot['tokens'],\n",
    "                                  annot['sentences'], annot['lemma'],\n",
    "                                  annot['morph'], annot['ud_postag'],\n",
    "                                  annot['syntax_dep_tree'])\n",
    "    features.to_pickle(os.path.join('data_ru', filename + '.gold.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for pklfile in tqdm(glob.glob('data_ru/*.gold.pkl')):\n",
    "    features = pd.read_pickle(pklfile)\n",
    "    if 'level_0' in features.keys():\n",
    "        features = features.drop(columns=['level_0'])\n",
    "    features = rename_relations(features)\n",
    "    features.to_pickle(pklfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}