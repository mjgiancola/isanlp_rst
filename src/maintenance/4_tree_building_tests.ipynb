{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree building evaluation on gold EDUs (mostly) and playground for tree building scripts\n",
    "\n",
    "1. Modifications of library components for tree building\n",
    "2. Scripts for test and evaluation of Sklearn-, AllenNLP- and gold-annotation-based RST parsers on manually segmented corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "#from utils.rst_annotation import DiscourseUnit\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation_rst import DiscourseUnit\n",
    "\n",
    "class DiscourseUnitCreator:\n",
    "    def __init__(self, id):\n",
    "        self.id = id\n",
    "        \n",
    "    def __call__(self, left_node, right_node, proba):\n",
    "        self.id += 1\n",
    "        return DiscourseUnit(\n",
    "            id=id,\n",
    "            left=left_node,\n",
    "            right=right_node,\n",
    "            relation=1,\n",
    "            proba=proba\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# pip install dostoevsky\n",
    "# dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _isanlp_rst.src.isanlp_rst.rst_tree_predictor import *\n",
    "from _isanlp_rst.src.isanlp_rst.greedy_rst_parser import GreedyRSTParser\n",
    "from _isanlp_rst.src.isanlp_rst.features_extractor import FeaturesExtractor\n",
    "from _isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "from _isanlp_rst.src.isanlp_rst.classifier_wrappers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "news in train: 0.5344827586206896,\tin dev: 0.6470588235294118,\tin test: 0.6086956521739131\n",
      "ling in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "comp in train: 0.0,\tin dev: 0.0,\tin test: 0.0\n",
      "blog in train: 0.43103448275862066,\tin dev: 0.5294117647058824,\tin test: 0.4782608695652174\n"
     ]
    }
   ],
   "source": [
    "from utils.train_test_split import split_train_dev_test\n",
    "\n",
    "train, dev, test = split_train_dev_test('./data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SPAN_PREDICTOR = {\n",
    "    'bimpm': (AllenNLPCustomBiMPMClassifier, 'structure_predictor_bimpm', 0., 0.6),\n",
    "    'baseline': (SklearnClassifier, 'structure_predictor_baseline', 0.1, 0.2),\n",
    "    'ensemble': (EnsembleClassifier,)\n",
    "}\n",
    "\n",
    "_LABEL_PREDICTOR = {\n",
    "    'bimpm': (AllenNLPBiMPMClassifier, 'label_predictor_bimpm'),\n",
    "    'esim': (AllenNLPBiMPMClassifier, 'label_predictor_esim'),\n",
    "    'baseline': (SklearnClassifier, 'label_predictor_baseline'),\n",
    "    'ensemble': (EnsembleClassifier,)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-15 16:06:26,462 - INFO - gensim.models.utils_any2vec - loading projection weights from models/w2v/default/model.vec\n",
      "2021-12-15 16:07:03,976 - INFO - gensim.models.utils_any2vec - loaded (195071, 300) matrix from models/w2v/default/model.vec\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "NEURAL_BINARY_PATH = 'models/structure_predictor_bimpm/elmo/'\n",
    "BASELINE_BINARY_PATH = 'models/structure_predictor_baseline/'\n",
    "\n",
    "NEURAL_LABEL_PATH = '../../models/label_predictor_esim/'\n",
    "BASELINE_LABEL_PATH = 'models/relation_predictor_baseline/'\n",
    "\n",
    "# neural_binary_classifier = _SPAN_PREDICTOR['bimpm'][0](NEURAL_BINARY_PATH)\n",
    "baseline_binary_classifier = _SPAN_PREDICTOR['baseline'][0](BASELINE_BINARY_PATH)\n",
    "# binary_classifier = _SPAN_PREDICTOR['ensemble'][0]((neural_binary_classifier, baseline_binary_classifier))\n",
    "binary_classifier = baseline_binary_classifier\n",
    "\n",
    "# neural_label_classifier = _LABEL_PREDICTOR['esim'][0](NEURAL_LABEL_PATH)\n",
    "baseline_label_classifier = _LABEL_PREDICTOR['baseline'][0](BASELINE_LABEL_PATH)\n",
    "# label_classifier = _LABEL_PREDICTOR['ensemble'][0]((neural_label_classifier, baseline_label_classifier), \n",
    "#                                                    weights=[1., 2.])\n",
    "# while label classifier is on train!\n",
    "label_classifier = baseline_label_classifier\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=False)\n",
    "features_extractor = FeaturesExtractor(features_processor)\n",
    "\n",
    "_predictor = [LargeNNTreePredictor,  # both classifiers are neural\n",
    "              EnsembleNNTreePredictor,  # structure predictions are neural, for labels use an ensemble\n",
    "              DoubleEnsembleNNTreePredictor,  # both classifiers are ensembles\n",
    "             ]\n",
    "\n",
    "predictor = _predictor[2](features_processor=features_extractor, \n",
    "                            relation_predictor_sentence=None,\n",
    "                            relation_predictor_text=binary_classifier, \n",
    "                            label_predictor=label_classifier)\n",
    "\n",
    "# paragraph_parser = GreedyRSTParser(predictor,\n",
    "#                                    confidence_threshold=_SPAN_PREDICTOR['bimpm'][2], \n",
    "#                                    _same_sentence_bonus=1.)\n",
    "\n",
    "# document_parser = GreedyRSTParser(predictor,\n",
    "#                                   confidence_threshold=_SPAN_PREDICTOR['bimpm'][3], \n",
    "#                                   _same_sentence_bonus=0.)\n",
    "\n",
    "# additional_document_parser = GreedyRSTParser(predictor,\n",
    "#                                              confidence_threshold=_SPAN_PREDICTOR['bimpm'][3]-0.15, \n",
    "#                                              _same_sentence_bonus=0.)\n",
    "\n",
    "\n",
    "paragraph_parser = GreedyRSTParser(predictor,\n",
    "                                   confidence_threshold=_SPAN_PREDICTOR['baseline'][2], \n",
    "                                   _same_sentence_bonus=1.)\n",
    "\n",
    "document_parser = GreedyRSTParser(predictor,\n",
    "                                  confidence_threshold=_SPAN_PREDICTOR['baseline'][3], \n",
    "                                  _same_sentence_bonus=0.)\n",
    "\n",
    "additional_document_parser = GreedyRSTParser(predictor,\n",
    "                                             confidence_threshold=_SPAN_PREDICTOR['baseline'][3]-0.15, \n",
    "                                             _same_sentence_bonus=0.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation import Sentence\n",
    "\n",
    "def split_by_paragraphs(annot_text, annot_tokens, annot_sentences, annot_lemma, annot_morph, annot_postag,\n",
    "                        annot_syntax_dep_tree):\n",
    "\n",
    "    def split_on_two(sents, boundary):\n",
    "        list_sum = lambda l: sum([len(sublist) for sublist in l])\n",
    "\n",
    "        i = 1\n",
    "        while list_sum(sents[:i]) < boundary and i < len(sents):\n",
    "            i += 1\n",
    "\n",
    "        intersentence_boundary = min(len(sents[i - 1]), boundary - list_sum(sents[:i - 1]))\n",
    "        return (sents[:i - 1] + [sents[i - 1][:intersentence_boundary]],\n",
    "                [sents[i - 1][intersentence_boundary:]] + sents[i:])\n",
    "\n",
    "    def recount_sentences(chunk):\n",
    "        sentences = []\n",
    "        lemma = []\n",
    "        morph = []\n",
    "        postag = []\n",
    "        syntax_dep_tree = []\n",
    "        tokens_cursor = 0\n",
    "\n",
    "        for i, sent in enumerate(chunk['syntax_dep_tree']):\n",
    "            if len(sent) > 0:\n",
    "                sentences.append(Sentence(tokens_cursor, tokens_cursor + len(sent)))\n",
    "                lemma.append(chunk['lemma'][i])\n",
    "                morph.append(chunk['morph'][i])\n",
    "                postag.append(chunk['postag'][i])\n",
    "                syntax_dep_tree.append(chunk['syntax_dep_tree'][i])\n",
    "                tokens_cursor += len(sent)\n",
    "\n",
    "        chunk['sentences'] = sentences\n",
    "        chunk['lemma'] = lemma\n",
    "        chunk['morph'] = morph\n",
    "        chunk['postag'] = postag\n",
    "        chunk['syntax_dep_tree'] = syntax_dep_tree\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    chunks = []\n",
    "    prev_right_boundary = -1\n",
    "\n",
    "    for i, token in enumerate(annot_tokens[:-1]):\n",
    "\n",
    "        if '\\n' in annot_text[token.end:annot_tokens[i + 1].begin]:\n",
    "            if prev_right_boundary > -1:\n",
    "                chunk = {\n",
    "                    'text': annot_text[annot_tokens[prev_right_boundary].end:token.end + 1].strip(),\n",
    "                    'tokens': annot_tokens[prev_right_boundary + 1:i + 1]\n",
    "                }\n",
    "            else:\n",
    "                chunk = {\n",
    "                    'text': annot_text[:token.end + 1].strip(),\n",
    "                    'tokens': annot_tokens[:i + 1]\n",
    "                }\n",
    "\n",
    "            lemma, annot_lemma = split_on_two(annot_lemma, i - prev_right_boundary)\n",
    "            morph, annot_morph = split_on_two(annot_morph, i - prev_right_boundary)\n",
    "            postag, annot_postag = split_on_two(annot_postag, i - prev_right_boundary)\n",
    "            syntax_dep_tree, annot_syntax_dep_tree = split_on_two(annot_syntax_dep_tree, i - prev_right_boundary)\n",
    "\n",
    "            chunk.update({\n",
    "                'lemma': lemma,\n",
    "                'morph': morph,\n",
    "                'postag': postag,\n",
    "                'syntax_dep_tree': syntax_dep_tree,\n",
    "            })\n",
    "            chunks.append(recount_sentences(chunk))\n",
    "\n",
    "            prev_right_boundary = i  # number of last token in the last chunk\n",
    "\n",
    "    chunk = {\n",
    "        'text': annot_text[annot_tokens[prev_right_boundary].end:].strip(),\n",
    "        'tokens': annot_tokens[prev_right_boundary + 1:],\n",
    "        'lemma': annot_lemma,\n",
    "        'morph': annot_morph,\n",
    "        'postag': annot_postag,\n",
    "        'syntax_dep_tree': annot_syntax_dep_tree,\n",
    "    }\n",
    "\n",
    "    chunks.append(recount_sentences(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_paragraphs_edus(edus, text):\n",
    "    res = []\n",
    "    parag = []\n",
    "    \n",
    "    for edu in edus:\n",
    "        parag.append(edu)\n",
    "        boundary = text.find(edu)+len(edu)\n",
    "        if boundary < len(text):\n",
    "            if text[boundary] == '\\n':\n",
    "                res.append(parag)\n",
    "                parag = []\n",
    "         \n",
    "    if parag:\n",
    "        res.append(parag)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import prepare_gold_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find edus containing multiple paragraphs and add to exceptions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11489670f72f489cb6dfcc28fbbe50b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import extr_pairs, extr_pairs_forest\n",
    "\n",
    "\n",
    "broken_files = []\n",
    "smallest_file = 'data/news2_4.edus'\n",
    "coolest_file = 'data/blogs_17.edus'\n",
    "shit = 'data/blogs_99.edus'\n",
    "#test[:1]\n",
    "for file in tqdm([smallest_file]):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = prepare_gold_pairs(read_gold(filename, features=True))\n",
    "    annot = read_annotation(filename)\n",
    "    \n",
    "    for missegmentation in (\"\\nIMG\", \n",
    "                            \"\\nгимнастический коврик;\",\n",
    "                            \"\\nгантели или бутылки с песком;\",\n",
    "                            \"\\nнебольшой резиновый мяч;\",\n",
    "                            \"\\nэластичная лента (эспандер);\",\n",
    "                            \"\\nхула-хуп (обруч).\",\n",
    "                            \"\\n200?\",\n",
    "                            \"\\n300?\",\n",
    "                            \"\\nНе требуйте странного.\",\n",
    "                            \"\\nИспользуйте мою модель.\",\n",
    "                            '\\n\"А чего вы от них требуете?\"',\n",
    "                            '\\n\"Решить проблемы с тестерами\".',\n",
    "                            \"\\nКак гончая на дичь.\", \"\\nИ крупная.\",\n",
    "                            \"\\nВ прошлом году компания удивила рынок\",\n",
    "                            \"\\nЧужой этики особенно.\",\n",
    "                            \"\\nНо и своей тоже.\",\n",
    "                            \"\\nАэропорт имени,\",\n",
    "                            \"\\nА вот и монголы.\",\n",
    "                            \"\\nЗолотой Будда.\", \n",
    "                            \"\\nДворец Богдо-Хана.\",\n",
    "                            \"\\nПлощадь Сухэ-Батора.\",\n",
    "                            \"\\nОдноклассники)\",\n",
    "                            \"\\nВечерняя площадь.\",\n",
    "                            \"\\nТугрики.\",\n",
    "                            \"\\nВнутренние монголы.\",\n",
    "                            \"\\nВид сверху.\",\n",
    "                            \"\\nНациональный парк Тэрэлж. IMG IMG\",\n",
    "                            '\\nГора \"Черепаха\".',\n",
    "                            \"\\nПуть к медитации.\",\n",
    "                            \"\\nЖить надо высоко,\",\n",
    "                            \"\\nЧан с кумысом.\",\n",
    "                            \"\\nЖилая юрта.\",\n",
    "                            \"\\nКумыс.\",\n",
    "                            \"\\nТрадиционное занятие монголов\",\n",
    "                            \"\\nДвугорбый верблюд мало где\",\n",
    "                            \"\\nМонгол Шуудан переводится\",\n",
    "                            \"\\nОвощные буузы.\",\n",
    "                            \"\\nЗнаменитый чай!\"\n",
    "                            ):\n",
    "        annot['text'] = annot['text'].replace(missegmentation, ' '+missegmentation[1:])\n",
    "\n",
    "    for edu in edus:\n",
    "        if annot['text'].find(edu) == -1:\n",
    "            print(f'::: {filename} ::: {edu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.export_to_rs3 import ForestExporter  # for list of units (whole document)\n",
    "from utils.export_to_rs3 import Exporter  # for single unit (one tree)\n",
    "\n",
    "exporter = ForestExporter(encoding='utf-8')\n",
    "\n",
    "! rm -r gold_predictions\n",
    "! mkdir gold_predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939d9d5c7c86442192c4aa838fb8ad8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c09234b63da4d30ba707291df63cada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seg_true_pred': 112, 'seg_all_pred': 112, 'seg_all_true': 112, 'unlab_true_pred': 35, 'unlab_all_pred': 96, 'unlab_all_true': 104, 'lab_true_pred': 19, 'lab_all_pred': 96, 'lab_all_true': 104, 'nuc_true_pred': 23, 'nuc_all_pred': 96, 'nuc_all_true': 104, 'full_true_pred': 18, 'full_all_pred': 96, 'full_all_true': 104, 'filename': './data/news1_47.edus'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0094c32374c00aa629ad55ef07273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'seg_true_pred': 245, 'seg_all_pred': 246, 'seg_all_true': 490, 'unlab_true_pred': 92, 'unlab_all_pred': 216, 'unlab_all_true': 227, 'lab_true_pred': 63, 'lab_all_pred': 216, 'lab_all_true': 227, 'nuc_true_pred': 78, 'nuc_all_pred': 216, 'nuc_all_true': 227, 'full_true_pred': 63, 'full_all_pred': 216, 'full_all_true': 227, 'filename': './data/news2_38.edus'}\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "from utils.file_reading import *\n",
    "from utils.evaluation import *\n",
    "\n",
    "\n",
    "broken_files = []\n",
    "smallest_file = 'data/news2_4.edus'\n",
    "weirdest_file = 'data/blogs_63.edus'\n",
    "\n",
    "for file in tqdm(test[1:3]):\n",
    "    filename = '.'.join(file.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = prepare_gold_pairs(read_gold(filename, features=True))\n",
    "    annot = read_annotation(filename)\n",
    "    annot['text'] = annot['text'].strip()\n",
    "    \n",
    "    for missegmentation in (\"\\nIMG\", \n",
    "                            \"\\nгимнастический коврик;\",\n",
    "                            \"\\nгантели или бутылки с песком;\",\n",
    "                            \"\\nнебольшой резиновый мяч;\",\n",
    "                            \"\\nэластичная лента (эспандер);\",\n",
    "                            \"\\nхула-хуп (обруч).\",\n",
    "                            \"\\n200?\",\n",
    "                            \"\\n300?\",\n",
    "                            \"\\nНе требуйте странного.\",\n",
    "                            \"\\nИспользуйте мою модель.\",\n",
    "                            '\\n\"А чего вы от них требуете?\"',\n",
    "                            '\\n\"Решить проблемы с тестерами\".',\n",
    "                            \"\\nКак гончая на дичь.\", \"\\nИ крупная.\",\n",
    "                            \"\\nВ прошлом году компания удивила рынок\",\n",
    "                            \"\\nЧужой этики особенно.\",\n",
    "                            \"\\nНо и своей тоже.\",\n",
    "                            \"\\nАэропорт имени,\",\n",
    "                            \"\\nА вот и монголы.\",\n",
    "                            \"\\nЗолотой Будда.\", \n",
    "                            \"\\nДворец Богдо-Хана.\",\n",
    "                            \"\\nПлощадь Сухэ-Батора.\",\n",
    "                            \"\\nОдноклассники)\",\n",
    "                            \"\\nВечерняя площадь.\",\n",
    "                            \"\\nТугрики.\",\n",
    "                            \"\\nВнутренние монголы.\",\n",
    "                            \"\\nВид сверху.\",\n",
    "                            \"\\nНациональный парк Тэрэлж. IMG IMG\",\n",
    "                            '\\nГора \"Черепаха\".',\n",
    "                            \"\\nПуть к медитации.\",\n",
    "                            \"\\nЖить надо высоко,\",\n",
    "                            \"\\nЧан с кумысом.\",\n",
    "                            \"\\nЖилая юрта.\",\n",
    "                            \"\\nКумыс.\",\n",
    "                            \"\\nТрадиционное занятие монголов\",\n",
    "                            \"\\nДвугорбый верблюд мало где\",\n",
    "                            \"\\nМонгол Шуудан переводится\",\n",
    "                            \"\\nОвощные буузы.\",\n",
    "                            \"\\nЗнаменитый чай!\",\n",
    "                            ):\n",
    "        annot['text'] = annot['text'].replace(missegmentation, ' ' + missegmentation[1:])\n",
    "\n",
    "    \n",
    "    if '\\n' in annot['text']:\n",
    "        chunks = split_by_paragraphs(\n",
    "            annot['text'],\n",
    "            annot['tokens'], \n",
    "            annot['sentences'], \n",
    "            annot['lemma'], \n",
    "            annot['morph'], \n",
    "            annot['postag'], \n",
    "            annot['syntax_dep_tree'])\n",
    "        \n",
    "        chunked_edus = split_by_paragraphs_edus(edus, annot['text'])\n",
    "    \n",
    "    dus = []\n",
    "    start_id = 0\n",
    "    for i, chunk in enumerate(tqdm(chunks)):\n",
    "        _edus = []\n",
    "        last_end = 0\n",
    "        \n",
    "        for max_id in range(len(chunked_edus[i])):\n",
    "            start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(chunked_edus[i][max_id])\n",
    "            end = start + len(chunked_edus[i][max_id])\n",
    "            temp = DiscourseUnit(\n",
    "                    id=start_id,\n",
    "                    left=None,\n",
    "                    right=None,\n",
    "                    relation='edu',\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    orig_text=annot['text'],\n",
    "                    proba=1.,\n",
    "                )\n",
    "\n",
    "            _edus.append(temp)\n",
    "            last_end = end + 1\n",
    "            start_id += 1\n",
    "            \n",
    "        if len(_edus) == 1:\n",
    "            dus += _edus\n",
    "            start_id = _edus[-1].id + 1\n",
    "\n",
    "        elif len(_edus) > 1:\n",
    "            trees = paragraph_parser(_edus,\n",
    "                annot['text'], chunk['tokens'], chunk['sentences'], chunk['lemma'],\n",
    "                chunk['morph'], chunk['postag'], chunk['syntax_dep_tree'])\n",
    "            \n",
    "            dus += trees\n",
    "            start_id = max([tree.id for tree in dus]) + 1\n",
    "        \n",
    "    parsed = document_parser(\n",
    "                dus, \n",
    "                annot['text'], \n",
    "                annot['tokens'], \n",
    "                annot['sentences'], \n",
    "                annot['lemma'], \n",
    "                annot['morph'], \n",
    "                annot['postag'], \n",
    "                annot['syntax_dep_tree'],\n",
    "                genre=filename.split('_')[0])\n",
    "    \n",
    "    if len(parsed) > len(annot['text']) // 400:\n",
    "        parsed = additional_document_parser(\n",
    "            parsed, \n",
    "            annot['text'], \n",
    "            annot['tokens'], \n",
    "            annot['sentences'], \n",
    "            annot['lemma'], \n",
    "            annot['morph'], \n",
    "            annot['postag'], \n",
    "            annot['syntax_dep_tree'],\n",
    "            genre=filename.split('_')[0]\n",
    "        )\n",
    "        \n",
    "    exporter(parsed, f\"gold_predictions/{filename.split('/')[-1]}_parsed_goldedu.rs3\")\n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed, annot['text']), \n",
    "                                columns=['snippet_x', 'snippet_y', 'category_id', 'order'])\n",
    "    evaluation = eval_pipeline(parsed_pairs=parsed_pairs,\n",
    "                               gold_edus=edus,\n",
    "                               gold_pairs=gold[['snippet_x', 'snippet_y', 'category_id', 'order']],\n",
    "                               text=annot['text'],\n",
    "                               trees=parsed)\n",
    "    evaluation['filename'] = file\n",
    "    print(evaluation)\n",
    "    cache.append(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3465346534653466"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re = 35./104\n",
    "pr = 35./98\n",
    "2 * pr * re / (pr + re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1881188118811881"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re = 19./104\n",
    "pr = 19./98\n",
    "2 * pr * re / (pr + re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.file_reading import *\n",
    "from utils.evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 454\n",
      "text: Проект по созданию объединенных арабских ВС требует перезапуска - Флоренс Гауб\n",
      "Проект по созданию объединенных арабских вооруженных сил, отложенный на неопределенный срок из-за разногласий между членами Лиги арабских государств (ЛАГ) и недостаточной проработки законодательной базы, нуждается в перезапуске в новом формате и с уточненными целями. К такому выводу пришла старший аналитик Института Европейского союза по вопросам безопасности (EUISS) Флоренс Гауб (Florence Gaub) в статье \"Застряли в бараках: объединенные арабские вооруженные силы\".\n",
      "Она напоминает, что в январе 2015 года секретариат ЛАГ вышел за рамки ранее обсуждавшихся ограниченных военных альянсов и предложил сформировать единые межарабские силы быстрого реагирования, ориентированные на борьбу с терроризмом, на основании договора о совместной обороне и экономическом сотрудничестве от 1950 года.\n",
      "proba: 0.4533515563355963\n",
      "relation: joint\n",
      "nuclearity: NN\n",
      "left: Проект по созданию объединенных арабских ВС требует перезапуска - Флоренс Гауб\n",
      "Проект по созданию объединенных арабских вооруженных сил, отложенный на неопределенный срок из-за разногласий между членами Лиги арабских государств (ЛАГ) и недостаточной проработки законодательной базы, нуждается в перезапуске в новом формате и с уточненными целями. К такому выводу пришла старший аналитик Института Европейского союза по вопросам безопасности (EUISS) Флоренс Гауб (Florence Gaub) в статье \"Застряли в бараках: объединенные арабские вооруженные силы\".\n",
      "right: Она напоминает, что в январе 2015 года секретариат ЛАГ вышел за рамки ранее обсуждавшихся ограниченных военных альянсов и предложил сформировать единые межарабские силы быстрого реагирования, ориентированные на борьбу с терроризмом, на основании договора о совместной обороне и экономическом сотрудничестве от 1950 года.\n",
      "start: 0\n",
      "end: 869\n"
     ]
    }
   ],
   "source": [
    "print(parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 80\n",
      "text: Что необычно, в первую очередь совет ЛАГ поспешил внести поправки в устав Межарабского совета мира и безопасности для организации встреч на правительственном уровне два раза в год. Ранее эта структура, основанная в 2006 году, не имела никаких полномочий и состояла всего из пяти выборных членов. Перед советом была поставлена задача - подготовить стратегии по сохранению мира в регионе и укреплению безопасности в арабских странах.\n",
      "proba: 0.21887695231527632\n",
      "relation: joint\n",
      "nuclearity: NN\n",
      "left: Что необычно, в первую очередь совет ЛАГ поспешил внести поправки в устав Межарабского совета мира и безопасности для организации встреч на правительственном уровне два раза в год. Ранее эта структура, основанная в 2006 году, не имела никаких полномочий и состояла всего из пяти выборных членов.\n",
      "right: Перед советом была поставлена задача - подготовить стратегии по сохранению мира в регионе и укреплению безопасности в арабских странах.\n",
      "start: 2389\n",
      "end: 2820\n"
     ]
    }
   ],
   "source": [
    "print(parsed[4].right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_true_pred</th>\n",
       "      <th>seg_all_pred</th>\n",
       "      <th>seg_all_true</th>\n",
       "      <th>unlab_true_pred</th>\n",
       "      <th>unlab_all_pred</th>\n",
       "      <th>unlab_all_true</th>\n",
       "      <th>lab_true_pred</th>\n",
       "      <th>lab_all_pred</th>\n",
       "      <th>lab_all_true</th>\n",
       "      <th>nuc_true_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_unlab</th>\n",
       "      <th>pr_lab</th>\n",
       "      <th>re_lab</th>\n",
       "      <th>f1_lab</th>\n",
       "      <th>pr_nuc</th>\n",
       "      <th>re_nuc</th>\n",
       "      <th>f1_nuc</th>\n",
       "      <th>pr_full</th>\n",
       "      <th>re_full</th>\n",
       "      <th>f1_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>35</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>19</td>\n",
       "      <td>96</td>\n",
       "      <td>104</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35000</td>\n",
       "      <td>0.197917</td>\n",
       "      <td>0.182692</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.221154</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>245</td>\n",
       "      <td>246</td>\n",
       "      <td>490</td>\n",
       "      <td>92</td>\n",
       "      <td>216</td>\n",
       "      <td>227</td>\n",
       "      <td>63</td>\n",
       "      <td>216</td>\n",
       "      <td>227</td>\n",
       "      <td>78</td>\n",
       "      <td>...</td>\n",
       "      <td>0.41535</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.277533</td>\n",
       "      <td>0.284424</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.343612</td>\n",
       "      <td>0.352144</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.277533</td>\n",
       "      <td>0.284424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seg_true_pred  seg_all_pred  seg_all_true  unlab_true_pred  unlab_all_pred  \\\n",
       "0            112           112           112               35              96   \n",
       "1            245           246           490               92             216   \n",
       "\n",
       "   unlab_all_true  lab_true_pred  lab_all_pred  lab_all_true  nuc_true_pred  \\\n",
       "0             104             19            96           104             23   \n",
       "1             227             63           216           227             78   \n",
       "\n",
       "   ...  f1_unlab    pr_lab    re_lab    f1_lab    pr_nuc    re_nuc    f1_nuc  \\\n",
       "0  ...   0.35000  0.197917  0.182692  0.190000  0.239583  0.221154  0.230000   \n",
       "1  ...   0.41535  0.291667  0.277533  0.284424  0.361111  0.343612  0.352144   \n",
       "\n",
       "    pr_full   re_full   f1_full  \n",
       "0  0.187500  0.173077  0.180000  \n",
       "1  0.291667  0.277533  0.284424  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tmp = pd.DataFrame(cache[7:27] + cache[28:])\n",
    "tmp = pd.DataFrame(cache)\n",
    "tmp['pr_seg'] = tmp.seg_true_pred / tmp.seg_all_pred\n",
    "tmp['re_seg'] = tmp.seg_true_pred / tmp.seg_all_true\n",
    "tmp['f1_seg'] = 2 * tmp.pr_seg * tmp.re_seg / (tmp.pr_seg + tmp.re_seg)\n",
    "tmp['pr_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_pred\n",
    "tmp['re_unlab'] = tmp.unlab_true_pred / tmp.unlab_all_true\n",
    "tmp['f1_unlab'] = 2 * tmp.pr_unlab * tmp.re_unlab / (tmp.pr_unlab + tmp.re_unlab)\n",
    "tmp['pr_lab'] = tmp.lab_true_pred / tmp.lab_all_pred\n",
    "tmp['re_lab'] = tmp.lab_true_pred / tmp.lab_all_true\n",
    "tmp['f1_lab'] = 2 * tmp.pr_lab * tmp.re_lab / (tmp.pr_lab + tmp.re_lab)\n",
    "tmp['pr_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_pred\n",
    "tmp['re_nuc'] = tmp.nuc_true_pred / tmp.nuc_all_true\n",
    "tmp['f1_nuc'] = 2 * tmp.pr_nuc * tmp.re_nuc / (tmp.pr_nuc + tmp.re_nuc)\n",
    "tmp['pr_full'] = tmp.full_true_pred / tmp.full_all_pred\n",
    "tmp['re_full'] = tmp.full_true_pred / tmp.full_all_true\n",
    "tmp['f1_full'] = 2 * tmp.pr_full * tmp.re_full / (tmp.pr_full + tmp.re_full)\n",
    "tmp.sort_values('f1_full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_true_pred</th>\n",
       "      <th>seg_all_pred</th>\n",
       "      <th>seg_all_true</th>\n",
       "      <th>unlab_true_pred</th>\n",
       "      <th>unlab_all_pred</th>\n",
       "      <th>unlab_all_true</th>\n",
       "      <th>lab_true_pred</th>\n",
       "      <th>lab_all_pred</th>\n",
       "      <th>lab_all_true</th>\n",
       "      <th>nuc_true_pred</th>\n",
       "      <th>...</th>\n",
       "      <th>f1_unlab</th>\n",
       "      <th>pr_lab</th>\n",
       "      <th>re_lab</th>\n",
       "      <th>f1_lab</th>\n",
       "      <th>pr_nuc</th>\n",
       "      <th>re_nuc</th>\n",
       "      <th>f1_nuc</th>\n",
       "      <th>pr_full</th>\n",
       "      <th>re_full</th>\n",
       "      <th>f1_full</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>96</td>\n",
       "      <td>103</td>\n",
       "      <td>14</td>\n",
       "      <td>96</td>\n",
       "      <td>103</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291457</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.140704</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.223301</td>\n",
       "      <td>0.231156</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.135922</td>\n",
       "      <td>0.140704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>37</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>21</td>\n",
       "      <td>76</td>\n",
       "      <td>79</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477419</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.270968</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.354430</td>\n",
       "      <td>0.361290</td>\n",
       "      <td>0.276316</td>\n",
       "      <td>0.265823</td>\n",
       "      <td>0.270968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>245</td>\n",
       "      <td>87</td>\n",
       "      <td>206</td>\n",
       "      <td>219</td>\n",
       "      <td>64</td>\n",
       "      <td>206</td>\n",
       "      <td>219</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409412</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.301176</td>\n",
       "      <td>0.388350</td>\n",
       "      <td>0.365297</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.310680</td>\n",
       "      <td>0.292237</td>\n",
       "      <td>0.301176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   seg_true_pred  seg_all_pred  seg_all_true  unlab_true_pred  unlab_all_pred  \\\n",
       "1            112           112           112               29              96   \n",
       "0             91            91            91               37              76   \n",
       "2            245           245           245               87             206   \n",
       "\n",
       "   unlab_all_true  lab_true_pred  lab_all_pred  lab_all_true  nuc_true_pred  \\\n",
       "1             103             14            96           103             23   \n",
       "0              79             21            76            79             28   \n",
       "2             219             64           206           219             80   \n",
       "\n",
       "   ...  f1_unlab    pr_lab    re_lab    f1_lab    pr_nuc    re_nuc    f1_nuc  \\\n",
       "1  ...  0.291457  0.145833  0.135922  0.140704  0.239583  0.223301  0.231156   \n",
       "0  ...  0.477419  0.276316  0.265823  0.270968  0.368421  0.354430  0.361290   \n",
       "2  ...  0.409412  0.310680  0.292237  0.301176  0.388350  0.365297  0.376471   \n",
       "\n",
       "    pr_full   re_full   f1_full  \n",
       "1  0.145833  0.135922  0.140704  \n",
       "0  0.276316  0.265823  0.270968  \n",
       "2  0.310680  0.292237  0.301176  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp2 = tmp[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp2[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_news = tmp2[tmp2.filename.str.contains('news')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_blog = tmp2[tmp2.filename.str.contains('blog')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlabeled tree building score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40.705128205128204, 38.368580060422964, 39.502332814930014)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_micro = tmp.unlab_true_pred.sum() / tmp.unlab_all_pred.sum() * 100.\n",
    "re_micro = tmp.unlab_true_pred.sum() / tmp.unlab_all_true.sum() * 100.\n",
    "f1_micro = 2. * pr_micro * re_micro / (pr_micro + re_micro)\n",
    "\n",
    "unlab_micro = (pr_micro, re_micro, f1_micro)\n",
    "unlab_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39.52546296296296, 37.091240257539816, 38.26968223444606)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_macro = tmp.pr_unlab.sum() / tmp.shape[0] * 100.\n",
    "re_macro = tmp.re_unlab.sum() / tmp.shape[0] * 100.\n",
    "f1_macro = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "unlab_macro = (pr_macro, re_macro, f1_macro)\n",
    "unlab_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_macro = tmp_blog.pr_unlab.sum() / tmp_blog.shape[0] * 100.\n",
    "re_macro = tmp_blog.re_unlab.sum() / tmp_blog.shape[0] * 100.\n",
    "unlab_blog = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "pr_macro = tmp_news.pr_unlab.sum() / tmp_news.shape[0] * 100.\n",
    "re_macro = tmp_news.re_unlab.sum() / tmp_news.shape[0] * 100.\n",
    "unlab_news = 2. * pr_macro * re_macro / (pr_macro + re_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeled tree building score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26.282051282051285, 24.773413897280967, 25.5054432348367)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_micro = tmp.lab_true_pred.sum() / tmp.lab_all_pred.sum() * 100.\n",
    "re_micro = tmp.lab_true_pred.sum() / tmp.lab_all_true.sum() * 100.\n",
    "f1_micro = 2. * pr_micro * re_micro / (pr_micro + re_micro)\n",
    "\n",
    "lab_micro = (pr_micro, re_micro, f1_micro)\n",
    "lab_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24.479166666666668, 23.011267366994236, 23.72253109704657)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_macro = tmp.pr_lab.sum() / tmp.shape[0] * 100.\n",
    "re_macro = tmp.re_lab.sum() / tmp.shape[0] * 100.\n",
    "f1_macro = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "lab_macro = (pr_macro, re_macro, f1_macro)\n",
    "lab_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_macro = tmp_blog.pr_lab.sum() / tmp_blog.shape[0] * 100.\n",
    "re_macro = tmp_blog.re_lab.sum() / tmp_blog.shape[0] * 100.\n",
    "lab_blog = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "pr_macro = tmp_news.pr_lab.sum() / tmp_news.shape[0] * 100.\n",
    "re_macro = tmp_news.re_lab.sum() / tmp_news.shape[0] * 100.\n",
    "lab_news = 2. * pr_macro * re_macro / (pr_macro + re_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuclearity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.371794871794876, 30.513595166163142, 31.41524105754277)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_micro = tmp.nuc_true_pred.sum() / tmp.nuc_all_pred.sum() * 100.\n",
    "re_micro = tmp.nuc_true_pred.sum() / tmp.nuc_all_true.sum() * 100.\n",
    "f1_micro = 2. * pr_micro * re_micro / (pr_micro + re_micro)\n",
    "\n",
    "nuc_micro = (pr_micro, re_micro, f1_micro)\n",
    "nuc_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.03472222222222, 28.23830904778041, 29.10882615135016)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_macro = tmp.pr_nuc.sum() / tmp.shape[0] * 100.\n",
    "re_macro = tmp.re_nuc.sum() / tmp.shape[0] * 100.\n",
    "f1_macro = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "nuc_macro = (pr_macro, re_macro, f1_macro)\n",
    "nuc_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_macro = tmp_blog.pr_nuc.sum() / tmp_blog.shape[0] * 100.\n",
    "re_macro = tmp_blog.re_nuc.sum() / tmp_blog.shape[0] * 100.\n",
    "nuc_blog = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "pr_macro = tmp_news.pr_nuc.sum() / tmp_news.shape[0] * 100.\n",
    "re_macro = tmp_news.re_nuc.sum() / tmp_news.shape[0] * 100.\n",
    "nuc_news = 2. * pr_macro * re_macro / (pr_macro + re_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full tree building score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.961538461538463, 24.47129909365559, 25.194401244167963)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_micro = tmp.full_true_pred.sum() / tmp.full_all_pred.sum() * 100.\n",
    "re_micro = tmp.full_true_pred.sum() / tmp.full_all_true.sum() * 100.\n",
    "f1_micro = 2. * pr_micro * re_micro / (pr_micro + re_micro)\n",
    "\n",
    "full_micro = pr_micro, re_micro, f1_micro\n",
    "full_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23.958333333333336, 22.530498136225006, 23.222488819371154)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr_macro = tmp.pr_full.sum() / tmp.shape[0] * 100.\n",
    "re_macro = tmp.re_full.sum() / tmp.shape[0] * 100.\n",
    "f1_macro = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "full_macro = (pr_macro, re_macro, f1_macro)\n",
    "full_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_macro = tmp_blog.pr_full.sum() / tmp_blog.shape[0] * 100.\n",
    "re_macro = tmp_blog.re_full.sum() / tmp_blog.shape[0] * 100.\n",
    "full_blog = 2. * pr_macro * re_macro / (pr_macro + re_macro)\n",
    "\n",
    "pr_macro = tmp_news.pr_full.sum() / tmp_news.shape[0] * 100.\n",
    "re_macro = tmp_news.re_full.sum() / tmp_news.shape[0] * 100.\n",
    "full_news = 2. * pr_macro * re_macro / (pr_macro + re_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|l|l|l|l|}\n",
      "\\toprule\n",
      " component &     P &     R &    F1 &     P &     R &    F1 \\\\\n",
      "\\midrule\n",
      "      span & 40.71 & 38.37 & 39.50 & 39.53 & 37.09 & 38.27 \\\\\n",
      "nuclearity & 32.37 & 30.51 & 31.42 & 30.03 & 28.24 & 29.11 \\\\\n",
      "  relation & 26.28 & 24.77 & 25.51 & 24.48 & 23.01 & 23.72 \\\\\n",
      "      full & 25.96 & 24.47 & 25.19 & 23.96 & 22.53 & 23.22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_table = pd.DataFrame(columns=['component', 'P', 'R', 'F1', 'P', 'R', 'F1'], data=[\n",
    "    ['span', unlab_micro[0], unlab_micro[1], unlab_micro[2], unlab_macro[0], unlab_macro[1], unlab_macro[2]],\n",
    "    ['nuclearity', nuc_micro[0], nuc_micro[1], nuc_micro[2], nuc_macro[0], nuc_macro[1], nuc_macro[2]],\n",
    "    ['relation', lab_micro[0], lab_micro[1], lab_micro[2], lab_macro[0], lab_macro[1], lab_macro[2]],\n",
    "    ['full', full_micro[0], full_micro[1], full_micro[2], full_macro[0], full_macro[1], full_macro[2]],\n",
    "])\n",
    "\n",
    "print(evaluation_table.to_latex(index=False, float_format='%.2f', column_format='|l|l|l|l|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|l|l|l|l|}\n",
      "\\toprule\n",
      " component &     P &     R &    F1 &     P &     R &    F1 \\\\\n",
      "\\midrule\n",
      "      span & 40.71 & 38.37 & 39.50 & 39.53 & 37.09 & 38.27 \\\\\n",
      "nuclearity & 32.37 & 30.51 & 31.42 & 30.03 & 28.24 & 29.11 \\\\\n",
      "  relation & 26.28 & 24.77 & 25.51 & 24.48 & 23.01 & 23.72 \\\\\n",
      "      full & 25.96 & 24.47 & 25.19 & 23.96 & 22.53 & 23.22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_table = pd.DataFrame(columns=['component', 'P', 'R', 'F1', 'P', 'R', 'F1'], data=[\n",
    "    ['span', unlab_micro[0], unlab_micro[1], unlab_micro[2], unlab_macro[0], unlab_macro[1], unlab_macro[2]],\n",
    "    ['nuclearity', nuc_micro[0], nuc_micro[1], nuc_micro[2], nuc_macro[0], nuc_macro[1], nuc_macro[2]],\n",
    "    ['relation', lab_micro[0], lab_micro[1], lab_micro[2], lab_macro[0], lab_macro[1], lab_macro[2]],\n",
    "    ['full', full_micro[0], full_micro[1], full_micro[2], full_macro[0], full_macro[1], full_macro[2]],\n",
    "])\n",
    "\n",
    "print(evaluation_table.to_latex(index=False, float_format='%.2f', column_format='|l|l|l|l|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append separated genres to the main table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|l|l|l|l|}\n",
      "\\toprule\n",
      " component &     P &     R &    F1 &     P &     R &    F1 &  blogs &  news \\\\\n",
      "\\midrule\n",
      "      span & 40.71 & 38.37 & 39.50 & 39.53 & 37.09 & 38.27 &    NaN & 38.27 \\\\\n",
      "nuclearity & 32.37 & 30.51 & 31.42 & 30.03 & 28.24 & 29.11 &    NaN & 29.11 \\\\\n",
      "  relation & 26.28 & 24.77 & 25.51 & 24.48 & 23.01 & 23.72 &    NaN & 23.72 \\\\\n",
      "      full & 25.96 & 24.47 & 25.19 & 23.96 & 22.53 & 23.22 &    NaN & 23.22 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_table = pd.DataFrame(columns=['component', 'P', 'R', 'F1', 'P', 'R', 'F1', 'blogs', 'news'], data=[\n",
    "    ['span', unlab_micro[0], unlab_micro[1], unlab_micro[2], unlab_macro[0], unlab_macro[1], unlab_macro[2], unlab_blog, unlab_news],\n",
    "    ['nuclearity', nuc_micro[0], nuc_micro[1], nuc_micro[2], nuc_macro[0], nuc_macro[1], nuc_macro[2], nuc_blog, nuc_news],\n",
    "    ['relation', lab_micro[0], lab_micro[1], lab_micro[2], lab_macro[0], lab_macro[1], lab_macro[2], lab_blog, lab_news],\n",
    "    ['full', full_micro[0], full_micro[1], full_micro[2], full_macro[0], full_macro[1], full_macro[2], full_blog, full_news],\n",
    "])\n",
    "\n",
    "print(evaluation_table.to_latex(index=False, float_format='%.2f', column_format='|l|l|l|l|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation (Gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import metric_parseval_df as metric_parseval\n",
    "from utils.evaluation import extr_pairs_forest\n",
    "from utils.file_reading import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "true_pos = []\n",
    "all_parsed = []\n",
    "all_gold = []\n",
    "\n",
    "for key, value in cache.items():\n",
    "    c_true_pos, c_all_parsed, c_all_gold = metric_parseval(value[0], value[1])\n",
    "    filenames.append(key)\n",
    "    true_pos.append(c_true_pos)\n",
    "    all_parsed.append(c_all_parsed)\n",
    "    all_gold.append(c_all_gold)\n",
    "    \n",
    "results = pd.DataFrame({'filename': filenames, \n",
    "                    'true_pos': true_pos,\n",
    "                    'all_parsed': all_parsed,\n",
    "                    'all_gold': all_gold})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _isanlp_rst.src.isanlp_rst.rst_tree_predictor import GoldTreePredictor\n",
    "from isanlp.annotation_rst import ForestExporter\n",
    "exporter = ForestExporter(encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_golds(filename):\n",
    "    filename = '.'.join(filename.split('.')[:-1])\n",
    "    edus = read_edus(filename)\n",
    "    gold = read_gold(filename, features=True)\n",
    "    annot = read_annotation(filename)\n",
    "    annot['text'] = annot['text'].replace('\\n', ' ').replace('  ', ' ').replace('  ', ' ')\n",
    "    \n",
    "    _edus = []\n",
    "    last_end = 0\n",
    "    last_id = 0\n",
    "    for max_id in range(len(edus)):\n",
    "        start = len(annot['text'][:last_end]) + annot['text'][last_end:].find(edus[max_id])\n",
    "        end = start + len(edus[max_id])\n",
    "        temp = DiscourseUnit(\n",
    "                id=start,\n",
    "                left=None,\n",
    "                right=None,\n",
    "                relation='edu',\n",
    "                start=start,\n",
    "                end=end,\n",
    "                text=edus[max_id],\n",
    "                orig_text=annot['text'],\n",
    "                proba=1.,\n",
    "            )\n",
    "        _edus.append(temp)\n",
    "        last_end = end\n",
    "        last_id += 1\n",
    "\n",
    "    parser = GreedyRSTParser(GoldTreePredictor(gold), confidence_threshold=0.)\n",
    "    parsed = parser(_edus, annot['text'], annot['tokens'], annot['sentences'],\n",
    "                    annot['postag'], annot['morph'], annot['lemma'], annot['syntax_dep_tree'])\n",
    "    \n",
    "    filename = filename.split('/')[-1]\n",
    "    exporter(parsed, 'parsed_golds_0406/'+filename+'.rs3')\n",
    "    \n",
    "    parsed_pairs = pd.DataFrame(extr_pairs_forest(parsed, annot['text'], locations=True), \n",
    "                                columns=['snippet_x', 'snippet_y', 'category_id', 'order', 'loc_x', 'loc_y'])\n",
    "    \n",
    "    return filename, metric_parseval(parsed_pairs, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘parsed_golds_0406’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir parsed_golds_0406"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/isanlp/annotation_rst.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /opt/.pyenv/versions/3.7.4/lib/python3.7/site-packages/isanlp/annotation_rst.py\n",
    "\n",
    "class DiscourseUnit:\n",
    "    def __init__(self, id, left=None, right=None, text='', start=None, end=None,\n",
    "                 orig_text=None, relation='', nuclearity='', proba=1.):\n",
    "        \"\"\"\n",
    "        :param int id:\n",
    "        :param DiscourseUnit left:\n",
    "        :param DiscourseUnit right:\n",
    "        :param str text: (optional)\n",
    "        :param int start: start position in original text\n",
    "        :param int end: end position in original text\n",
    "        :param string relation: {the relation between left and right components | 'elementary' | 'root'}\n",
    "        :param string nuclearity: {'NS' | 'SN' | 'NN'}\n",
    "        :param float proba: predicted probability of the relation occurrence\n",
    "        \"\"\"\n",
    "        self.id = id\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.relation = relation\n",
    "        self.nuclearity = nuclearity\n",
    "        self.proba = proba\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "        if self.left:\n",
    "            self.start = left.start\n",
    "            self.end = right.end\n",
    "\n",
    "        if orig_text:\n",
    "            self.text = orig_text[self.start:self.end + 1].strip()\n",
    "        else:\n",
    "            self.text = text.strip()\n",
    "\n",
    "        self._exporter = None\n",
    "\n",
    "    def __str__(self):\n",
    "        result = f\"id: {self.id}\\n\"\n",
    "        result += f\"text: {self.text}\\n\"\n",
    "        result += f\"proba: {self.proba}\\n\"\n",
    "        result += f\"relation: {self.relation}\\n\"\n",
    "        result += f\"nuclearity: {self.nuclearity}\\n\"\n",
    "        result += f\"left: {self.left.text if self.left else None}\\n\"\n",
    "        result += f\"right: {self.right.text if self.right else None}\\n\"\n",
    "        result += f\"start: {self.start}\\n\"\n",
    "        result += f\"end: {self.end}\"\n",
    "        return result\n",
    "\n",
    "    def to_rs3(self, filename, encoding='utf8'):\n",
    "        self._exporter = Exporter(encoding=encoding)\n",
    "        self._exporter(self, filename)\n",
    "\n",
    "\n",
    "class Segment:\n",
    "    def __init__(self, _id, parent, relname, text):\n",
    "        self.id = _id + 1\n",
    "        self.parent = parent + 1\n",
    "        self.relname = relname\n",
    "        self.text = text\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.parent != -1:\n",
    "            return f'<segment id=\"{self.id}\" parent=\"{self.parent}\" relname=\"{self.relname}\">{self.text}</segment>'\n",
    "\n",
    "        return f'<segment id=\"{self.id}\" relname=\"{self.relname}\">{self.text}</segment>'\n",
    "\n",
    "\n",
    "class Group:\n",
    "    def __init__(self, _id, type, parent, relname):\n",
    "        self.id = _id + 1\n",
    "        self.type = type\n",
    "        self.parent = parent + 1\n",
    "        self.relname = relname\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'<group id=\"{self.id}\" type=\"{self.type}\" parent=\"{self.parent}\" relname=\"{self.relname}\"/>'\n",
    "\n",
    "\n",
    "class Root(Group):\n",
    "    def __init__(self, _id, type=\"span\"):\n",
    "        Group.__init__(self, _id, type=type, parent=-1, relname=\"span\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'<group id=\"{self.id}\" type=\"{self.type}\"/>'\n",
    "\n",
    "\n",
    "class Exporter:\n",
    "    def __init__(self, encoding='cp1251', verbose=False):\n",
    "        self._encoding = encoding\n",
    "        self.verbose = verbose\n",
    "        self.max_id = 0\n",
    "\n",
    "    def __call__(self, tree, filename):\n",
    "\n",
    "        with open(filename, 'w', encoding=self._encoding) as fo:\n",
    "            fo.write('<rst>\\n')\n",
    "            fo.write(self.make_header(tree))\n",
    "            fo.write(self.make_body(tree))\n",
    "            fo.write('</rst>')\n",
    "\n",
    "    def compile_relation_set(self, tree):\n",
    "        result = ['_'.join([tree.relation, tree.nuclearity])] + ['antithesis_NN']\n",
    "        if not tree.left:\n",
    "            return result\n",
    "        if tree.left.left:\n",
    "            result += self.compile_relation_set(tree.left)\n",
    "        if tree.right.left:\n",
    "            result += self.compile_relation_set(tree.right)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def make_header(self, tree):\n",
    "        relations = list(set(self.compile_relation_set(tree)))\n",
    "        relations = [value if value != \"elementary__\" else \"antithesis_NN\" for value in relations]\n",
    "\n",
    "        result = '\\t<header>\\n'\n",
    "        result += '\\t\\t<relations>\\n'\n",
    "        for rel in relations:\n",
    "            _relname, _type = rel.split('_')\n",
    "            _type = 'multinuc' if _type == 'NN' else 'rst'\n",
    "            result += f'\\t\\t\\t<rel name=\"{_relname}\" type=\"{_type}\" />\\n'\n",
    "        result += '\\t\\t</relations>\\n'\n",
    "        result += '\\t</header>\\n'\n",
    "\n",
    "        return result\n",
    "\n",
    "    def print_log(self, log):\n",
    "        if self.verbose:\n",
    "            print(log)\n",
    "\n",
    "    def get_max_id(self):\n",
    "        self.max_id += 1\n",
    "        return self.max_id\n",
    "\n",
    "    def get_groups_and_edus(self, tree, terminal=False):\n",
    "        groups = []\n",
    "        edus = []\n",
    "\n",
    "        if not tree.left:\n",
    "            edus.append(Segment(tree.id, parent=-2, relname='antithesis', text=tree.text))\n",
    "            return groups, edus\n",
    "\n",
    "        if not tree.left.left:\n",
    "            if tree.nuclearity == \"SN\":\n",
    "                edus.append(Segment(tree.left.id, parent=tree.right.id, relname=tree.relation, text=tree.left.text))\n",
    "            elif tree.nuclearity == \"NS\":\n",
    "                edus.append(Segment(tree.left.id, parent=tree.id, relname='span', text=tree.left.text))\n",
    "            else:\n",
    "                edus.append(Segment(tree.left.id, parent=tree.id, relname=tree.relation, text=tree.left.text))\n",
    "\n",
    "        else:\n",
    "            if tree.nuclearity == \"SN\":\n",
    "                groups.append(Group(tree.left.id, type='span', parent=tree.right.id, relname=tree.relation))\n",
    "            elif tree.nuclearity == \"NS\":\n",
    "                groups.append(Group(tree.left.id, type='span', parent=tree.id, relname='span'))\n",
    "            else:\n",
    "                groups.append(Group(tree.left.id, type='multinuc', parent=tree.id, relname=tree.relation))\n",
    "\n",
    "            _groups, _edus = self.get_groups_and_edus(tree.left)\n",
    "            groups += _groups\n",
    "            edus += _edus\n",
    "\n",
    "        if not tree.right.left:\n",
    "            if tree.nuclearity == \"SN\":\n",
    "                edus.append(Segment(tree.right.id, parent=tree.id, relname='span', text=tree.right.text))\n",
    "            elif tree.nuclearity == \"NS\":\n",
    "                edus.append(Segment(tree.right.id, parent=tree.left.id, relname=tree.relation, text=tree.right.text))\n",
    "            else:\n",
    "                edus.append(Segment(tree.right.id, parent=tree.id, relname=tree.relation, text=tree.right.text))\n",
    "\n",
    "        else:\n",
    "            if tree.nuclearity == \"SN\":\n",
    "                groups.append(Group(tree.right.id, type='multinuc', parent=tree.id, relname='span'))\n",
    "            elif tree.nuclearity == \"NS\":\n",
    "                groups.append(Group(tree.right.id, type='span', parent=tree.left.id, relname=tree.relation))\n",
    "            else:\n",
    "                groups.append(Group(tree.right.id, type='span', parent=tree.id, relname=tree.relation))\n",
    "\n",
    "            _groups, _edus = self.get_groups_and_edus(tree.right)\n",
    "            groups += _groups\n",
    "            edus += _edus\n",
    "\n",
    "        if terminal:\n",
    "            if len(edus) > 1:\n",
    "                if tree.nuclearity == \"NN\":\n",
    "                    groups.append(Root(tree.id, type='multinuc'))\n",
    "                else:\n",
    "                    groups.append(Root(tree.id))\n",
    "\n",
    "        return groups, edus\n",
    "\n",
    "    def make_body(self, tree):\n",
    "        groups, edus = self.get_groups_and_edus(tree, terminal=True)\n",
    "        result = '\\t<body>\\n'\n",
    "        for edu in edus + groups:\n",
    "            result += '\\t\\t' + str(edu) + '\\n'\n",
    "        result += '\\t</body>\\n'\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class ForestExporter:\n",
    "    def __init__(self, encoding='cp1251', verbose=False):\n",
    "        self._encoding = encoding\n",
    "        self._tree_exporter = Exporter(self._encoding, verbose=verbose)\n",
    "\n",
    "    def __call__(self, trees, filename):\n",
    "\n",
    "        with open(filename, 'w', encoding=self._encoding) as fo:\n",
    "            fo.write('<rst>\\n')\n",
    "            fo.write(self.make_header(trees))\n",
    "            fo.write(self.make_body(trees))\n",
    "            fo.write('</rst>')\n",
    "\n",
    "    def compile_relation_set(self, trees):\n",
    "        result = []\n",
    "\n",
    "        for tree in trees:\n",
    "            result += list(set(self._tree_exporter.compile_relation_set(tree)))\n",
    "\n",
    "        result = [value if value != \"elementary__\" else \"antithesis_NN\" for value in result]\n",
    "        return result\n",
    "\n",
    "    def make_header(self, trees):\n",
    "        relations = list(set(self.compile_relation_set(trees)))\n",
    "\n",
    "        result = '\\t<header>\\n'\n",
    "        result += '\\t\\t<relations>\\n'\n",
    "        for rel in relations:\n",
    "            _relname, _type = rel.split('_')\n",
    "            _type = 'multinuc' if _type == 'NN' else 'rst'\n",
    "            result += f'\\t\\t\\t<rel name=\"{_relname}\" type=\"{_type}\" />\\n'\n",
    "        result += '\\t\\t</relations>\\n'\n",
    "        result += '\\t</header>\\n'\n",
    "\n",
    "        return result\n",
    "\n",
    "    def make_body(self, trees):\n",
    "        groups, edus = [], []\n",
    "        roots = []\n",
    "\n",
    "        for tree in trees:\n",
    "            _groups, _edus = self._tree_exporter.get_groups_and_edus(tree, terminal=True)\n",
    "\n",
    "            if len(edus) > 1:\n",
    "                if tree.nuclearity == \"NN\":\n",
    "                    roots.append(Root(tree.id, type='multinuc'))\n",
    "                else:\n",
    "                    roots.append(Root(tree.id))\n",
    "\n",
    "            groups += _groups\n",
    "            edus += _edus\n",
    "\n",
    "        result = '\\t<body>\\n'\n",
    "        for edu in edus + groups:\n",
    "            result += '\\t\\t' + str(edu) + '\\n'\n",
    "        result += '\\t</body>\\n'\n",
    "\n",
    "        return result.replace('\\u2015', '-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.9 ms, sys: 100 ms, total: 121 ms\n",
      "Wall time: 15.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(5)\n",
    "result = pool.map(parse_golds, test)\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [(f[0], f[1][0], f[1][1], f[1][2]) for f in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['filename', 'true_pos', 'all_parsed', 'all_gold'], data=result)\n",
    "difference = results['all_parsed'] - results['true_pos']\n",
    "results['all_gold'] += difference\n",
    "results['true_pos'] = results['all_parsed']\n",
    "\n",
    "results['recall'] = results['true_pos'] / results['all_gold']\n",
    "results['precision'] = results['true_pos'] / results['all_parsed']\n",
    "results['F1'] = 2 * results['precision'] * results['recall'] / (results['precision'] + results['recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>true_pos</th>\n",
       "      <th>all_parsed</th>\n",
       "      <th>all_gold</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>blogs_21</td>\n",
       "      <td>265</td>\n",
       "      <td>265</td>\n",
       "      <td>272</td>\n",
       "      <td>0.974265</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.986965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news2_34</td>\n",
       "      <td>159</td>\n",
       "      <td>159</td>\n",
       "      <td>161</td>\n",
       "      <td>0.987578</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.993750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>blogs_99</td>\n",
       "      <td>200</td>\n",
       "      <td>200</td>\n",
       "      <td>202</td>\n",
       "      <td>0.990099</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>blogs_39</td>\n",
       "      <td>202</td>\n",
       "      <td>202</td>\n",
       "      <td>204</td>\n",
       "      <td>0.990196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.995074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>news1_28</td>\n",
       "      <td>164</td>\n",
       "      <td>164</td>\n",
       "      <td>165</td>\n",
       "      <td>0.993939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>news1_23</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>blogs_86</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>136</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>blogs_63</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>blogs_52</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>blogs_31</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>blogs_60</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>blogs_69</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>news1_29</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>news1_42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>news1_24</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>news2_4</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>news1_78</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>61</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>news2_16</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news1_77</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news1_25</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news2_48</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>205</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news2_38</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>227</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>news1_47</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>blogs_72</td>\n",
       "      <td>243</td>\n",
       "      <td>243</td>\n",
       "      <td>243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>blogs_17</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    filename  true_pos  all_parsed  all_gold    recall  precision        F1\n",
       "16  blogs_21       265         265       272  0.974265        1.0  0.986965\n",
       "8   news2_34       159         159       161  0.987578        1.0  0.993750\n",
       "17  blogs_99       200         200       202  0.990099        1.0  0.995025\n",
       "22  blogs_39       202         202       204  0.990196        1.0  0.995074\n",
       "5   news1_28       164         164       165  0.993939        1.0  0.996960\n",
       "0   news1_23        83          83        83  1.000000        1.0  1.000000\n",
       "21  blogs_86       136         136       136  1.000000        1.0  1.000000\n",
       "20  blogs_63        84          84        84  1.000000        1.0  1.000000\n",
       "19  blogs_52        91          91        91  1.000000        1.0  1.000000\n",
       "18  blogs_31       155         155       155  1.000000        1.0  1.000000\n",
       "15  blogs_60        51          51        51  1.000000        1.0  1.000000\n",
       "14  blogs_69        96          96        96  1.000000        1.0  1.000000\n",
       "13  news1_29        79          79        79  1.000000        1.0  1.000000\n",
       "12  news1_42        42          42        42  1.000000        1.0  1.000000\n",
       "11  news1_24        45          45        45  1.000000        1.0  1.000000\n",
       "10   news2_4        33          33        33  1.000000        1.0  1.000000\n",
       "9   news1_78        61          61        61  1.000000        1.0  1.000000\n",
       "7   news2_16        47          47        47  1.000000        1.0  1.000000\n",
       "6   news1_77        54          54        54  1.000000        1.0  1.000000\n",
       "4   news1_25        39          39        39  1.000000        1.0  1.000000\n",
       "3   news2_48       205         205       205  1.000000        1.0  1.000000\n",
       "2   news2_38       227         227       227  1.000000        1.0  1.000000\n",
       "1   news1_47       104         104       104  1.000000        1.0  1.000000\n",
       "23  blogs_72       243         243       243  1.000000        1.0  1.000000\n",
       "24  blogs_17        93          93        93  1.000000        1.0  1.000000"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values('F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9987109548727691"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.F1.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
