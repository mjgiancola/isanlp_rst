{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Discourse segmentation\n",
    "\n",
    "(Customizes the multilingual RST segmentation system introduced in https://www.aclweb.org/anthology/W19-2715/ )\n",
    "\n",
    "Create new train and test sets for Russian; make up new models configs and modify train/evaluation scripts.\n",
    " \n",
    "Output:\n",
    "\n",
    " - models/segmenter/rus.rst.rrt_train{.conll, .conll2003}\n",
    " - models/segmenter/rus.rst.rrt_dev{.conll, .conll2003}\n",
    " - models/segmenter/rus.rst.rrt_test{.conll, .conll2003}\n",
    " - configs & scripts for segmentation models\n",
    " - models/segmenter/.../model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 1. Prepare dataset for model training and evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.file_reading import SYMBOL_MAP\n",
    "\n",
    "def prepare_token(token):\n",
    "    for key, value in SYMBOL_MAP.items():\n",
    "        token = token.replace(key, value)\n",
    "        \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp.utils.annotation_conll_converter import AnnotationCONLLConverter\n",
    "\n",
    "converter = AnnotationCONLLConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from isanlp.annotation import Token, Sentence\n",
    "\n",
    "\n",
    "def split_by_paragraphs(annot_text, annot_tokens, annot_lemma, annot_morph, annot_postag,\n",
    "                        annot_syntax_dep_tree):\n",
    "\n",
    "        def split_on_two(sents, boundary):\n",
    "            list_sum = lambda l: sum([len(sublist) for sublist in l])\n",
    "\n",
    "            i = 1\n",
    "            while list_sum(sents[:i]) < boundary and i < len(sents):\n",
    "                i += 1\n",
    "\n",
    "            intersentence_boundary = min(len(sents[i - 1]), boundary - list_sum(sents[:i - 1]))\n",
    "            return (sents[:i - 1] + [sents[i - 1][:intersentence_boundary]],\n",
    "                    [sents[i - 1][intersentence_boundary:]] + sents[i:])\n",
    "\n",
    "        def recount_sentences(chunk):\n",
    "            sentences = []\n",
    "            lemma = []\n",
    "            morph = []\n",
    "            postag = []\n",
    "            syntax_dep_tree = []\n",
    "            tokens_cursor = 0\n",
    "\n",
    "            for i, sent in enumerate(chunk['syntax_dep_tree']):\n",
    "                if len(sent) > 0:\n",
    "                    sentences.append(Sentence(tokens_cursor, tokens_cursor + len(sent)))\n",
    "                    lemma.append(chunk['lemma'][i])\n",
    "                    morph.append(chunk['morph'][i])\n",
    "                    postag.append(chunk['postag'][i])\n",
    "                    syntax_dep_tree.append(chunk['syntax_dep_tree'][i])\n",
    "                    tokens_cursor += len(sent)\n",
    "\n",
    "            chunk['sentences'] = sentences\n",
    "            chunk['lemma'] = lemma\n",
    "            chunk['morph'] = morph\n",
    "            chunk['postag'] = postag\n",
    "            chunk['ud_postag'] = postag\n",
    "            chunk['syntax_dep_tree'] = syntax_dep_tree\n",
    "\n",
    "            return chunk\n",
    "\n",
    "        chunks = []\n",
    "        prev_right_boundary = -1\n",
    "\n",
    "        for i, token in enumerate(annot_tokens):\n",
    "\n",
    "            if i < len(annot_tokens)-1 and '\\n' in annot_text[token.end:annot_tokens[i + 1].begin]:\n",
    "                if prev_right_boundary > -1:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[annot_tokens[prev_right_boundary].end:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[prev_right_boundary + 1:i + 1]\n",
    "                    }\n",
    "                else:\n",
    "                    chunk = {\n",
    "                        'text': annot_text[:token.end + 1].strip(),\n",
    "                        'tokens': annot_tokens[:i + 1]\n",
    "                    }\n",
    "\n",
    "                lemma, annot_lemma = split_on_two(annot_lemma, i - prev_right_boundary)\n",
    "                morph, annot_morph = split_on_two(annot_morph, i - prev_right_boundary)\n",
    "                postag, annot_postag = split_on_two(annot_postag, i - prev_right_boundary)\n",
    "                syntax_dep_tree, annot_syntax_dep_tree = split_on_two(annot_syntax_dep_tree, i - prev_right_boundary)\n",
    "\n",
    "                chunk.update({\n",
    "                    'lemma': lemma,\n",
    "                    'morph': morph,\n",
    "                    'postag': postag,\n",
    "                    'ud_postag': postag,\n",
    "                    'syntax_dep_tree': syntax_dep_tree,\n",
    "                })\n",
    "                chunks.append(recount_sentences(chunk))\n",
    "\n",
    "                prev_right_boundary = i  # number of last token in the last chunk\n",
    "\n",
    "        chunk = {\n",
    "            'text': annot_text[annot_tokens[prev_right_boundary].end:].strip(),\n",
    "            'tokens': annot_tokens[prev_right_boundary + 1:],\n",
    "            'lemma': annot_lemma,\n",
    "            'morph': annot_morph,\n",
    "            'postag': annot_postag,\n",
    "            'ud_postag': annot_postag,\n",
    "            'syntax_dep_tree': annot_syntax_dep_tree,\n",
    "        }\n",
    "\n",
    "        chunks.append(recount_sentences(chunk))\n",
    "        return chunks\n",
    "    \n",
    "\n",
    "def annot2tags(annot, edus):\n",
    "    tags = []\n",
    "    cursor = 0\n",
    "    from_prev_begin_to_here = ''\n",
    "\n",
    "    for sentence in range(len(annot['sentences'])):\n",
    "        sentence_tags = []\n",
    "        previous_first_token = 0\n",
    "        previous_edu = ''\n",
    "\n",
    "        for token in range(annot['sentences'][sentence].begin, annot['sentences'][sentence].end):\n",
    "            is_first_token = False\n",
    "            token_text = prepare_token(annot['tokens'][token].text).strip()  # Look at the current token\n",
    "            \n",
    "            if cursor != len(edus):              \n",
    "                tmp_edu = prepare_token(edus[cursor]).strip()  # Look at the current EDU\n",
    "                # token_text = annot['text'][annot['tokens'][token].begin:annot['tokens'][token].end].strip()\n",
    "\n",
    "                if tmp_edu.startswith(token_text):\n",
    "                    if previous_edu:\n",
    "                        # from_prev_begin_to_here = prepare_token(\n",
    "                        #     annot['text'][annot['tokens'][previous_first_token].begin:annot['tokens'][token].begin].strip())\n",
    "                        # print('***', ''.join(from_prev_begin_to_here.strip().split()), '***', ''.join(previous_edu.strip().split()))\n",
    "                        \n",
    "                        if len(''.join(from_prev_begin_to_here.strip().split())) == len(''.join(previous_edu.strip().split())):\n",
    "                            is_first_token = True\n",
    "                            from_prev_begin_to_here = token_text\n",
    "                            previous_first_token = token\n",
    "                            previous_edu = tmp_edu\n",
    "                            cursor += 1\n",
    "                        else:\n",
    "                            from_prev_begin_to_here += token_text\n",
    "                    else:\n",
    "                        is_first_token = True\n",
    "                        from_prev_begin_to_here = token_text\n",
    "                        previous_first_token = token\n",
    "                        previous_edu = tmp_edu\n",
    "                        cursor += 1\n",
    "                else:\n",
    "                    from_prev_begin_to_here += token_text\n",
    "\n",
    "            tag = 'BeginSeg=Yes' if is_first_token else '_'\n",
    "            sentence_tags.append(tag)\n",
    "\n",
    "        tags.append(sentence_tags)\n",
    "\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils.train_test_split import split_rstreebank, split_essays\n",
    "\n",
    "print('Loading RSTreebank:')\n",
    "train, dev, test = split_rstreebank('./data_ru')\n",
    "print('Train length:', len(train), 'Dev length:', len(dev), 'Test length:', len(test), '(files)')\n",
    "\n",
    "print('\\nLoading Essays:')\n",
    "train_dep, dev_dep, test_dep = split_essays('./dep_data')\n",
    "print('Train length:', len(train_dep), 'Dev length:', len(dev_dep), 'Test length:', len(test_dep), '(files)')\n",
    "\n",
    "train += train_dep\n",
    "dev += dev_dep\n",
    "test += test_dep\n",
    "print('\\nOverall\\nTrain length:', len(train), 'Dev length:', len(dev), 'Test length:', len(test), '(files)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from utils.file_reading import read_annotation, read_edus\n",
    "import re\n",
    "\n",
    "output_dir = 'models/segmenter'\n",
    "! rm -r $output_dir\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "TRAIN_FILE = os.path.join(output_dir, 'rus.rst.rrt_train.conll')\n",
    "DEV_FILE = os.path.join(output_dir, 'rus.rst.rrt_dev.conll')\n",
    "TEST_FILE = os.path.join(output_dir, 'rus.rst.rrt_test.conll')\n",
    "MAX_LEN = 500\n",
    "\n",
    "def preprocess(files, subset='train'):\n",
    "    print(f'preprocess {subset} set')\n",
    "    \n",
    "    if subset == 'train':\n",
    "        output_file = TRAIN_FILE\n",
    "    else:\n",
    "        output_file = DEV_FILE if subset=='dev' else TEST_FILE\n",
    "    \n",
    "    with open(output_file, 'w') as fo:\n",
    "        for filename in tqdm(files):\n",
    "            filename = filename.replace('.edus', '')\n",
    "            annot = read_annotation(filename)\n",
    "            edus = read_edus(filename)\n",
    "            last_edu = 0\n",
    "\n",
    "            for i, chunk in enumerate(split_by_paragraphs(  # self,\n",
    "                    annot['text'],\n",
    "                    annot['tokens'],\n",
    "                    annot['lemma'],\n",
    "                    annot['morph'],\n",
    "                    annot['postag'],\n",
    "                    annot['syntax_dep_tree'])):\n",
    "\n",
    "                sentence = 0\n",
    "                token = 0\n",
    "                chunk['text'] = annot['text']\n",
    "                tags = annot2tags(chunk, edus[last_edu:])\n",
    "                \n",
    "                for string in converter(filename.replace('data/', ''), chunk):                    \n",
    "                    if string.startswith('# newdoc id ='):\n",
    "                        sentence = 0\n",
    "                        token = 0\n",
    "                        fo.write(string + '\\n')\n",
    "\n",
    "                    elif string == '\\n':\n",
    "                        fo.write(string)\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "\n",
    "                    else:\n",
    "                        if ' ' in string:\n",
    "                            string = re.sub(r' .*\\t', '\\t', string)\n",
    "                        if 'www' in string:\n",
    "                            string = re.sub(r'www[^\\t]*', '_html_', string)\n",
    "                        if 'http' in string:\n",
    "                            string = re.sub(r'http[^ \\t]*', '_html_', string)\n",
    "                            \n",
    "                        string = prepare_token(string)                        \n",
    "                        fo.write(string + '\\t' + tags[sentence][token] + '\\n')\n",
    "                        \n",
    "                        if tags[sentence][token] != '_':\n",
    "                            last_edu += 1\n",
    "                        \n",
    "                        token += 1\n",
    "\n",
    "                    if token == MAX_LEN:\n",
    "                        print(filename + ' ::: occured very long sentence; truncate to ' + str(MAX_LEN) + ' tokens.')\n",
    "                        fo.write('\\n')\n",
    "                        sentence += 1\n",
    "                        token = 0\n",
    "                        break\n",
    "\n",
    "\n",
    "preprocess(train, subset='train')\n",
    "preprocess(dev, subset='dev')\n",
    "preprocess(test, subset='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we will use Conll2003 dataset reader, it will expect this particular format instead of Conll-U we have now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for filename in (TRAIN_FILE, DEV_FILE, TEST_FILE):\n",
    "    contents = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for string in f:\n",
    "            if string.startswith('# newdoc id ='):\n",
    "                pass\n",
    "                #contents.append('-DOCSTART-\\t-X-\\t-X-\\tO')\n",
    "            \n",
    "            elif string == '\\n':\n",
    "                contents.append(string)\n",
    "                \n",
    "            else:\n",
    "                tokid, form, lemma, upos, xpos, feats, head, deprel, deps, segmenttag = string.split('\\t')\n",
    "                segmenttag = 'B-S' if segmenttag.strip() == 'BeginSeg=Yes' else 'O'\n",
    "                contents.append('\\t'.join([form, 'O', 'O', segmenttag]))\n",
    "                \n",
    "    outname = filename.replace('.conll', '.conll2003')\n",
    "    with open(outname, 'w') as f:\n",
    "        f.write('\\n'.join(contents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configs 1-5 (Outdated) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Baseline model (BERT-M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/bertM.jsonnet\n",
    "\n",
    "\n",
    "// Configuration for a named entity recognization model based on:\n",
    "//   Peters, Matthew E. et al. “Deep contextualized word representations.” NAACL-HLT (2018).\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": std.extVar(\"BERT_VOCAB\"),\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"simple_tagger\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "        },\n",
    "        \"token_embedders\": {\n",
    "            \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": std.extVar(\"BERT_WEIGHTS\")\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 128,\n",
    "                    \"ngram_filter_sizes\": [3],\n",
    "                    \"conv_layer_activation\": \"relu\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 768 + 128,\n",
    "        \"hidden_size\": 100,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bidirectional\": true\n",
    "    },\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"bert_adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. CRF model (BERT-M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/bertM_crf.jsonnet\n",
    "\n",
    "// Configuration for a named entity recognization model based on:\n",
    "//   Peters, Matthew E. et al. “Deep contextualized word representations.” NAACL-HLT (2018).\n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": \"bert-base-multilingual-cased\",\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "        },\n",
    "        \"token_embedders\": {\n",
    "            \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": \"bert-base-multilingual-cased\",\n",
    "            },\n",
    "            \"token_characters\": {\n",
    "                \"type\": \"character_encoding\",\n",
    "                \"embedding\": {\n",
    "                    \"embedding_dim\": 16\n",
    "                },\n",
    "                \"encoder\": {\n",
    "                    \"type\": \"cnn\",\n",
    "                    \"embedding_dim\": 16,\n",
    "                    \"num_filters\": 128,\n",
    "                    \"ngram_filter_sizes\": [3],\n",
    "                    \"conv_layer_activation\": \"relu\",\n",
    "                },\n",
    "                \"dropout\": 0.2,\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 768 + 128,\n",
    "        \"hidden_size\": 100,\n",
    "        \"num_layers\": 1,\n",
    "        \"dropout\": 0.5,\n",
    "        \"bidirectional\": true\n",
    "    },\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"bert_adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. CRF model (ELMo)\n",
    "\n",
    "ELMo embedder: Place ``model.hdf5`` and ``options.json`` files from ``http://vectors.nlpl.eu/repository/20/195.zip`` in ``models/rsv_elmo/`` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/elmo.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo, modified slightly from\n",
    "// the version included in \"Deep Contextualized Word Representations\",\n",
    "// taken from AllenNLP examples\n",
    "// modified for the disrpt discourse segmentation shared task -- 2019 \n",
    "{\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+128,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "      [\n",
    "        \"scalar_parameters\",\n",
    "        {\n",
    "          \"type\": \"l2\",\n",
    "          \"alpha\": 0.01,\n",
    "        }\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. CRF model (ELMo+fastText)\n",
    "\n",
    "fastText embedder: place ``http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec`` in ``models/``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/elmo_ft.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo, modified slightly from\n",
    "// the version included in \"Deep Contextualized Word Representations\",\n",
    "// taken from AllenNLP examples\n",
    "// modified for the disrpt discourse segmentation shared task -- 2019 \n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": false\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     }\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"tokens\": {\n",
    "            \"type\": \"embedding\",\n",
    "            \"embedding_dim\": 300,\n",
    "            \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "            \"trainable\": false\n",
    "        },\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.25\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+128+300,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "      [\n",
    "        \"scalar_parameters\",\n",
    "        {\n",
    "          \"type\": \"l2\",\n",
    "          \"alpha\": 0.01,\n",
    "        }\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 0.001\n",
    "    },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. CRF model (ELMo + RuBERT)\n",
    "\n",
    "RuBERT embedder: unpack ``http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_pt.tar.gz`` in ``models/``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/elmo_rubert.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo and RuBERT\n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      //\"tokens\": {\n",
    "      //  \"type\": \"single_id\",\n",
    "      //  \"lowercase_tokens\": true\n",
    "      //},\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     },\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": std.extVar(\"BERT_VOCAB\"),\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "            \"elmo\": [\"elmo\"],\n",
    "            \"tokens\": [\"tokens\"],\n",
    "        },\n",
    "      \"token_embedders\": {\n",
    "        //\"tokens\": {\n",
    "        //    \"type\": \"embedding\",\n",
    "        //    \"embedding_dim\": 300,\n",
    "        //    \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "        //    \"trainable\": true\n",
    "        //},\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": std.extVar(\"BERT_WEIGHTS\"),\n",
    "                \"requires_grad\": true,\n",
    "                \"top_layer_only\": false\n",
    "            },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 1024+128+768,\n",
    "      \"hidden_size\": 200,\n",
    "      \"num_layers\": 2,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "            [\n",
    "                \"scalar_parameters\",\n",
    "                {\n",
    "                    \"alpha\": 0.01,\n",
    "                    \"type\": \"l2\"\n",
    "                }\n",
    "            ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"bert_adam\",\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 2,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. RuBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/rubert.jsonnet\n",
    "\n",
    "// Configuration for the NER model with ELMo, modified slightly from\n",
    "// the version included in \"Deep Contextualized Word Representations\",\n",
    "// taken from AllenNLP examples\n",
    "// modified for the disrpt discourse segmentation shared task -- 2019 \n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      //\"tokens\": {\n",
    "      //  \"type\": \"single_id\",\n",
    "      //  \"lowercase_tokens\": true\n",
    "      //},\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "#       \"elmo\": {\n",
    "#         \"type\": \"elmo_characters\"\n",
    "#      },\n",
    "      \"bert\": {\n",
    "          \"type\": \"bert-pretrained\",\n",
    "          \"pretrained_model\": std.extVar(\"BERT_VOCAB\"),\n",
    "          \"do_lowercase\": false,\n",
    "          \"use_starting_offsets\": true\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": std.extVar(\"TRAIN_DATA_PATH\"),\n",
    "  \"validation_data_path\": std.extVar(\"TEST_A_PATH\"),\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "        \"allow_unmatched_keys\": true,\n",
    "        \"embedder_to_indexer_map\": {\n",
    "            \"bert\": [\"bert\", \"bert-offsets\"],\n",
    "            \"token_characters\": [\"token_characters\"],\n",
    "            \"elmo\": [\"elmo\"],\n",
    "            \"tokens\": [\"tokens\"],\n",
    "        },\n",
    "      \"token_embedders\": {\n",
    "        //\"tokens\": {\n",
    "        //    \"type\": \"embedding\",\n",
    "        //    \"embedding_dim\": 300,\n",
    "        //    \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "        //    \"trainable\": true\n",
    "        //},\n",
    "#         \"elmo\":{\n",
    "#             \"type\": \"elmo_token_embedder\",\n",
    "#             \"options_file\": \"rsv_elmo/options.json\",\n",
    "#             \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "#             \"do_layer_norm\": false,\n",
    "#             \"dropout\": 0.0\n",
    "#         },\n",
    "        \"bert\": {\n",
    "                \"type\": \"bert-pretrained\",\n",
    "                \"pretrained_model\": std.extVar(\"BERT_WEIGHTS\"),\n",
    "                \"requires_grad\": true,\n",
    "                \"top_layer_only\": false\n",
    "            },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"num_filters\": 128,\n",
    "                \"ngram_filter_sizes\": [3],\n",
    "                \"conv_layer_activation\": \"relu\"\n",
    "            },\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "      \"type\": \"lstm\",\n",
    "      \"input_size\": 128+768,\n",
    "      \"hidden_size\": 100,\n",
    "      \"num_layers\": 1,\n",
    "      \"dropout\": 0.5,\n",
    "      \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": [\n",
    "            [\n",
    "                \"scalar_parameters\",\n",
    "                {\n",
    "                    \"alpha\": 0.01,\n",
    "                    \"type\": \"l2\"\n",
    "                }\n",
    "            ]\n",
    "    ]\n",
    "  },\n",
    "  \"iterator\": {\n",
    "    \"type\": \"basic\",\n",
    "    \"batch_size\": 2\n",
    "  },\n",
    "  \"trainer\": {\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"bert_adam\",\n",
    "            \"lr\": 0.001\n",
    "        },\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"num_serialized_models_to_keep\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"patience\": 3,\n",
    "    \"cuda_device\": 1\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configs 6-... (Brand new) \n",
    "Allennlp v2.9.3, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/rubert.jsonnet\n",
    "\n",
    "\n",
    "local BERT_MODEL = \"DeepPavlov/rubert-base-cased\";\n",
    "local TRAIN_FILE = \"segmenter/rus.rst.rrt_train.conll2003\";\n",
    "local DEV_FILE = \"segmenter/rus.rst.rrt_dev.conll2003\";\n",
    "\n",
    "local CNN_CHAR_HIDDEN = std.parseJson(std.extVar('CNN_CHAR_HIDDEN'));  // 128\n",
    "local LSTM_HIDDEN = std.parseJson(std.extVar('LSTM_HIDDEN'));  // 100\n",
    "local LR = std.parseJson(std.extVar('LR'));  // 100\n",
    "\n",
    "local NUM_EPOCHS = 30;\n",
    "\n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 30\n",
    "      },\n",
    "      \"bert\": {\n",
    "          \"type\": \"pretrained_transformer_mismatched\",\n",
    "          \"model_name\": BERT_MODEL,\n",
    "          \"max_length\": 512,\n",
    "      },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": TRAIN_FILE,\n",
    "  \"validation_data_path\": DEV_FILE,\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"bert\": {\n",
    "                \"type\": \"pretrained_transformer_mismatched\",\n",
    "                \"model_name\": BERT_MODEL,\n",
    "                \"train_parameters\": true,\n",
    "                \"last_layer_only\": false\n",
    "            },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16,\n",
    "                \"sparse\": false,\n",
    "                \"vocab_namespace\": \"token_characters\"\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"gru\",\n",
    "                \"input_size\": $.model.text_field_embedder.token_embedders.token_characters.embedding.embedding_dim,\n",
    "                \"hidden_size\": CNN_CHAR_HIDDEN,\n",
    "                \"num_layers\": 1,\n",
    "                \"bidirectional\": true,\n",
    "            },\n",
    "            \"dropout\": 0.2\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"pass_through\",\n",
    "        \"input_dim\": 768+CNN_CHAR_HIDDEN*2,\n",
    "    },\n",
    "  },\n",
    "  \"data_loader\": {\n",
    "    \"batch_sampler\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 8,\n",
    "        \"padding_noise\": 0.0,\n",
    "    },\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 3,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"cuda_device\": 1,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adamw\",\n",
    "      \"lr\": LR,\n",
    "      \"parameter_groups\": [[[\"bias\", \"LayerNorm\\\\.weight\", \"layer_norm\\\\.weight\"], {\"weight_decay\": 0}]],\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter/elmo_ft.jsonnet\n",
    "\n",
    "\n",
    "local BERT_MODEL = \"DeepPavlov/rubert-base-cased\";\n",
    "local TRAIN_FILE = \"segmenter/rus.rst.rrt_train.conll2003\";\n",
    "local DEV_FILE = \"segmenter/rus.rst.rrt_dev.conll2003\";\n",
    "\n",
    "local LSTM_HIDDEN = std.parseJson(std.extVar('LSTM_HIDDEN'));  // 100\n",
    "local LR = std.parseJson(std.extVar('LR'));  // 0.001\n",
    "\n",
    "local NUM_EPOCHS = 30;\n",
    "\n",
    "{\n",
    "\n",
    "  \"dataset_reader\": {\n",
    "    \"type\": \"conll2003\",\n",
    "    \"tag_label\": \"ner\",\n",
    "    \"coding_scheme\": \"BIOUL\",\n",
    "    \"token_indexers\": {\n",
    "      \"tokens\": {\n",
    "        \"type\": \"single_id\",\n",
    "        \"lowercase_tokens\": true,\n",
    "      },\n",
    "      \"token_characters\": {\n",
    "        \"type\": \"characters\",\n",
    "        \"min_padding_length\": 3\n",
    "      },\n",
    "      \"elmo\": {\n",
    "        \"type\": \"elmo_characters\"\n",
    "     },\n",
    "    }\n",
    "  },\n",
    "  \"train_data_path\": TRAIN_FILE,\n",
    "  \"validation_data_path\": DEV_FILE,\n",
    "  \"model\": {\n",
    "    \"type\": \"crf_tagger\",\n",
    "    \"dropout\": 0.25,\n",
    "    \"calculate_span_f1\": true,\n",
    "    \"label_encoding\": \"BIOUL\",\n",
    "    \"text_field_embedder\": {\n",
    "      \"token_embedders\": {\n",
    "        \"tokens\": {\n",
    "            \"type\": \"embedding\",\n",
    "            \"embedding_dim\": 300,\n",
    "            \"pretrained_file\": \"ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec\",\n",
    "            \"trainable\": false\n",
    "        },\n",
    "        \"elmo\":{\n",
    "            \"type\": \"elmo_token_embedder\",\n",
    "            \"options_file\": \"rsv_elmo/options.json\",\n",
    "            \"weight_file\": \"rsv_elmo/model.hdf5\",\n",
    "            \"do_layer_norm\": false,\n",
    "            \"dropout\": 0.0\n",
    "        },\n",
    "        \"token_characters\": {\n",
    "            \"type\": \"character_encoding\",\n",
    "            \"dropout\": 0.25,\n",
    "            \"embedding\": {\n",
    "                \"embedding_dim\": 16\n",
    "            },\n",
    "            \"encoder\": {\n",
    "                \"type\": \"cnn\",\n",
    "                \"conv_layer_activation\": \"relu\",\n",
    "                \"embedding_dim\": 16,\n",
    "                \"ngram_filter_sizes\": [\n",
    "                    3\n",
    "                ],\n",
    "                \"num_filters\": 64\n",
    "            }\n",
    "        },\n",
    "      }\n",
    "    },\n",
    "    \"encoder\": {\n",
    "        \"type\": \"lstm\",\n",
    "        \"input_size\": 1024+$.model.text_field_embedder.token_embedders.tokens.embedding_dim+$.model.text_field_embedder.token_embedders.token_characters.encoder.num_filters,\n",
    "        \"hidden_size\": LSTM_HIDDEN,\n",
    "        \"num_layers\": 2,\n",
    "        \"dropout\": 0.25,\n",
    "        \"bidirectional\": true\n",
    "    },\n",
    "    \"regularizer\": {\n",
    "        \"regexes\": [\n",
    "            [\n",
    "                \"scalar_parameters\",\n",
    "                {\n",
    "                    \"alpha\": 0.001,\n",
    "                    \"type\": \"l2\"\n",
    "                }\n",
    "            ]\n",
    "        ]\n",
    "    },\n",
    "  },\n",
    "  \"data_loader\": {\n",
    "    \"batch_sampler\": {\n",
    "        \"type\": \"bucket\",\n",
    "        \"batch_size\": 8,\n",
    "        \"padding_noise\": 0.0,\n",
    "    },\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"patience\": 3,\n",
    "    \"grad_norm\": 5.0,\n",
    "    \"num_serialized_models_to_keep\": 1,\n",
    "    \"validation_metric\": \"+f1-measure-overall\",\n",
    "    \"cuda_device\": 1,\n",
    "    \"optimizer\": {\n",
    "      \"type\": \"adamw\",\n",
    "      \"lr\": LR,\n",
    "    },\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/segmenter_params.json\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"type\": \"int\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"LSTM_HIDDEN\",\n",
    "      \"low\": 64,\n",
    "      \"high\": 256\n",
    "    }\n",
    "  },\n",
    "  {\n",
    "    \"type\": \"float\",\n",
    "    \"attributes\": {\n",
    "      \"name\": \"LR\",\n",
    "      \"low\": 2e-4,\n",
    "      \"high\": 2e-2,\n",
    "      \"log\": true\n",
    "    }\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/tune_segmenter.sh\n",
    "\n",
    "export METHOD=elmo_ft\n",
    "export STUDY_NAME=segmenter_tuning_2\n",
    "mkdir optuna\n",
    "rm -r optuna/segmenter_$METHOD\n",
    "mkdir optuna/segmenter_$METHOD\n",
    "\n",
    "# optuna delete-study --study-name $STUDY_NAME\n",
    "allennlp tune segmenter/${METHOD}.jsonnet segmenter_params.json --serialization-dir optuna/segmenter_$METHOD \\\n",
    "    --study-name $STUDY_NAME \\\n",
    "    --skip-if-exists \\\n",
    "    --metrics best_validation_f1-measure-overall \\\n",
    "    --direction maximize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collect_optuna_results(path):\n",
    "    for trial in glob(os.path.join(path, 'trial_*/')):\n",
    "        try:\n",
    "            metrics = json.load(open(os.path.join(trial, 'metrics.json')))\n",
    "            print(trial, metrics['best_validation_f1-measure-overall'])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "collect_optuna_results('models/optuna/segmenter_elmo_ft/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(sorted([0.8663200891198907, # Trial 5\n",
    "                  0.8804893636446136, # Trial 6\n",
    "                  0.8804893636446136, # Trial 7\n",
    "                  0.8928604329801436, # Trial 8 [x]\n",
    "                  0.8899245036815606, # Trial 9\n",
    "                  0.8914864242981554, # Trial 10\n",
    "                  0.8900630329995792, # Trial 11\n",
    "                  0.8923220102372698, # Trial 12\n",
    "                  0.8894610998712026, # Trial 13\n",
    "                 ])).plot(kind='density', bw_method=0.3, title='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! cd models && allennlp best-params --study-name segmenter_tuning_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_trial = 'trial_8'\n",
    "! rm -r models/segmenter/elmo_ft/\n",
    "! mv models/optuna/segmenter_elmo_ft/$best_trial models/segmenter/elmo_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls models/segmenter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile models/eval_segmenter.sh\n",
    "# usage:\n",
    "# $ cd models \n",
    "# $ sh eval_segmenter.sh {bert|elmo_ft}\n",
    "\n",
    "export RESULT_DIR=${1}\n",
    "export DEV_FILE_PATH=\"rus.rst.rrt_dev.conll2003\"\n",
    "export TEST_FILE_PATH=\"rus.rst.rrt_test.conll2003\"\n",
    "\n",
    "allennlp predict --use-dataset-reader --cuda-device 0 --silent \\\n",
    "    --output-file segmenter/${RESULT_DIR}/predictions_dev.json segmenter/${RESULT_DIR}/model.tar.gz segmenter/${DEV_FILE_PATH}\n",
    "allennlp predict --use-dataset-reader --cuda-device 0 --silent \\\n",
    "    --output-file segmenter/${RESULT_DIR}/predictions_test.json segmenter/${RESULT_DIR}/model.tar.gz segmenter/${TEST_FILE_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax\n",
    "import json\n",
    "\n",
    "\n",
    "def load_predictions(path, threshold=None):\n",
    "    result = []\n",
    "    \n",
    "    if threshold:\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                result += json.loads(line)['logits']\n",
    "\n",
    "        probs = softmax(torch.tensor(result), dim=-1)\n",
    "        result = probs[:,1] > threshold\n",
    "        return ['B-S' if tag else 'O' for tag in result]\n",
    "    else:\n",
    "        with open(path, 'r') as file:\n",
    "            for line in file:\n",
    "                result += json.loads(line)['tags']\n",
    "        return [tag if tag=='O' else 'B-S' for tag in result]\n",
    "\n",
    "RESULT_DIR = 'elmo_ft'\n",
    "DEV_FILE_PATH=\"models/segmenter/rus.rst.rrt_dev.conll2003\"\n",
    "TEST_FILE_PATH=\"models/segmenter/rus.rst.rrt_test.conll2003\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### On dev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true = [line.strip().split()[-1] for line in open(DEV_FILE_PATH, 'r').readlines() if line.strip()]\n",
    "best_f1 = 0.\n",
    "best_threshold = 1.\n",
    "for threshold in tqdm(np.arange(1., 0.6, -0.01)):\n",
    "    pred = load_predictions(f'models/segmenter/{RESULT_DIR}/predictions_dev.json', threshold=threshold)\n",
    "    f1 = f1_score(true, pred, pos_label='B-S')\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_threshold = threshold\n",
    "print(best_f1, ':::', best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_recall = 0.\n",
    "best_threshold = 1.\n",
    "for threshold in tqdm(np.arange(1., 0.6, -0.01)):\n",
    "    pred = load_predictions(f'models/segmenter/{RESULT_DIR}/predictions_dev.json', threshold=threshold)\n",
    "    recall = recall_score(true, pred, pos_label='B-S')\n",
    "    if recall > best_recall:\n",
    "        best_recall = recall\n",
    "        best_threshold = threshold\n",
    "print(best_recall, ':::', best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! ls ../../models/segmenter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = load_predictions(f'models/segmenter/{RESULT_DIR}/predictions_dev.json')\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### On test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true = [line.strip().split()[-1] for line in open(TEST_FILE_PATH, 'r').readlines() if line.strip()]\n",
    "pred = load_predictions(f'models/segmenter/{RESULT_DIR}/predictions_test.json')\n",
    "\n",
    "print('length of true:', len(true))\n",
    "print('length of pred:', len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(true[:len(pred)], pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Playground "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.predictors import Predictor\n",
    "\n",
    "predictor = Predictor.from_path('models/segmenter/elmo_ft/model.tar.gz', cuda_device=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Тогда пришлите мне тексты . Я соберу парсер с уже дообученными сегментацией и классификацией отношений и размечу предложения до конца недели\"\n",
    "tags = predictor.predict(sentence)[\"tags\"]\n",
    "tuple(zip(sentence.split(), tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.whitespace_tokenizer import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor._tokenizer = WhitespaceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictor._to_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Тогда пришлите мне тексты . Я соберу парсер с уже дообученными сегментацией и классификацией отношений и размечу предложения до конца недели\"\n",
    "tags = predictor.predict(sentence)[\"tags\"]\n",
    "tuple(zip(sentence.split(), tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}