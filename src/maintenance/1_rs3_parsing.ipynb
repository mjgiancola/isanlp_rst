{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> in the original RuRSTreebank dataset, some deprecated symbols occure (>, <, &, etc.), breaking the xml parser, as well as EDUs with punctuation marks at the beginning (it happens when brackets and dots/commas are separated with space in the original text). The latest version of the corpus (at the time of this notebooks' latest commit) has been corrected and dumped in <b>corpus/RuRsTreebank_full_corrected.zip</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read RS3 files into isanlp.DiscourseUnit annotations\n",
    "input:\n",
    " - corpus with .rs3 files\n",
    "output:\n",
    " - ``binarized_trees/file_filename_PART.du``  - pickled isanlp DiscourseUnit with tree number PART from the original *.rs3 file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split dataset files into separated trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RS3ForestSplitter:\n",
    "    def __call__(self, filename: str, output_dir: str):\n",
    "        output_filename = filename.split('/')[-1]\n",
    "        output_filename = output_filename.replace(\".rst\", \"\").replace(\".rs3\", \"\")\n",
    "        \n",
    "        # Save file header and make adjacency matrix\n",
    "        \n",
    "        pairs = []  # [[id1, parent1], [id2, parent2], ...]\n",
    "        context = ET.iterparse(filename, events=('end', ))\n",
    "        for event, elem in context:\n",
    "            if elem.tag == 'header':\n",
    "                header = ET.tostring(elem).decode('utf-8')\n",
    "\n",
    "            elif elem.tag == 'body':\n",
    "                for child in elem:\n",
    "                    if child.get('parent'):\n",
    "                        pairs.append(list(map(int, [child.get('id'), child.get('parent')])))\n",
    "                    else:\n",
    "                        pairs.append(list(map(int, [child.get('id'), child.get('id')])))\n",
    "\n",
    "        max_id = np.array(pairs).max()\n",
    "        adj_matrix = np.zeros((max_id, max_id))\n",
    "        for pair in pairs:\n",
    "            adj_matrix[pair[0]-1, pair[1]-1] = 1\n",
    "            adj_matrix[pair[1]-1, pair[0]-1] = 1\n",
    "            adj_matrix[pair[0]-1, pair[0]-1] = 1\n",
    "            adj_matrix[pair[1]-1, pair[1]-1] = 1\n",
    "            \n",
    "        # Find separated trees\n",
    "        n_components, labels = connected_components(adj_matrix)\n",
    "        trees = dict()\n",
    "        for _id, tree_number in enumerate(labels):\n",
    "            if not trees.get(tree_number):\n",
    "                trees[tree_number] = [str(_id+1)]\n",
    "            else:\n",
    "                trees[tree_number].append(str(_id+1))\n",
    "                \n",
    "        trees_body = dict()\n",
    "        context = ET.iterparse(filename, events=('end', ))\n",
    "        for event, elem in context:\n",
    "            if elem.tag == 'body':\n",
    "                for child in elem:\n",
    "                    for tree_number, tree_ids in trees.items():\n",
    "                        if child.get('id') in tree_ids:\n",
    "                            if not trees_body.get(tree_number):\n",
    "                                trees_body[tree_number] = [child]\n",
    "                            else:\n",
    "                                trees_body[tree_number].append(child)\n",
    "        \n",
    "        # Write the results\n",
    "        for tree_number in trees_body.keys():\n",
    "            try:\n",
    "                with open(os.path.join(output_dir, f'{output_filename}_part_{tree_number}.rs3'), 'w') as f:\n",
    "                    f.write('<rst>\\n')\n",
    "                    f.write(header)\n",
    "                    f.write('<body>\\n')\n",
    "                    for element in trees_body.get(tree_number):\n",
    "                        _id = element.get('id')\n",
    "                        _type = element.get('type')\n",
    "                        _par = element.get('parent')\n",
    "                        parent = f'parent=\"{_par}\"' if _par else ''\n",
    "                        _relname = element.get('relname')\n",
    "                        f.write(f'\\t\\t<{element.tag} id=\"{_id}\" type=\"{_type}\" {parent} relname=\"{_relname}\"')\n",
    "                        if element.tag == 'segment':\n",
    "                            f.write(f'>{self.debracket_text(element.text)}</segment>\\n')\n",
    "                        elif element.tag == 'group':\n",
    "                            f.write('/>\\n')\n",
    "                    f.write('\\t</body>\\n')\n",
    "                    f.write('</rst>')\n",
    "            except:\n",
    "                print(f\"Skip tree {tree_number} in file {filename}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def debracket_text(text):\n",
    "        return text.replace(')', '-RB-').replace('(', '-LB-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RS3ForestSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls corpus/RuRsTreebank_full_6/news1/news1_rs3/* | head -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_dir = 'corpus_rs3'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "for filename in tqdm(glob.glob('corpus/RuRsTreebank_full_6/news1/news1_rs3/*.rst')):\n",
    "    splitter(filename, output_dir)\n",
    "    \n",
    "for filename in tqdm(glob.glob('corpus/RuRsTreebank_full_6/news2/news2_rs3/*.rst')):\n",
    "    splitter(filename, output_dir)\n",
    "    \n",
    "for filename in tqdm(glob.glob('corpus/RuRsTreebank_full_6/blogs/blogs_rs3/*.rst')):\n",
    "    splitter(filename, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht $output_dir/*.rs3 | wc -l  # Overall number of trees in news+blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert them all to *.dis files\n",
    "\n",
    "Using https://github.com/rst-workbench/rst-converter-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "input_dir = 'corpus_rs3'\n",
    "output_dir = 'corpus_dis'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "for file in glob.glob(os.path.join(input_dir, '*.rs3')):\n",
    "    output_file = os.path.join(output_dir, file.split('/')[-1].replace('.rs3', '.dis'))\n",
    "    ! curl -XPOST echistova:5000/convert/rs3/dis -F input=@$file > $output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lath $output_dir/*.dis | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Collect DiscourseUnit annotations for isanlp library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U git+https://github.com/tchewik/dis2du.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dis2du.read_dis import read_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dis2du.tree import RSTTree\n",
    "# from dis2du.convert2isanlp import convert2isanlp\n",
    "\n",
    "\n",
    "# def read_dis(filename):\n",
    "#     rst = RSTTree(filename)\n",
    "#     rst.build()\n",
    "#     return convert2isanlp(rst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_dir = 'corpus_dis'\n",
    "output_dir = 'corpus_du'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "failed = []\n",
    "for file in tqdm(glob.glob(os.path.join(input_dir, '*.dis'))):\n",
    "    \n",
    "    try:\n",
    "        tree = read_dis(file)\n",
    "        output_file = file.split('/')[-1].replace('.dis', '.du')\n",
    "        with open(os.path.join(output_dir, output_file), 'wb') as f:\n",
    "            pickle.dump(tree, f)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        failed.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align trees with the original texts (collect ``start`` and ``end`` for each node). Otherwise, we can't define paragraph boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.file_reading import prepare_text\n",
    "import pandas as pd\n",
    "\n",
    "def align_du2text(tree, text):\n",
    "    tree.text = prepare_text(tree.text).strip()\n",
    "    tree.start = text.find(tree.text)\n",
    "    tree.end = tree.start + len(tree.text)\n",
    "    if tree.relation != 'elementary':\n",
    "        tree.left = align_du2text(tree.left, text)\n",
    "        tree.right = align_du2text(tree.right, text)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob('corpus/RuRsTreebank_full_6/news1/news1_txt/*.txt')):\n",
    "    text = prepare_text(open(file, 'r').read().strip())\n",
    "    filename = file.split('/')[-1].replace('.txt', '')\n",
    "\n",
    "    for du_filename in sorted(glob.glob(os.path.join('corpus_du/', filename + '_part_*'))):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        all_pairs += extr_pairs(tree)\n",
    "        tree = align_du2text(tree, text)\n",
    "        pickle.dump(tree, open(du_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob('corpus/RuRsTreebank_full_6/news2/news2_txt/*.txt')):\n",
    "    text = prepare_text(open(file, 'r').read().strip())\n",
    "    filename = file.split('/')[-1].replace('.txt', '')\n",
    "\n",
    "    for du_filename in sorted(glob.glob(os.path.join('corpus_du/', filename + '_part_*'))):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        all_pairs += extr_pairs(tree)\n",
    "        tree = align_du2text(tree, text)\n",
    "        pickle.dump(tree, open(du_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extr_pairs(tree):\n",
    "#     pp = []\n",
    "#     if tree.left:\n",
    "#         pp.append([tree.left.text, tree.right.text, \n",
    "#                    tree.left.start, tree.right.start,\n",
    "#                    tree.relation, tree.nuclearity])\n",
    "#         pp += extr_pairs(tree.left)\n",
    "#         pp += extr_pairs(tree.right)\n",
    "#     return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(glob.glob('corpus/RuRsTreebank_full_6/blogs/blogs_txt/*.txt')):\n",
    "    text = prepare_text(open(file, 'r').read().strip())\n",
    "    filename = file.split('/')[-1].replace('.txt', '')\n",
    "\n",
    "    for du_filename in sorted(glob.glob(os.path.join('corpus_du/', filename + '_part_*'))):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        all_pairs += extr_pairs(tree)\n",
    "        tree = align_du2text(tree, text)\n",
    "        pickle.dump(tree, open(du_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(all_pairs, columns=['snippet_x', 'snippet_y', 'loc_x', 'loc_y', 'category_id', 'order'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Он страдает сам, он заставляет окружающих'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[pairs.snippet_x.str.contains(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pairs[pairs.snippet_y.str.contains(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! ls corpus/RuRsTreebank_full_6/news1/news1_txt/* | head -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from isanlp.annotation_rst import ForestExporter\n",
    "\n",
    "# exp = ForestExporter()\n",
    "# exp([tree], 'some_tree.rs3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Collect data for train and evaluation \n",
    "output:\n",
    " - ``data/file.edus``  - text file with edus from .rs3 - each line contains one edu\n",
    " - ``data/file.json``  - json file with du-pairs from gold trees. \n",
    " keys: ``['snippet_x', 'snippet_y', 'loc_x', 'loc_y', 'category_id', 'order', 'filename]``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extr_pairs(tree):\n",
    "    pp = []\n",
    "    if tree.left:\n",
    "        pp.append([tree.left.text, tree.right.text, \n",
    "                   tree.left.start, tree.right.start,\n",
    "                   tree.relation, tree.nuclearity])\n",
    "        pp += extr_pairs(tree.left)\n",
    "        pp += extr_pairs(tree.right)\n",
    "    return pp\n",
    "\n",
    "def extr_edus(tree):\n",
    "    if tree.relation == 'elementary':\n",
    "        return [tree.text]\n",
    "    else:\n",
    "        tt = []\n",
    "        tt += extr_edus(tree.left)\n",
    "        tt += extr_edus(tree.right)\n",
    "    return tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm -r data && mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "all_pairs = []\n",
    "for orig_filename in glob.glob(os.path.join('corpus_du/', '*part_0.du')):\n",
    "    for du_filename in sorted(glob.glob(orig_filename.replace('_0', '_*')),\n",
    "                             key=lambda x: float(re.findall(\"(\\d+)\",x)[-1])):\n",
    "        tree = pickle.load(open(du_filename, 'rb'))\n",
    "        pairs = extr_pairs(tree)\n",
    "        filename = du_filename.split('/')[-1].replace('.du', '')\n",
    "        filename = '_'.join(filename.split('_')[:2])\n",
    "        if pairs:\n",
    "            pairs = [pair + [filename] for pair in pairs]\n",
    "            all_pairs += pairs\n",
    "\n",
    "        edus = extr_edus(tree)\n",
    "        with open(os.path.join('data/', filename + '.edus'), 'a') as f:\n",
    "            f.write('\\n'.join(edus))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame(all_pairs, columns=['snippet_x', 'snippet_y', 'loc_x', 'loc_y', 'category_id', 'order', 'filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs[pairs.loc_x == -1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pairs[pairs.loc_x != -1]\n",
    "pairs = pairs[pairs.loc_y != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs['category_id'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs['relation'] = pairs['category_id'] + '_' + pairs['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.relation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pairs[pairs.relation == 'background_NN'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.reset_index().to_feather('data/all_pairs.fth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs[pairs.filename == 'blogs_65'].category_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate the texts with isanlp \n",
    "output:\n",
    " - file.annot.pkl  # morphology, syntax, semantics to use with isanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "from isanlp.ru.processor_mystem import ProcessorMystem\n",
    "from isanlp.processor_razdel import ProcessorRazdel\n",
    "# from isanlp.processor_deeppavlov_syntax import ProcessorDeeppavlovSyntax\n",
    "\n",
    "host_udpipe = 'papertext'\n",
    "port_udpipe = '3200'\n",
    "port_udpipe = '3134'\n",
    "\n",
    "ppl = PipelineCommon([\n",
    "    (ProcessorRazdel(), ['text'],\n",
    "    {'tokens': 'tokens',\n",
    "     'sentences': 'sentences'}),\n",
    "    (ProcessorRemote(host_udpipe, port_udpipe, '0'),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'lemma': 'lemma',\n",
    "      'syntax_dep_tree': 'syntax_dep_tree',\n",
    "      'postag': 'ud_postag'}),\n",
    "    (ProcessorMystem(delay_init=False),\n",
    "     ['tokens', 'sentences'],\n",
    "     {'postag': 'postag'}),\n",
    "    (ConverterMystemToUd(),\n",
    "     ['postag'],\n",
    "     {'morph': 'morph',\n",
    "      'postag': 'postag'}),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl('Мама мыла раму.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import prepare_text\n",
    "#from utils.file_reading import _prepare_text as prepare_text\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full_6/blogs/blogs_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/news1/news1_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/news2/news2_txt/'\n",
    "               ]\n",
    "\n",
    "for path in directories:\n",
    "    print('analyze path:', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        if not os.path.isfile(os.path.join('data', filename)):\n",
    "            text = prepare_text(open(file, 'r').read().strip())\n",
    "            try:\n",
    "                annot = ppl(text)\n",
    "                pickle.dump(annot, open(os.path.join('data', filename), 'wb'))\n",
    "            except:\n",
    "                print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht data/*.pkl | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -laht data/*.edus | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) parse science texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import _prepare_text as prepare_text\n",
    "\n",
    "directories = ['corpus/RuRsTreebank_full_6/sci_comp/sci_comp_txt/',\n",
    "               'corpus/RuRsTreebank_full_6/sci_ling/sci_ling_txt/',\n",
    "               ]\n",
    "\n",
    "for path in directories:\n",
    "    print('analyze path:', path)\n",
    "    for file in tqdm(glob.glob(f'{path}*.txt')):\n",
    "        text = open(file, 'r').read()\n",
    "        text = text.replace('  \\n', '#####').replace('\\n', ' ')\n",
    "        text = prepare_text(text)\n",
    "        annot = ppl(text)\n",
    "        filename = file.split('/')[-1].replace('.txt', '.annot.pkl')\n",
    "        pickle.dump(annot, open(os.path.join('data', filename), 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold trees\n",
    "### Extract features \n",
    "output:\n",
    " - models/tf_idf/pipeline.pkl  # is used in default feature extraction\n",
    " - file.gold.pkl  # dataset with extracted default features for gold trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from utils.file_reading import read_annotation\n",
    "\n",
    "\n",
    "input_dir = 'data/'\n",
    "\n",
    "if not os.path.isdir('models'):\n",
    "    os.path.mkdir('models')\n",
    "\n",
    "if not os.path.isdir('models/tf_idf'):\n",
    "    os.path.mkdir('models/tf_idf')\n",
    "\n",
    "corpus = []\n",
    "for file in glob.glob(os.path.join(input_dir, f\"*.annot.pkl\")):\n",
    "    tokens = read_annotation(file.replace('.annot.pkl', ''))['tokens']\n",
    "    corpus.append(list(map(lambda token: token.text.lower(), tokens)))\n",
    "\n",
    "    \n",
    "from utils.count_vectorizer import MyCountVectorizer\n",
    "count_vect = MyCountVectorizer(ngram_range=(1, 2), tokenizer=MyCountVectorizer.dummy, preprocessor=MyCountVectorizer.dummy)\n",
    "\n",
    "svd = TruncatedSVD(n_components=25,\n",
    "                   tol=0.0,\n",
    "                   n_iter=7,\n",
    "                   random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', count_vect),\n",
    "    ('svd', svd)\n",
    "])\n",
    "\n",
    "pipeline.fit(corpus)\n",
    "pickle.dump(pipeline, open('models/tf_idf/pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# python -c \"import nltk; nltk.download('stopwords')\"\n",
    "# pip install dostoevsky\n",
    "# dostoevsky download fasttext-social-network-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../isanlp_rst/utils/features_processor_variables.py utils/features_processor_variables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install \"scikit_learn==0.22.2.post1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.print_tree import printBTree\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../')\n",
    "\n",
    "from _isanlp_rst.src.isanlp_rst.features_processor_default import FeaturesProcessor\n",
    "\n",
    "features_processor = FeaturesProcessor(model_dir_path='models', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.autonotebook import tqdm\n",
    "from utils.file_reading import read_gold, read_annotation\n",
    "\n",
    "table = read_gold('data/all_pairs')\n",
    "table = table[table.snippet_x.map(len) > 0]\n",
    "table = table[table.snippet_y.map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[table.snippet_x.str.contains('И так. Поправим ACL domains следующим')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.category_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[table.snippet_x.str.contains('И так. Поправим ACL domains следующим')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename, df in tqdm(table.groupby('filename')):\n",
    "    annot = read_annotation(os.path.join('data', filename))\n",
    "    features = features_processor(df, \n",
    "                                  annot['text'], annot['tokens'], \n",
    "                                  annot['sentences'], annot['lemma'], \n",
    "                                  annot['morph'], annot['ud_postag'], \n",
    "                                  annot['syntax_dep_tree'])\n",
    "    features.to_pickle(filename + '.gold.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pklfile in tqdm(glob.glob('data/*.gold.pkl')):\n",
    "    features = pd.read_pickle(pklfile)\n",
    "    merge = pd.merge(features, table, on=['snippet_x', 'snippet_y'])\n",
    "    features['category_id'] = merge.category_id_y\n",
    "    features['order'] = merge.order_y\n",
    "    features['filename'] = merge.filename_y\n",
    "    features.to_pickle(pklfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
